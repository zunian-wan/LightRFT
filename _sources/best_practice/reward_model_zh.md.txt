# LightRFT Reward Model è®­ç»ƒæœ€ä½³å®è·µæŒ‡å—

## ç›®å½•

1. [æ¦‚è¿°](#1-æ¦‚è¿°)
2. [Reward Model ç±»å‹](#2-reward-model-ç±»å‹)
3. [ç¯å¢ƒå‡†å¤‡](#3-ç¯å¢ƒå‡†å¤‡)
4. [æ¨¡å‹è®­ç»ƒ](#4-æ¨¡å‹è®­ç»ƒ)
5. [æ¨¡å‹è¯„ä¼°](#5-æ¨¡å‹è¯„ä¼°)
6. [å¸¸è§é—®é¢˜](#6-å¸¸è§é—®é¢˜)
7. [åŸºå‡†æµ‹è¯•ä¸æ€§èƒ½å‚è€ƒ](#7-åŸºå‡†æµ‹è¯•ä¸æ€§èƒ½å‚è€ƒ)
8. [è¿›é˜¶è¯é¢˜](#8-è¿›é˜¶è¯é¢˜)
9. [å‚è€ƒèµ„æº](#9-å‚è€ƒèµ„æº)

---

## 1. æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ Reward Model

åœ¨åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (Reinforcement Learning from Human Feedback, RLHF) çš„æµç¨‹ä¸­ï¼ŒReward Model æ‰®æ¼”ç€äººç±»åå¥½ä»£ç†çš„å…³é”®è§’è‰²ã€‚

**ä¸ºä»€ä¹ˆéœ€è¦ Reward Modelï¼Ÿ**
åœ¨è®¸å¤šå¤æ‚çš„ä»»åŠ¡åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬é¢ä¸´ç€**å¥–åŠ±å‡½æ•°éš¾ä»¥æ˜¾å¼ç”¨ç®€å•çš„è§„åˆ™å®šä¹‰**çš„æŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼š
*   **ç”Ÿæˆä»»åŠ¡**ï¼šå¦‚ä½•ç”¨å…¬å¼è¡¡é‡ä¸€æ®µæ–‡æœ¬çš„æ–‡é‡‡ã€ä¸€å¼ å›¾åƒçš„ç¾æ„Ÿï¼Ÿ
*   **äº¤äº’ä¸æ§åˆ¶**ï¼šå¦‚ä½•å®šä¹‰é©¾é©¶çš„èˆ’é€‚ç¨‹åº¦ã€æˆ–è€…æœºå™¨äººåŠ¨ä½œçš„è‡ªç„¶åº¦ï¼Ÿ

è™½ç„¶äººç±»å¯ä»¥å¯¹è¿™äº›ç»“æœè¿›è¡Œä¸»è§‚è¯„åˆ¤ï¼Œä½†åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­å®æ—¶è·å–äººå·¥åé¦ˆæ—¢æ˜‚è´µåˆè€—æ—¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è®­ç»ƒä¸€ä¸ª Reward Model æ¥**æ¨¡æ‹Ÿäººç±»çš„åˆ¤æ–­æ ‡å‡†**ã€‚

**Reward Model çš„ä½œç”¨**
Reward Model æ¥æ”¶ä¸€ä¸ªè¾“å…¥ï¼ˆå¦‚ Prompt æˆ–ç¯å¢ƒçŠ¶æ€ï¼‰åŠå…¶å¯¹åº”çš„è¾“å‡ºï¼ˆResponse æˆ–åŠ¨ä½œï¼‰ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªè¯„ä»·ä¿¡å·ã€‚è¿™ä¸ªä¿¡å·é‡åŒ–äº†ç»“æœåœ¨å¤šå¤§ç¨‹åº¦ä¸Šç¬¦åˆäººç±»çš„æœŸæœ›ï¼ˆå¦‚æœ‰ç”¨æ€§ã€å®‰å…¨æ€§ã€çœŸå®æ€§ç­‰ï¼‰ï¼Œä»è€Œä¸ºç­–ç•¥æ¨¡å‹ï¼ˆPolicy Modelï¼‰çš„ä¼˜åŒ–æä¾›å¯æ‰©å±•çš„ã€ä¸€è‡´çš„åé¦ˆæŒ‡å¯¼ã€‚

æ ¹æ®è¾“å‡ºå½¢å¼çš„ä¸åŒï¼ŒReward Model ä¸»è¦åˆ†ä¸ºä¸¤ç±»ï¼š

1.  **Scalar Reward Model (SRM)**: è¿™æ˜¯æœ€ç»å…¸çš„å¥–åŠ±æ¨¡å‹å½¢å¼ã€‚å®ƒå°†è¾“å…¥å’Œå“åº”æ˜ å°„ä¸ºä¸€ä¸ªå•ä¸€çš„æ ‡é‡åˆ†æ•°ï¼ˆScalar Scoreï¼‰ã€‚SRM çš„ä¼˜åŠ¿åœ¨äºè®¡ç®—æ•ˆç‡é«˜ï¼Œä¸”è¾“å‡ºçš„æ•°å€¼ä¿¡å·å¯ä»¥ç›´æ¥ä½œä¸ºå¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆå¦‚ PPOï¼‰ä¸­çš„ Rewardï¼Œæˆ–è€…ç”¨äº Rejection Samplingã€‚ç„¶è€Œï¼Œå•ä¸€çš„æ ‡é‡å¾€å¾€éš¾ä»¥è§£é‡Šæ¨¡å‹æ‰“åˆ†çš„ä¾æ®ï¼Œä¹Ÿéš¾ä»¥æ•æ‰å¤æ‚çš„å¤šç»´åº¦åå¥½ã€‚

2.  **Generative Reward Model (GRM)**: è¿™æ˜¯ä¸€ç§æ–°å…´çš„å¥–åŠ±æ¨¡å‹èŒƒå¼ã€‚GRM åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œä»¥è‡ªç„¶è¯­è¨€çš„å½¢å¼è¾“å‡ºè¯„ä»·ã€‚å®ƒä¸ä»…èƒ½ç»™å‡ºæœ€ç»ˆçš„åˆ¤æ–­ï¼ˆå¦‚â€œå›ç­” A æ›´å¥½â€ï¼‰ï¼Œè¿˜èƒ½ç”Ÿæˆè¯¦ç»†çš„æ€ç»´é“¾ï¼ˆChain-of-Thought, CoTï¼‰æ¥è§£é‡Šè¯„ä»·ç†ç”±ã€‚GRM å…·æœ‰æ›´å¼ºçš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸”é€šè¿‡æ¨¡æ‹Ÿäººç±»çš„æ¨ç†è¿‡ç¨‹ï¼Œå¾€å¾€èƒ½åœ¨å¤æ‚çš„è¯„ä¼°ä»»åŠ¡ä¸­å±•ç°å‡ºæ›´é«˜çš„å‡†ç¡®æ€§ã€‚

Reward Model ç»™å‡ºçš„å¥–åŠ±ä¿¡å·ï¼ˆæ— è®ºæ˜¯æ ‡é‡è¿˜æ˜¯åŸºäºæ–‡æœ¬è§£æå‡ºçš„ç»“æœï¼‰å°†ç”¨äºæŒ‡å¯¼ Policy Modelï¼ˆç­–ç•¥æ¨¡å‹ï¼‰çš„ä¼˜åŒ–ã€‚é€šè¿‡æœ€å¤§åŒ– RM ç»™å‡ºçš„å¥–åŠ±ï¼ŒPolicy Model èƒ½å¤Ÿå­¦ä¹ ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½å’Œä»·å€¼è§‚çš„å†…å®¹ã€‚å› æ­¤ï¼ŒReward Model çš„è´¨é‡ç›´æ¥å†³å®šäº†æœ€ç»ˆæ¨¡å‹çš„å¯¹é½æ•ˆæœï¼Œæ˜¯å¯¹é½å¤§æ¨¡å‹ç”Ÿæˆè¡Œä¸ºä¸äººç±»åå¥½çš„å…³é”®æ¡¥æ¢ã€‚ 

### 1.2 LightRFT ä¸­çš„ RM æ”¯æŒ

LightRFT æä¾›äº†å®Œæ•´çš„å¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒï¼š

**æ¨¡å‹ç±»å‹ï¼š**
- **Scalar Reward Model (SRM)**: æ ‡é‡å¥–åŠ±æ¨¡å‹ï¼Œè¾“å‡ºæ ‡é‡åˆ†æ•°ï¼ˆ0-1 ä¹‹é—´ï¼‰
- **Generative Reward Model (GRM)**: ç”Ÿæˆå¼çš„å¥–åŠ±æ¨¡å‹ï¼Œç”Ÿæˆå¸¦æ¨ç†è¿‡ç¨‹ï¼ˆCoTï¼‰ä¸æœ€ç»ˆç»“è®ºçš„æ–‡æœ¬å¼è¯„ä¼°ï¼ˆå¦‚ <think>â€¦</think><answer>â€¦</answer>ï¼‰ï¼Œå…·æœ‰æ›´å¥½çš„å¯è§£é‡Šæ€§ã€‚

**æ”¯æŒçš„æ¨¡æ€ï¼š**
- **Vision-Language (VL)**: å›¾åƒ-æ–‡æœ¬ã€è§†é¢‘-æ–‡æœ¬
- **Audio-Language (AL)**: éŸ³é¢‘-æ–‡æœ¬
- **Language-Only**: çº¯æ–‡æœ¬ï¼Œå³ç°æœ‰çš„ LLM æ¨¡å‹

**è®­ç»ƒåç«¯ï¼š**
- DeepSpeed ZeRO (Stage 1/2/3)

---

## 2. Reward Model ç±»å‹

### 2.1 Scalar Reward Model (SRM)

#### ç‰¹ç‚¹
- è¾“å‡ºå•ä¸€æ ‡é‡åˆ†æ•°ï¼ˆé€šè¿‡ Sigmoid æ˜ å°„åˆ° 0-1ï¼‰
- è®­ç»ƒä½¿ç”¨æˆå¯¹åå¥½æ•°æ®ï¼ˆPairwise Preferenceï¼‰
- æ”¯æŒå¤šä¸ª reward headï¼ˆå¦‚ preference, alignment, helpfulnessï¼‰

#### é€‚ç”¨åœºæ™¯
- éœ€è¦å¿«é€Ÿæ¨ç†çš„åœºæ™¯
- ä½œä¸º PPO/GRPO ç­‰ RL ç®—æ³•çš„å¥–åŠ±ä¿¡å·
- å¤šç»´åº¦åå¥½å»ºæ¨¡

#### æŸå¤±å‡½æ•°
- **Bradley-Terry Loss** (Log-Sigmoid): `-log(Ïƒ(r_chosen - r_reject - margin))`
- **LogExp Loss**: `log(1 + exp(r_reject - r_chosen))`
- **HPS Scale Loss**: Cross-entropy with learnable temperature (åœ¨æˆ‘ä»¬çš„å®éªŒä¸­è¡¨ç°æ›´ä¼˜)

#### æ¶æ„
```
Backbone (e.g. Vision-Language Model)
    â†“
Hidden States (from probing_layer)
    â†“
Pooling (Attention or Last Token)
    â†“
Reward Head (MLP + Sigmoid)
    â†“
Scalar Score (0-1)
```

### 2.2 Generative Reward Model (GRM)

#### ç‰¹ç‚¹
- ç”Ÿæˆæ–‡æœ¬å½¢å¼çš„è¯„ä»·å’Œç†ç”±
- è®­ç»ƒä½¿ç”¨æ ‡å‡†è¯­è¨€æ¨¡å‹æŸå¤± (Next-Token Prediction)
- å¯è§£é‡Šæ€§å¼º

#### é€‚ç”¨åœºæ™¯
- éœ€è¦æä¾›è¯„ä»·ç†ç”±çš„åœºæ™¯
- å¤æ‚ä»»åŠ¡çš„ç»†ç²’åº¦è¯„ä¼°
- ç ”ç©¶å’Œåˆ†ææ¨¡å‹è¡Œä¸º

#### è®­ç»ƒæ–¹å¼
- **GPT-LM Loss**: æ ‡å‡†çš„ next-token prediction loss
- **å¼ºåŒ–å¾®è°ƒï¼ˆReinforcement Fine-Tuning, RFTï¼‰**ï¼š ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼è®­ç»ƒï¼Œä»¥æé«˜ GRM çš„æ¨ç†èƒ½åŠ›ã€‚åœ¨ LightRFT ä¸­è¯¥æ¨¡å—è¿˜åœ¨å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…ã€‚

#### æ¶æ„
```
Multi-modal Encoder (e.g. Vision2Seq Model)
    â†“
LLM Decoder
    â†“
Generated Text (Reward Description)
```

### 2.3 é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èç±»å‹ | åŸå›  |
|------|---------|------|
| PPO/DPO è®­ç»ƒ | SRM | æ¨ç†é€Ÿåº¦å¿«ï¼Œæ ‡é‡ä¿¡å·æ˜“äºä½¿ç”¨ |
| å¤æ‚ä»»åŠ¡è¯„ä¼° | GRM | å¯ä»¥ç”Ÿæˆè¯¦ç»†çš„è¯„ä»·ç†ç”± |
| å¤šç»´åº¦åå¥½ | SRM/GRM (å¤š head) | å¯ä»¥åŒæ—¶è®­ç»ƒå¤šä¸ªç»´åº¦ |
| å¯è§£é‡Šæ€§è¦æ±‚é«˜ | GRM | æä¾›æ–‡æœ¬è§£é‡Š |
| å®æ—¶åº”ç”¨ | SRM | æ¨ç†å¼€é”€å° |

---

## 3. ç¯å¢ƒå‡†å¤‡

### 3.1 ä¾èµ–å®‰è£…

```bash
cd LightRFT

# å®‰è£…åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹
pip install qwen-vl-utils  # Qwen-VL ç³»åˆ—
pip install keye-vl-utils  # KeyeVL ç³»åˆ—

# å¯¹äºéŸ³é¢‘è¯­è¨€æ¨¡å‹
pip install librosa
```

### 3.2 GPU è¦æ±‚

**æœ€ä½é…ç½®ï¼š**
- SRM (3B æ¨¡å‹, å…¨é‡å¾®è°ƒ): 1x H200/A100 (80GB)
- SRM (7B æ¨¡å‹): 1x H200 with ZeRO-3
- GRM (3B æ¨¡å‹): 1x H200 with ZeRO-3
- GRM (7B æ¨¡å‹): 2x H200 with ZeRO-3

**æ¨èé…ç½®ï¼š**
- 8x A100 (80GB) for 7B-72B models
- ä½¿ç”¨ ZeRO-3 + LoRA for larger models

---

## 4. æ¨¡å‹è®­ç»ƒ

LightRFT æä¾›äº†å¼€ç®±å³ç”¨çš„è®­ç»ƒè„šæœ¬ï¼Œæ”¯æŒ SRM å’Œ GRM çš„è®­ç»ƒã€‚
æˆ‘ä»¬å·²ç»ä¸ºå¸¸è§åœºæ™¯å‡†å¤‡å¥½å¯ç›´æ¥è¿è¡Œçš„è„šæœ¬ä¸é»˜è®¤é…ç½®ï¼Œå¼€ç®±å³ç”¨ï¼›åŒæ—¶ä¹Ÿæ”¯æŒæŒ‰éœ€ä¿®æ”¹å‚æ•°ä»¥é€‚é…ä½ çš„å®éªŒã€‚

### 4.0 ä¸€é”®è¿è¡Œï¼ˆSRM / GRMï¼‰

å¿«é€Ÿå¼€å§‹ï¼šæŒ‰éœ€ä¿®æ”¹è„šæœ¬ä¸­çš„æ•°æ®è·¯å¾„ä¸ä¿å­˜ç›®å½•åç›´æ¥è¿è¡Œã€‚

```bash
# é¦–å…ˆè¿›å…¥ LightRFT æ ¹ç›®å½•
cd LightRFT

# è®­ç»ƒ Vision-Language æ¨¡å‹çš„ SRM
bash examples/srm_training/run_srm_vl.sh

# è®­ç»ƒ Audio-Language æ¨¡å‹çš„ SRM
bash examples/srm_training/run_srm_al.sh

# è®­ç»ƒ Vision-Language æ¨¡å‹çš„ GRM
bash examples/srm_training/run_grm_vl.sh
```

è¯´æ˜ï¼š
- è„šæœ¬å·²å†…ç½®æ¨èé…ç½®ã€‚
- æ”¯æŒå•æœºå¤šå¡è¿è¡Œï¼Œé»˜è®¤ä½¿ç”¨ torchrun å¯åŠ¨ï¼›å¦‚éœ€åˆ†å¸ƒå¼å¤šæœºï¼Œè¯·åœ¨è„šæœ¬é¡¶éƒ¨æŒ‰æ³¨é‡Šè®¾ç½® NNODESã€MASTER_ADDR/PORT ç­‰ç¯å¢ƒå˜é‡ã€‚

ä»¥ä¸‹æ˜¯åŸºäºæˆ‘ä»¬å®éªŒè®¾ç½®çš„è¯¦ç»†å‘½ä»¤ä¸å…³é”®å‚æ•°è¯´æ˜ã€‚

### 4.1 è®­ç»ƒ Scalar Reward Model (SRM)

#### 4.1.1 åŸºç¡€è®­ç»ƒè„šæœ¬ (åŸºäº T2I å®éªŒ)

```bash
#!/bin/bash

# è®¾ç½®ç¯å¢ƒå˜é‡
export GPUS_PER_NODE= 2             # æ¯ä¸ªèŠ‚ç‚¹çš„ GPU æ•°é‡
export NNODES=1                     # èŠ‚ç‚¹æ•°é‡
export NODE_RANK=0                  # å½“å‰èŠ‚ç‚¹çš„ rank
export MASTER_ADDR=127.0.0.1        # ä¸»èŠ‚ç‚¹åœ°å€
export MASTER_PORT=29500            # ä¸»èŠ‚ç‚¹ç«¯å£

# è®­ç»ƒå‚æ•°
PRETRAIN="Qwen/Qwen2.5-VL-3B"
# ä½¿ç”¨ HPDv3 è®­ç»ƒé›† å’Œ æµ‹è¯•é›†
TRAIN_DATA="/path/to/hpdv3/train.json"
EVAL_DATA="/path/to/hpdv3/test.json"
SAVE_PATH="./checkpoints/srm_qwen2.5vl_3b_hpdv3"

# è®¾ç½® Task Instruncion
TASK_INSTRUCTION="Your will act as an expert image evaluator for text-to-image generation.
Given a text prompt and a generated image, your task is to assess the overall quality of the image in relation to the prompt.
Your evaluation should focus on the following key aspects:
â€¢ Preference: Which image would a human viewer find more satisfying or visually appealing overall.
â€¢ Alignment: How well the image content matches the given text prompt in semantics, objects, and attributes.
Your task is provided in the following, please give your judgement based on above criteria.
The prompt used for generation is as follows: {prompt}.
"

# å¯åŠ¨è®­ç»ƒ
set -x

torchrun --nnodes $NNODES --nproc-per-node $GPUS_PER_NODE --node_rank $NODE_RANK --master-port $MASTER_PORT --master-addr $MASTER_ADDR \
    examples/demo_srm_training/train_srm_vl.py \
    --pretrain $PRETRAIN \
    --train_data $TRAIN_DATA \
    --save_path $SAVE_PATH \
    --ckpt_path $SAVE_PATH \
    --train_batch_size 32 \
    --micro_train_batch_size 2 \
    --max_epochs 5 \
    --actor_learning_rate 1e-5 \
    --lr_warmup_ratio 0.05 \
    --prompt_max_len 2048 \
    --pooling_method attn \
    --probing_layer -1 \
    --heads_types preference \
    --scale_for_train \
    --margin 0.1 \
    --task_instruction "$TASK_INSTRUCTION" \
    --loss_type hps \
    --zero_stage 2 \
    --bf16 \
    --flash_attn \
    --gradient_checkpointing \
    --logging_steps 1 \
    --save_steps 100 \
    --use_wandb "your_wandb_key" \
    --wandb_project "reward_model_training"
```

#### 4.1.2 å…³é”®å‚æ•°è¯´æ˜

**æ¨¡å‹å‚æ•°ï¼š**
- `--pretrain`: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ã€‚å®éªŒä¸­ä½¿ç”¨ `Qwen/Qwen2.5-VL-3B`ã€‚
- `--pooling_method`:
  - `attn`: ä½¿ç”¨ attention poolingï¼ˆ**å®éªŒä¸­ä½¿ç”¨ï¼Œæ¨è**ï¼‰ã€‚
  - `last`: ä½¿ç”¨æœ€åä¸€ä¸ª tokenã€‚
- `--probing_layer`: ä»å“ªä¸€å±‚æå–ç‰¹å¾ä½œä¸º reward head çš„è¾“å…¥ã€‚
  - `-1`: æœ€åä¸€å±‚ï¼ˆ**å®éªŒä¸­ä½¿ç”¨ï¼Œé»˜è®¤**ï¼‰ã€‚
  - `17`: ç¬¬ 17 å±‚ï¼ˆå¯ä½œä¸ºå˜ä½“å°è¯•ï¼‰ã€‚
- `--heads_types`: reward head ç±»å‹ï¼Œé»˜è®¤åªä½¿ç”¨ `preference`ã€‚
                   å¯ä»¥è®¾ç½®ä¸ºå¤šä¸ªç»´åº¦ï¼Œå¦‚ `preference alignment coherence`ã€‚
                   ä½†éœ€è¦ç¡®ä¿æ•°æ®ä¸­åŒ…å«å¯¹åº”çš„æ ‡ç­¾ã€‚

**è®­ç»ƒå‚æ•°ï¼š**
- `--train_batch_size`: å…¨å±€ batch sizeã€‚T2I å®éªŒè®¾ä¸º `32`ï¼ŒT2V å®éªŒè®¾ä¸º `8`ã€‚
- `--micro_train_batch_size`: æ¯ä¸ª GPU çš„ batch sizeã€‚
- `--actor_learning_rate`: å­¦ä¹ ç‡ã€‚**å®éªŒä¸­å…¨é‡å¾®è°ƒä½¿ç”¨ `1e-5`**ã€‚
- `--video_fps`: è§†é¢‘æ•°æ®çš„é‡‡æ ·å¸§ç‡ï¼ŒT2V å®éªŒä¸­è®¾ä¸º `2.0`ã€‚
- `--scale_for_train`: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯ç”¨å¯å­¦ä¹ çš„ç¼©æ”¾ç³»æ•° (learnable scaling factor)ã€‚
- `--margin`: åœ¨ BT Loss ä¸­ä½¿ç”¨çš„ margin å€¼ï¼ŒHPS å’Œ LogExp ä¸­æ— æ•ˆã€‚

**Prompt ç›¸å…³ï¼š**
- `--task_instruction`: ä»»åŠ¡æŒ‡ä»¤ï¼Œç”¨äºæŒ‡å¯¼å¥–åŠ±æ¨¡å‹ç†è§£è¯„ä»·æ ‡å‡†ã€‚å®éªŒä¸­ T2I è®¾ä¸ºä¸Šè¿°ç¤ºä¾‹å†…å®¹ã€‚

**è®­ç»ƒè®°å½•ï¼š**
LightRFT æ”¯æŒå¤šç§è®­ç»ƒæ—¥å¿—è®°å½•æ–¹å¼ï¼š
- `--use_wandb`: å¯ç”¨ Weights & Biases è¿›è¡Œè®­ç»ƒæ—¥å¿—è®°å½•ã€‚
- `--use_tensorboard`: å¯ç”¨ TensorBoard è¿›è¡Œè®­ç»ƒæ—¥å¿—è®°å½•å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ã€‚
                       å¯é€šè¿‡åŠ å…¥ --use_tensorboard "path/to/logs" å¯ç”¨ã€‚

**æŸå¤±å‡½æ•°ï¼š**
- `--loss_type`:
  - `hps`: ä½¿ç”¨ HPS Scale Lossï¼ˆ**å®éªŒä¸­ä½¿ç”¨ï¼Œé»˜è®¤**ï¼‰ã€‚
  - `sigmoid`: ä½¿ç”¨ BT (Bradley-Terry) Lossã€‚
  - `logexp`: ä½¿ç”¨ LogExp Lossã€‚

#### 4.1.3 LoRA è®­ç»ƒ

å¯¹äºå¤§æ¨¡å‹ï¼ˆ> 7Bï¼‰æˆ–æ˜¾å­˜å—é™æƒ…å†µï¼Œæ¨èä½¿ç”¨ LoRAï¼š

```bash
python examples/demo_srm_training/train_srm.py \
    --pretrain "Qwen/Qwen2.5-VL-72B-Instruct" \
    ... \
    --lora_rank 64 \
    --lora_alpha 128 \
    --lora_dropout 0.1 \
    --target_modules all-linear \
    --zero_stage 2
```

### 4.2 è®­ç»ƒ Generative Reward Model (GRM)

#### 4.2.1 åŸºç¡€è®­ç»ƒè„šæœ¬ (åŸºäº T2I å®éªŒ)

```bash
#!/bin/bash

PRETRAIN="Qwen/Qwen2.5-VL-3B"
TRAIN_DATA="/path/to/ImageGen-CoT-Reward-5K.json"
SAVE_PATH="./checkpoints/grm_qwen2.5vl_3b"

torchrun --nnodes $NNODES --nproc-per-node $GPUS_PER_NODE --node_rank $NODE_RANK --master-port $MASTER_PORT --master-addr $MASTER_ADDR \
    examples/demo_grm_training/train_grm_vl.py \
    --pretrain $PRETRAIN \
    --train_data $TRAIN_DATA \
    --save_path $SAVE_PATH \
    --ckpt_path $SAVE_PATH \
    --train_batch_size 4 \
    --micro_train_batch_size 1 \
    --max_epochs 2 \
    --actor_learning_rate 1e-5 \
    --prompt_max_len 4096 \
    --zero_stage 2 \
    --bf16 \
    --flash_attn \
    --gradient_checkpointing \
    --logging_steps 10 \
    --save_steps 500
```

**æ³¨æ„äº‹é¡¹ï¼š**
- GRM é€šå¸¸éœ€è¦æ›´é•¿çš„åºåˆ—é•¿åº¦ (`--prompt_max_len 4096`) ä»¥å®¹çº³ CoT æ–‡æœ¬ã€‚
- ç”±äºåºåˆ—é•¿ï¼Œbatch size éœ€ç›¸åº”å‡å°ï¼Œå®éªŒä¸­ä½¿ç”¨ `4`ã€‚
- å­¦ä¹ ç‡ä¸ SRM å®éªŒä¿æŒä¸€è‡´ (`1e-5`)ã€‚

---

## 5. æ¨¡å‹è¯„ä¼°

### 5.1 æ¨¡å‹è½¬æ¢
æˆ‘ä»¬çš„è®­ç»ƒè„šæœ¬é»˜è®¤ä½¿ç”¨ DeepSpeed ZeRO ä½œä¸ºè®­ç»ƒå¼•æ“ï¼Œè®­ç»ƒä¸­é€”ä¼šä¿å­˜ DeepSpeed æ ¼å¼çš„ checkpointï¼Œå› æ­¤éœ€è¦å°†å…¶è½¬æ¢ä¸ºæ ‡å‡†çš„ HuggingFace æ ¼å¼ä»¥ä¾¿è¿›è¡Œæ¨ç†å’Œè¯„ä¼°ã€‚

å¯¹äº SRM æ¨¡å‹ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿›è¡Œè½¬æ¢ï¼š
```bash
python examples/ckpt_scripts/ds2hf.py \
    --hf_base /path/to/base/model \
    --model_type srm_vl \
    --checkpoint_dir /path/to/deepspeed/checkpoint/dir \
    --output_dir /path/to/output/huggingface/output/dir \
    --scale_for_train \
    --pooling_method attn \
    --heads_types preference
```

å¯¹äº GRM æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿›è¡Œè½¬æ¢ï¼š
```bash
python examples/ckpt_scripts/ds2hf.py \
    --hf_base /path/to/base/model \
    --model_type grm \
    --checkpoint_dir /path/to/deepspeed/checkpoint/dir \
    --output_dir /path/to/output/huggingface/output/dir
```

### 5.2 è¯„ä¼°æ•°æ®å‡†å¤‡

è¯„ä¼°æ•°æ®æ ¼å¼ä¸è®­ç»ƒæ•°æ®ç›¸åŒï¼Œä½†åº”ä¸ºç‹¬ç«‹çš„æµ‹è¯•é›†ã€‚

**å®éªŒä¸­ä½¿ç”¨çš„è¯„ä¼°é›†ï¼š**
- **Text-to-Image (T2I) ä»»åŠ¡**
  - `OmniReward-Bench-T2I`
  - `HPDv3 Test Set`
  - `ImageRewardDB Test Set`
  - `GenAI-Bench`
- **Text-to-Video (T2V) ä»»åŠ¡**
  - `OmniReward-Bench-T2V`
- **Text-to-Audio (T2A) ä»»åŠ¡**
  - `OmniReward-Bench-T2A`

### 5.3 è¯„ä¼°è„šæœ¬ç¤ºä¾‹

æˆ‘ä»¬åœ¨ `examples/demo_srm_training` å’Œ `examples/demo_grm_training` ç›®å½•ä¸‹æä¾›äº†è¯„ä¼°è„šæœ¬ `test_srm_vl.py` å’Œ `test_grm_vl.py`ï¼Œåˆ†åˆ«ç”¨äº SRM å’Œ GRM çš„è¯„æµ‹ã€‚è¯„ä¼°è„šæœ¬ä¸­ä¸ºä¸åŒçš„ benchmark å®ç°ç›¸åº”çš„ Evaluator ç±»ã€‚æ”¯æŒå®ç°è‡ªå®šä¹‰çš„ Evaluator ä»¥é€‚åº”æ–°çš„è¯„ä¼°éœ€æ±‚ã€‚

å¦å¤–ï¼Œä¹Ÿæ”¯æŒåœ¨è®­ç»ƒè„šæœ¬ä¸­æŒ‡å®šè¯„ä¼°æ•°æ®ï¼Œå®ç°è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®šæœŸè¯„ä¼°:
```bash
--eval_data "/path/to/your/eval.json" \
--eval_steps 100  # æ¯ 100 æ­¥è¯„ä¼°ä¸€æ¬¡
```
å¯ç”¨åï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šåœ¨æŒ‡å®šæ­¥æ•°è¿›è¡Œè¿›è¡Œè¯„ä¼°ï¼Œå¹¶å°†ç»“æœè®°å½•åˆ° save_path ä¸‹çš„ jsonl æ–‡ä»¶ä¸­ã€‚


### 5.4 è¯„ä¼°æŒ‡æ ‡

#### 5.4.1 SRM è¯„ä¼°

- **Accuracy**: chosen æ ·æœ¬å¾—åˆ† > reject æ ·æœ¬çš„æ¯”ä¾‹ã€‚è¿™æ˜¯æˆ‘ä»¬å®éªŒä¸­çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚
- **Mean Reward Gap**: `mean(score_chosen - score_reject)`ã€‚
- **Score Distribution**: åˆ†ææ¨¡å‹å¯¹ chosen/rejected æ ·æœ¬çš„æ‰“åˆ†åˆ†å¸ƒã€‚

#### 5.4.2 GRM è¯„ä¼°

- **Ranking Accuracy**: é€šè¿‡è§£æç”Ÿæˆæ–‡æœ¬ä¸­çš„ `<answer>` æ ‡ç­¾ï¼Œè®¡ç®—å…¶ä¸çœŸå®åå¥½çš„ä¸€è‡´æ€§ã€‚

### 5.5 åŸºå‡†æµ‹è¯•
 
æˆ‘ä»¬åŸºäº `Qwen2.5-VL-3B` æ¨¡å‹è¿›è¡Œäº†ä¸€ç³»åˆ—åŸºå‡†æµ‹è¯•ã€‚è¯¦ç»†çš„è®¾ç½®å’Œç»“æœè¯·å‚è€ƒ **[ç« èŠ‚ 7: åŸºå‡†æµ‹è¯•ä¸æ€§èƒ½å‚è€ƒ](#7-åŸºå‡†æµ‹è¯•ä¸æ€§èƒ½å‚è€ƒ)**ã€‚

---

## 6. å¸¸è§é—®é¢˜

### 6.1 è®­ç»ƒé—®é¢˜

#### Q1: OOM (Out of Memory)

**è§£å†³æ–¹æ¡ˆï¼š**
1. **å‡å° batch size:** `--micro_train_batch_size 1`ã€‚
2. **å¯ç”¨ gradient checkpointing:** `--gradient_checkpointing`ã€‚
3. **ä½¿ç”¨æ›´é«˜çš„ ZeRO stage:** `--zero_stage 3 --adam_offload`ã€‚
4. **ä½¿ç”¨ LoRA:** `--lora_rank 32`ã€‚
5. **å‡å°åºåˆ—é•¿åº¦:** `--prompt_max_len 1024`ã€‚
6. **ä½¿ç”¨ BF16/FP16:** `--bf16`ã€‚

#### Q2: è®­ç»ƒä¸ç¨³å®š/Loss ä¸ä¸‹é™

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š**
1. **å­¦ä¹ ç‡ä¸åˆé€‚ï¼š** å¯¹äºå…¨é‡å¾®è°ƒï¼Œ`1e-5` æ˜¯ä¸€ä¸ªè¾ƒå¥½çš„èµ·ç‚¹ã€‚å¦‚æœæ¨¡å‹æ›´å¤§æˆ–æ•°æ®æ›´å°‘ï¼Œå¯å°è¯• `5e-6` æˆ– `1e-6`ã€‚
2. **æ•°æ®é—®é¢˜ï¼š** æ£€æŸ¥æ•°æ®æ¸…æ´—æ­¥éª¤æ˜¯å¦åˆ°ä½ï¼Œæ ‡ç­¾æ˜¯å¦å‡†ç¡®ã€‚
3. **æ¢¯åº¦çˆ†ç‚¸ï¼š** å°è¯•æ·»åŠ æ¢¯åº¦è£å‰ªã€‚
4. **Warmup ä¸è¶³ï¼š** ç¡®ä¿ `--lr_warmup_ratio` è®¾ç½®åˆç†ï¼ˆå¦‚ `0.05`ï¼‰ã€‚

### 6.2 æ¨ç†é—®é¢˜

#### Q3: å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†

æˆ‘ä»¬åœ¨ `examples/demo_srm_training/srm_vl_inference.py` å’Œ `examples/demo_grm_training/grm_vl_inference.py` æä¾›äº† SRM å’Œ GRM çš„æ¨ç†è„šæœ¬ç¤ºä¾‹ã€‚
è¯·å‚è€ƒè„šæœ¬ä¸­çš„ç”¨æ³•è¯´æ˜ï¼ŒæŒ‰éœ€ä¿®æ”¹æ¨¡å‹è·¯å¾„å’Œè¾“å…¥æ•°æ®ï¼Œå³å¯è¿›è¡Œæ¨ç†ã€‚

### 6.3 æ•°æ®é›†é—®é¢˜

#### Q4: å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒ

LightRFT ä½¿ç”¨ Data Handler æ¨¡å¼ï¼Œé€šè¿‡ä¸ºä¸åŒæ•°æ®é›†å®ç°ç›¸åº”çš„ Data Handler ç±»æ¥æ”¯æŒå¤šç§æ•°æ®é›†ï¼Œä»è€Œå®ç°çµæ´»çš„æ•°æ®åŠ è½½ä¸é¢„å¤„ç†ã€‚

##### å®ç°è‡ªå®šä¹‰çš„ Data Handler
1. **ç»§æ‰¿ BaseDataHandler**:
   åˆ›å»ºä¸€ä¸ªæ–°çš„ Python ç±»ï¼Œç»§æ‰¿è‡ª `lightrft.datasets.BaseDataHandler`ã€‚
2. **å®ç°å¿…è¦çš„æ–¹æ³•**:
    - `load_data`: ä»æ•°æ®é…ç½®æ–‡ä»¶ï¼ˆå¦‚ json, parquetç­‰ï¼‰æˆ– æ–‡ä»¶å¤¹ ä¸­åŠ è½½æ‰€æœ‰æ•°æ®é¡¹ã€‚è¿”å›åŸå§‹æ•°æ®é¡¹çš„åˆ—è¡¨ã€‚
    - `get_media_info`: ä»åŸå§‹æ•°æ®é¡¹ä¸­æå–æ‰€æœ‰åª’ä½“ä¿¡æ¯ï¼ˆå›¾ç‰‡ï¼Œè§†é¢‘ å’Œ éŸ³é¢‘ç­‰ï¼‰çš„è·¯å¾„ä¿¡æ¯ã€‚
    - `parse_item`: å°†åŸå§‹æ•°æ®é¡¹å’ŒåŠ è½½çš„è§†è§‰å†…å®¹è§£æä¸ºé€‚åˆæ¨¡å‹è¾“å…¥çš„æ ‡å‡†æ ¼å¼ï¼Œå¹¶è¿”å›åŒ…å«æ ‡ç­¾ç­‰å…¶ä»–å¿…è¦ä¿¡æ¯çš„å­—å…¸ã€‚
3. **æ³¨å†Œ Data Handler**:
    åœ¨ `lightrft.datasets` æ¨¡å—çš„ `srm_datset` æˆ–è€… `grm_dataset` ä¸­çš„ `self.handlers` å­—å…¸ä¸­æ³¨å†Œä½ çš„ Data Handler ç±»ã€‚

##### æ•°æ®æ ¼å¼

###### SRM æ•°æ®æ ¼å¼

**ä¸€ä¸ªå…¸å‹çš„æ ¼å¼ç¤ºä¾‹** (JSON Lines):

```json
{
  "prompt": "A beautiful sunset over the ocean",
  "image_0": "/path/to/image0.jpg",
  "image_1": "/path/to/image1.jpg",
  "preference": "A",
  "alignment": "B"
}
```

**æ ‡ç­¾è¯´æ˜ï¼š**
æˆ‘ä»¬ä½¿ç”¨å¦‚ä¸‹æ‰€ç¤ºçš„æ ‡ç­¾è¿›è¡Œåå¥½è®­ç»ƒã€‚
- `"A"`: image_0/response_0 æ›´å¥½
- `"B"`: image_1/response_1 æ›´å¥½
- `"C"`: ä¸¤è€…ç›¸å½“

###### GRM æ•°æ®æ ¼å¼

GRM çš„è®­ç»ƒæ•°æ®éœ€è¦åŒ…å«æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬è¯„ä»·ï¼Œé€šå¸¸åŒ…å«æ€ç»´é“¾å’Œæœ€ç»ˆç»“è®ºï¼Œä»¥ä¾¿åŸºäº Next-Token Prediction è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒè®­ç»ƒã€‚

**ä¸€ä¸ªæ•°æ®æ ¼å¼ç¤ºä¾‹** (JSON Lines):
```json
{
  "prompt": "Describe this image",
  "image_0": "/path/to/image0.jpg",
  "image_1": "/path/to/image1.jpg",
  "response": "<think>Reasoning here</think><answer>Image 1 is better</answer>"
}
```

é€šå¸¸ï¼Œ`response` ä¸­åŒ…å« `<think>` å’Œ `<answer>` æ ‡ç­¾ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ç”Ÿæˆç»“æ„åŒ–çš„è¯„ä»·å’Œæœ€ç»ˆåˆ¤æ–­ï¼ŒåŒæ—¶æ–¹ä¾¿è¿›è¡Œæ–‡æœ¬è§£æã€‚
æ‚¨ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦è®¾è®¡ä¸åŒçš„æ ‡ç­¾ä½“ç³»ã€‚

##### æ•°æ®é›†ç»„ç»‡

**æ¨èç›®å½•ç»“æ„ï¼š**
```
/data/reward_model/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ HPDv3/
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â”œâ”€â”€ test.json
â”‚   â”‚   â””â”€â”€ images/
â”‚   â”‚       â”œâ”€â”€ img_001.jpg
â”‚   â”‚       â”œâ”€â”€ img_002.jpg
â”‚   â”‚       â””â”€â”€ ... 
â”‚   â”‚   
â”‚   â”œâ”€â”€ ImageGen-CoT-Reward-5K/
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â””â”€â”€ images/
â”‚   â”‚       â”œâ”€â”€ img_001.jpg
â”‚   â”‚       â”œâ”€â”€ img_002.jpg
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ rapidata-text-2-video-human-preferences-pika2.2/
â”‚   â”‚   â”œâ”€â”€ train.parquet
â”‚   â”‚   â””â”€â”€ videos/
â”‚   â”‚       â”œâ”€â”€ vid_001.mp4
â”‚   â”‚       â”œâ”€â”€ vid_002.mp4
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â””â”€â”€ ...
```

##### æ•°æ®é¢„å¤„ç†

###### è§†è§‰æ•°æ®é¢„å¤„ç†

- å›¾åƒåº”è¯¥å­˜å‚¨ä¸º JPEG/PNG/webp æ ¼å¼
- å»ºè®®åˆ†è¾¨ç‡: 224x224 åˆ° 1024x1024
- è§†é¢‘å¸§ç‡: å®éªŒä¸­è®¾ç½®ä¸º 2.0 FPS (åœ¨é…ç½®ä¸­é€šè¿‡ --fps æŒ‡å®š)

###### æ•°æ®æ¸…æ´—

**å¿…é¡»æ£€æŸ¥ï¼š**
1. âœ… æ‰€æœ‰æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨
2. âœ… æ ‡ç­¾æ˜¯å¦åˆæ³• (A/B/C)
3. âœ… å›¾åƒæ˜¯å¦å¯è¯»
4. âœ… æ–‡æœ¬æ˜¯å¦åŒ…å«ç‰¹æ®Šå­—ç¬¦

**ç¤ºä¾‹è„šæœ¬ï¼š**
```python
import json
from pathlib import Path
from PIL import Image

def validate_dataset(json_path):
    with open(json_path) as f:
        for line_no, line in enumerate(f, 1):
            item = json.loads(line)
            # æ ¹æ®ä½ çš„æ•°æ®æ ¼å¼è¿›è¡Œæ£€æŸ¥...
            # ...
``` 
---

## 7. åŸºå‡†æµ‹è¯•ä¸æ€§èƒ½å‚è€ƒ

æœ¬ç« èŠ‚æä¾›äº†åŸºäº LightRFT æ¡†æ¶çš„ SRM å’Œ GRM æ¨¡å‹çš„åˆæ­¥å®éªŒç»“æœï¼Œå¯ä½œä¸ºè®­ç»ƒçš„æ€§èƒ½å‚è€ƒã€‚

### 7.1 Scalar Reward Model (SRM) å®éªŒ

#### 7.1.2 Text-to-Image (T2I) ä»»åŠ¡æ€§èƒ½

**å®éªŒè®¾ç½®**
- **åŸºç¡€æ¨¡å‹**: `Qwen2.5-VL-3B`
- **è®­ç»ƒæ–¹å¼**: å…¨é‡å¾®è°ƒ (Full fine-tuning)
- **Batch Size**: å…¨å±€ Batch Size `32`ï¼Œæ¯å¡ Micro Batch Size `4`
- **æœ€å¤§è®­ç»ƒè½®æ•°**: æ‰€æœ‰å®éªŒå‡è®­ç»ƒ `5` ä¸ª Epochã€‚æˆ‘ä»¬å– 2000 global step çš„æ£€æŸ¥ç‚¹è¿›è¡Œè¯„ä¼°ã€‚
- **å­¦ä¹ ç‡**: `1e-5`
- **Reward Head**: å•ä¸€ Preference Headï¼Œè¾“å‡ºæ•´ä½“åå¥½åˆ†æ•°
- **ç¡¬ä»¶**: åŒå¡ NVIDIA H200 (140GBx2)
- **ä»»åŠ¡æŒ‡ä»¤**:
    ```
    Your will act as an expert image evaluator for text-to-image generation.
    Given a text prompt and a generated image, your task is to assess the overall quality of the image in relation to the prompt.
    Your evaluation should focus on the following key aspects:
    â€¢ Preference: Which image would a human viewer find more satisfying or visually appealing overall.
    â€¢ Alignment: How well the image content matches the given text prompt in semantics, objects, and attributes.
    Your task is provided in the following, please give your judgement based on above criteria.
    The prompt used for generation is as follows: {prompt}.
    ```

**è®­ç»ƒæ•°æ®**:
- **HPDv3 Subset**: ä» HPDv3-Train ä¸­éšæœºæŠ½å–çš„çº¦ ~57K æ ·æœ¬å¯¹ã€‚åŸå§‹çš„ HPDv3 åŒ…å«çº¦ 1.17M æ ·æœ¬å¯¹ï¼Œè€ƒè™‘åˆ°èµ„æºé™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå­é›†è¿›è¡Œè®­ç»ƒã€‚

**è¯„ä¼°æ•°æ®**:
- **OmniReward-Bench-T2I**: OmniReward-Bench ä¸­çš„ Text-to-Image è¯„ä¼°å­é›†ã€‚
- **HPDv3 Test Set**: HPDv3 æ•°æ®é›†çš„æµ‹è¯•é›†éƒ¨åˆ†ã€‚
- **ImageRewardDB Test Set**: ImageRewardDB æ•°æ®é›†çš„æµ‹è¯•é›†éƒ¨åˆ†ã€‚

**æµ‹è¯•ç»“æœ**:
| æ¨¡å‹å˜ä½“ | æŸå¤±å‡½æ•° | Scale for Train | Pooling Method | Probing Layer | OmniReward-Bench-T2I (Acc) | HPDv3 Test (Acc) | ImageRewardDB (Acc) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **HPS** | HPS | No | attn | -1 | 31.83% | 53.84% | 41.93% |
| **BT** | BT | No | attn | -1 | 30.06% | 60.54% | 42.51% |
| **BT Scale** | BT | Yes | attn | -1 | 53.83% | 69.74% | 58.98% |
| **HPS Scale** | HPS | Yes | attn | -1 | 55.21% | **72.35%** | **61.37%** |
| **HPS + BT Scale** | HPS + BT | Yes | attn | -1 | **56.19%** | 68.86% | 59.48% |
| **HPS Scale Probing 17** | HPS + BT | Yes | attn | 17 | 55.21% | 71.4% | 57.37% |
| **HPS Scale Last** | HPS + BT | Yes | last | -1 | 48.92% | 70.10% | 59.12% |
| --- | --- | --- | --- | --- | --- | --- | --- |
| *Baseline: HPSv3 (Qwen2VL-7B)* | - | - | - | - | *76.9%* | *66.8%* | - |
| *Baseline: ImageReward (BLIP)* | - | - | - | - | *58.6%* | *65.1%* | - |

**è®­ç»ƒæ—¥å¿—**:
- **ä½¿ç”¨ä¸åŒæŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒçš„å¯¹æ¯”**:
  - HPS Loss (çº¢çº¿) vs. BT (ç°çº¿) vs. HPS Scale (æµ…è“çº¿) vs. BT Scale (æ·±è“çº¿)
    ![è®­ç»ƒæŸå¤±æ›²çº¿å›¾](./images/srm_t2i_train_loss.png)
    ![è¯„ä¼°å‡†ç¡®ç‡æ›²çº¿å›¾](./images/srm_t2i_prefer_acc_mean.png)
  - **è§‚å¯Ÿ**: 
    - å¯¹äº HPS å’Œ BT ä¸¤ç§æŸå¤±å‡½æ•°ï¼Œå¯ç”¨å¯å­¦ä¹ çš„ç¼©æ”¾ç³»æ•°ï¼ˆScale for Trainï¼‰æ˜¾è‘—æå‡äº†è®­ç»ƒç¨³å®šæ€§å’Œæœ€ç»ˆæ€§èƒ½ã€‚
    - åœ¨å¯ç”¨ Scale for Train çš„æƒ…å†µä¸‹ï¼ŒHPS å’Œ BT ä¸¤è€…çš„æŸå¤±å‡½æ•°æ›²çº¿åŸºæœ¬ä¸€è‡´ï¼Œä½†åœ¨è¯„ä¼°å‡†ç¡®ç‡ä¸Šï¼ŒHPS åŠ ä¸Š Scale æ˜¾è‘—ä¼˜äº BTã€‚

- **Pooling æ–¹æ³•å¯¹æ¯”**:
  - æˆ‘ä»¬åœ¨ HPS Scale çš„é…ç½®ä¸Šå¯¹ä¸åŒçš„ Pooling æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”å®éªŒã€‚
  - Attention Pooling (æµ…è“çº¿) vs. Last Token Pooling (ç²‰çº¢çº¿)
    ![Pooling æ–¹æ³•è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”å›¾](./images/srm_t2i_pooling_comparison.png)
  - **è§‚å¯Ÿ**: ä»è®­ç»ƒçš„æŸå¤±æ›²çº¿ä¸Šçœ‹ï¼Œä¸¤ç§ pooling æ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦ç›¸ä¼¼ï¼Œä½†åœ¨è¯„ä¼°å‡†ç¡®ç‡ä¸Šï¼ŒAttention Pooling æ˜æ˜¾ä¼˜äº Last Token Poolingã€‚

- **Probing Layer å¯¹æ¯”**:
  - æˆ‘ä»¬åœ¨ HPS Scale çš„é…ç½®ä¸Šå¯¹ä¸åŒçš„ Probing Layer è¿›è¡Œäº†å¯¹æ¯”å®éªŒã€‚
    ä½¿ç”¨æœ€åä¸€å±‚ã€€(æµ…è“çº¿) vs. ä¸­é—´å±‚ 17 (æ©™çº¿)
    ![Probing Layer è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”å›¾](./images/srm_t2i_probing_layer_comparison.png)
  - **è§‚å¯Ÿ**: ä¸¤ç§ Probing Layer çš„è®­ç»ƒæŸå¤±æ›²çº¿å‡è¾ƒä¸ºæ¥è¿‘ã€‚ä½†åœ¨è¯„ä¼°å‡†ç¡®ç‡ä¸Šï¼Œä½¿ç”¨æœ€åä¸€å±‚çš„ç‰¹å¾ä¼˜äºä¸­é—´å±‚ã€‚

**åˆ†æ•°åˆ†å¸ƒ**:
- æˆ‘ä»¬å¯è§†åŒ–äº†ä¸åŒæ¨¡å‹åœ¨åœ¨ OmniReward-Bench-T2I å’Œ HPDv3 Test æ•°æ®é›†ä¸Šçš„åˆ†æ•°åˆ†å¸ƒæƒ…å†µã€‚å¯ä»¥æ¯”è¾ƒæ˜æ˜¾åœ°çš„çœ‹åˆ°ï¼Œåœ¨ä¸å¯ç”¨ Scale for Train çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹å€¾å‘äºç»™å‡ºæç«¯çš„åˆ†æ•°ï¼ˆæ¥è¿‘ 0 æˆ– 1ï¼‰ï¼Œè€Œåœ¨å¯ç”¨ Scale for Train åï¼Œæ ·æœ¬çš„åˆ†æ•°çš„åˆ†å¸ƒæ›´åŠ å‡åŒ€ï¼Œ
- **HPDv3 Test æ•°æ®é›†ä¸Šçš„åˆ†æ•°åˆ†å¸ƒå›¾**:
  - ![HPS æ¨¡å‹åˆ†æ•°åˆ†å¸ƒå›¾](./images/hps_hpdv3_score_distribution.png)
  - ![HPS Scale æ¨¡å‹åˆ†æ•°åˆ†å¸ƒå›¾](./images/hpsscale_hpdv3_score_distribution.png)
  - ![BT æ¨¡å‹åˆ†æ•°åˆ†å¸ƒå›¾](./images/bt_hpdv3_score_distribution.png)
  - ![BT Scale æ¨¡å‹åˆ†æ•°åˆ†å¸ƒå›¾](./images/btscale_hpdv3_score_distribution.png)

- **OmniReward-Bench-T2I æ•°æ®é›†ä¸Šçš„åˆ†æ•°åˆ†å¸ƒå›¾**:
  - ![HPS æ¨¡å‹åˆ†æ•°åˆ†å¸ƒå›¾](./images/hps_omni_t2i_score_distribution.png)
  - ![HPS Scale æ¨¡å‹åˆ†æ•°åˆ†å¸ƒå›¾](./images/hpsscale_omni_t2i_score_distribution.png)
  - ![BT æ¨¡å‹åˆ†æ•°åˆ†å¸ƒå›¾](./images/bt_omni_t2i_score_distribution.png)
  - ![BT Scale æ¨¡å‹åˆ†æ•°åˆ†å¸ƒå›¾](./images/btscale_omni_t2i_score_distribution.png)

**ç»“è®ºä¸åˆ†æ**:
1.  **æŸå¤±å‡½æ•°é€‰æ‹©**: åœ¨ T2I ä»»åŠ¡ä¸Šï¼Œ**BT Loss** æ˜æ˜¾å¥½äº HPS Lossã€‚ ä½¿ç”¨å¯å­¦ä¹ æ¸©åº¦ç¼©æ”¾çš„ BT Lossï¼ˆBT Scaleï¼‰è¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ã€‚è€Œç»“åˆ HPS å’Œ BT çš„æ··åˆæŸå¤±ï¼ˆHPS + BT Scaleï¼‰åœ¨ OmniReward-Bench-T2I ä¸Šå–å¾—äº†æœ€ä½³ç»“æœï¼ˆ56.39%ï¼‰ã€‚
2.  **å¯å­¦ä¹ çš„ç¼©æ”¾ç³»æ•°**: å¯ç”¨ `Scale for Train` ï¼ˆå¯å­¦ä¹ çš„ç¼©æ”¾ç³»æ•°ï¼‰æ˜¾è‘—æå‡äº†æ¨¡å‹æ€§èƒ½ï¼Œè¡¨æ˜åŠ¨æ€è°ƒæ•´å¥–åŠ±åˆ†æ•°çš„åˆ†å¸ƒå¯¹äºè®­ç»ƒæ›´æœ‰æ•ˆçš„å¥–åŠ±æ¨¡å‹è‡³å…³é‡è¦ã€‚å…¶ä¸­ï¼Œ HPS Loss åœ¨å¯ç”¨ç¼©æ”¾åæå‡å°¤ä¸ºæ˜¾è‘—ï¼Œåœ¨ HPDv3 Test å’Œ ImageRewardDB ä¸Šåˆ†åˆ«è¾¾åˆ°äº† 72.35% å’Œ 61.37% çš„å‡†ç¡®ç‡ã€‚
2.  **Pooling æ–¹æ³•**: Attention Pooling æ˜æ˜¾ä¼˜äº Last Token Poolingï¼Œè¡¨æ˜å°†æ•´ä¸ªåºåˆ—çš„ token è¿›è¡ŒåŠ æƒèšåˆæœ‰åŠ©äºæå‡å¥–åŠ±æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚
3.  **Probing Layer é€‰æ‹©**: ä½¿ç”¨æœ€åä¸€å±‚ï¼ˆ-1ï¼‰å’Œ ä¸­é—´å±‚ï¼ˆ17ï¼‰çš„ç‰¹å¾ä½œä¸ºå¥–åŠ±å¤´çš„è¾“å…¥ï¼Œç»“æœè¡¨æ˜ä¸¤è€…çš„ç»“æœç›¸è¿‘ï¼Œä½†æœ€åä¸€å±‚ç•¥ä¼˜ã€‚
3.  **æ€§èƒ½å¯¹æ¯”**: æˆ‘ä»¬çš„ 3B æ¨¡å‹åœ¨ HPS Scale çš„é…ç½®ä¸‹ï¼Œ ä»…åœ¨ä» HPDv3 ä¸­æŠ½å–å¾—åˆ°çš„ä¸€ä¸ªå°å­é›†æ•°æ®ä¸Šè®­ç»ƒåï¼Œå°±èƒ½åœ¨ HPDv3 Test å’Œ ImageRewardDB ä¸Šåˆ†åˆ«è¾¾åˆ° 72.35% å’Œ 61.37% çš„å‡†ç¡®ç‡ï¼Œæ¥è¿‘æ­¤å‰åŸºäº 7B æ¨¡å‹çš„ SOTA ç»“æœï¼ˆåˆ†åˆ«ä¸º 76.9% å’Œ 66.8%ï¼‰ã€‚è¿™è¡¨æ˜ LightRFT æ¡†æ¶åœ¨è®­ç»ƒé«˜æ•ˆä¸”æ€§èƒ½ä¼˜å¼‚çš„å¥–åŠ±æ¨¡å‹æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

### 7.1.3 Text-to-Audio (T2A) ä»»åŠ¡æ€§èƒ½

æˆ‘ä»¬ä½¿ç”¨ T2I ä»»åŠ¡ä¸­è¡¨ç°æœ€ä¼˜çš„ HPS Scale é…ç½®ï¼Œè¿›è¡Œäº† T2A ä»»åŠ¡çš„åˆæ­¥å®éªŒã€‚

**å®éªŒè®¾ç½®**
- **è®­ç»ƒæ–¹å¼**: å…¨é‡å¾®è°ƒ (Full fine-tuning)
- **Batch Size**: å…¨å±€ Batch Size `32`ï¼Œæ¯å¡ Micro Batch Size `4`
- **æœ€å¤§è®­ç»ƒè½®æ•°**: æ‰€æœ‰å®éªŒå‡è®­ç»ƒ `5` ä¸ª Epochã€‚å– 500 global step çš„æ£€æŸ¥ç‚¹è¿›è¡Œè¯„ä¼°ã€‚
- **å­¦ä¹ ç‡**: `1e-5`
- **Reward Head**: å•ä¸€ Preference Headï¼Œè¾“å‡ºæ•´ä½“åå¥½åˆ†æ•°
- **ç¡¬ä»¶**: åŒå¡ NVIDIA H200 (140GBx2)
- **ä»»åŠ¡æŒ‡ä»¤**:
    ```
    You will act as an expert audio evaluator for text-to-audio generation.
    Given a text prompt and a generated audio clip, your task is to assess the overall quality of the audio in relation to the prompt.
    Your evaluation should focus on the following key aspects:
    â€¢ Preference: Which audio would a human listener find more satisfying or acoustically pleasing overall (considering audio fidelity, clarity, and musicality/naturalness).
    â€¢ Alignment: How well the audio content matches the given text prompt in semantics, sound events, mood, and acoustic attributes (e.g., genre, tempo, instruments).
    Your task is provided in the following, please give your judgement based on above criteria.
    The prompt used for generation is as follows: {prompt}.
    ```

**è®­ç»ƒæ•°æ®**: 
- **Audio-Alpca**ï¼šåŒ…å« 15K æ¡æ–‡æœ¬åˆ°éŸ³é¢‘çš„ç”Ÿæˆåå¥½æ•°æ®ã€‚

**è¯„ä¼°æ•°æ®**:
- **OmniReward-Bench-T2A**: OmniReward-Bench ä¸­çš„ Text-to-Audio è¯„ä¼°å­é›†ã€‚

**æµ‹è¯•ç»“æœ**:
| æ¨¡å‹å˜ä½“ | åŸºç¡€æ¨¡å‹ | è®­ç»ƒæ•°æ® | æŸå¤±å‡½æ•° | OmniReward-Bench-T2A (Acc) |
| :--- | :--- | :--- | :--- | :--- |
| **Qwen2.5-Omni-HPS** | Qwen2.5-Omni-3B | Audio-Alpca | HPS | 69.10% |
| **Qwen2-Audio-HPS** | Qwen2-Audio-7B | Audio-Alpca | HPS | *70.07%* |
| **MiniCPM_o-HPS** | MiniCPM-o 2.6 | Audio-Alpca | HPS | **70.32%** |
| --- | --- | --- | --- | --- |
| *Baseline: Qwen2.5-Omni-7B* | - | - | - | 50.76% |
| *Baseline: Gemini-2.0-Flash* | - | - | - | 60.86% |
| *Baseline: Gemini-2.5-Flash* | - | - | - | 60.10% |
| *Baseline: Gemini-2.5-Pro* | - | - | - | *65.41%* |
| *Baseline: Omini-RewardModel-BT* | - | - | BT | 66.41% |

**è®­ç»ƒæ—¥å¿—**:
- Qwen2.5-Omni-HPS (ç°çº¿) vs Qwen2-Audio-HPS (æ©™çº¿) vs MiniCPM_o-HPS (ç»¿çº¿)
- *è®­ç»ƒæŸå¤±æ›²çº¿å›¾* ![è®­ç»ƒæŸå¤±æ›²çº¿å›¾](./images/srm_t2a_train_loss.png)
- *è¯„ä¼°å‡†ç¡®ç‡æ›²çº¿å›¾* ![è¯„ä¼°å‡†ç¡®ç‡æ›²çº¿å›¾](./images/srm_t2a_train_acc.png)

**åˆ†æ•°åˆ†å¸ƒ**:
- æˆ‘ä»¬å¯è§†åŒ–äº†æ¨¡å‹åœ¨ OmniReward-Bench-T2A æ•°æ®é›†ä¸Šçš„åˆ†æ•°åˆ†å¸ƒæƒ…å†µã€‚
  - *Qwen2.5-Omni-HPS* ![Qwen2.5-Omni-HPS åˆ†æ•°åˆ†å¸ƒå›¾](./images/qwen2.5-omni-hps_t2a_score_distribution.png)
  - *Qwen2-Audio-HPS* ![Qwen2-Audio-HPS åˆ†æ•°åˆ†å¸ƒå›¾](./images/qwen2-audio-hps_t2a_score_distribution.png)
  - *MiniCPM_o-HPS* ![MiniCPM_o-HPS åˆ†æ•°åˆ†å¸ƒå›¾](./images/minicpm_o-hps_t2a_score_distribution.png)

**ç»“è®ºä¸åˆ†æ**:
- **æ€§èƒ½è¡¨ç°**: åˆæ­¥å®éªŒç»“æœè¡¨æ˜ï¼ŒLightRFT æ¡†æ¶èƒ½å¤Ÿæœ‰æ•ˆåº”ç”¨äº Text-to-Audioï¼ˆT2Aï¼‰ä»»åŠ¡ã€‚åœ¨å¤šç§ä¸åŒåŸºç¡€æ¨¡å‹ä¸Šï¼ŒåŸºäº `HPS Loss` è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹å‡å–å¾—äº†ç¨³å®šä¸”æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚å…¶ä¸­ï¼Œ`MiniCPM_o-HPS` åœ¨ `OmniReward-Bench-T2A` åŸºå‡†ä¸Šè¾¾åˆ°äº† **70.32%** çš„æœ€é«˜å‡†ç¡®ç‡ï¼Œå±•ç°å‡ºæœ€ä¼˜çš„æ•´ä½“è¡¨ç°ã€‚
- **è¶…è¶ŠåŸºçº¿**: ä¸ç°æœ‰å¼ºåŸºçº¿ç›¸æ¯”ï¼ŒLightRFT æ¡†æ¶ä¸‹è®­ç»ƒçš„æ¨¡å‹æ˜¾è‘—ä¼˜äºå•†ä¸šé€šç”¨æ¨¡å‹ `Gemini-2.5-Pro`ï¼ˆ65.41%ï¼‰ä»¥åŠä¸“é—¨è®¾è®¡çš„å¥–åŠ±æ¨¡å‹ `Omini-RewardModel-BT`ï¼ˆ66.41%ï¼‰ã€‚è¿™ä¸€ç»“æœéªŒè¯äº† LightRFT åœ¨éŸ³é¢‘å¥–åŠ±å»ºæ¨¡åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ã€‚
- **æ¡†æ¶é€šç”¨æ€§**: åœ¨ä¸åŒæ¨¡å‹æ¶æ„ï¼ˆ**Qwen2.5-Omniã€Qwen2-Audioã€MiniCPM-o**ï¼‰ä¸Šçš„ä¸€è‡´æ€§èƒ½æå‡ï¼Œè¿›ä¸€æ­¥è¡¨æ˜ LightRFT æ¡†æ¶å…·å¤‡è‰¯å¥½çš„é€šç”¨æ€§ï¼Œèƒ½å¤Ÿç¨³å®šæ”¯æŒéŸ³é¢‘â€“è¯­è¨€åå¥½å»ºæ¨¡ä»»åŠ¡ã€‚

#### 7.1.4 Text-to-Video (T2V) ä»»åŠ¡æ€§èƒ½

**å®éªŒè®¾ç½®**
- **åŸºç¡€æ¨¡å‹**: `Qwen2.5-VL-3B`
- **è®­ç»ƒæ–¹å¼**: å…¨é‡å¾®è°ƒ (Full fine-tuning)
- **Batch Size**: å…¨å±€ Batch Size `32`ï¼Œæ¯å¡ Micro Batch Size `4`
- **å­¦ä¹ ç‡**: `1e-5`
- **Reward Head**: å•ä¸€ Preference Headï¼Œè¾“å‡ºæ•´ä½“åå¥½åˆ†æ•°
- **Pooling Method**: Attention Pooling
- **Probing Layer**: æœ€åä¸€å±‚
- **Scale for Train**: å¯ç”¨
- **è§†é¢‘å¸§ç‡**: `2.0` FPS
- **ç¡¬ä»¶**: åŒå¡ NVIDIA H200 (140GB)
- **ä»»åŠ¡æŒ‡ä»¤**:
    ```
    Your will act as an expert video evaluator for text-to-video generation.
    Given a text prompt, and a generated video, your task is to assess the generated video on the following key aspects:
    â€¢ Preference: How visually appealing participants found each video, independent of the prompt.
    â€¢ Alignment: How well an video matches its prompt.
    â€¢ Coherence: Whether the generated video is logically consistent and free from artifacts or visual glitches.
    Your task is provided in the following. Please give your judgement based on above criteria.
    The prompt used for generation is as follows: {prompt}.
    ```

**è®­ç»ƒæ•°æ®**: 
- Rapidata-text-2-video-human-preferences-veo3
- Rapidata-text-2-video-human-preferences-pika2.2
- Rapidata-text-2-video-human-preferences-wan2.1

**è®­ç»ƒæ—¥å¿—**:
- *è®­ç»ƒæŸå¤±æ›²çº¿å›¾*
  ![è®­ç»ƒæŸå¤±æ›²çº¿å›¾](./images/t2v-rapidata-loss.png)
- *å‡†ç¡®ç‡æ›²çº¿å›¾*
  ![å‡†ç¡®ç‡æ›²çº¿å›¾](./images/t2v-rapidata-acc.png)

**æµ‹è¯•ç»“æœ**:

| æ¨¡å‹å˜ä½“ | æŸå¤±å‡½æ•° | OmniReward-Bench-T2V (Acc) |
| :--- | :--- | :--- |
| **BT Loss** (step 100) | BT | 59.74% |
| **HPS Loss** (step 100) | HPS | *62.19%* |
| --- | --- | --- |
| *Baseline: Omni-RewardModel-BT*| BT | **64.08%** |

**ç»“è®ºä¸åˆ†æ**:
- åˆæ­¥å®éªŒè¡¨æ˜ï¼ŒLignhtRFT æ¡†æ¶å¯ä»¥æ— ç¼æ‰©å±•åˆ°é’ˆå¯¹ T2V ä»»åŠ¡çš„æ ‡é‡å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œå¹¶åœ¨ `OmniReward-Bench-T2V` ä¸Šå–å¾—äº†çº¦ 62% çš„å‡†ç¡®ç‡, æ¥è¿‘ç°æœ‰çš„ SOTA åŸºçº¿ `Omni-RewardModel-BT` (64.08%)ã€‚

### 7.2 Generative Reward Model (GRM) å®éªŒ

#### 7.2.1 å®éªŒè®¾ç½®

- **åŸºç¡€æ¨¡å‹**: `Qwen2.5-VL-3B`
- **è®­ç»ƒæ–¹å¼**: å…¨é‡å¾®è°ƒ
- **å­¦ä¹ ç‡**: `1e-5`
- **æŸå¤±å‡½æ•°**: Next-Token Prediction Loss
- **è®­ç»ƒæ•°æ®**:
 - ImageGen-CoT-Reward-5K: åŒ…å«è¯¦ç»†çš„ CoT è¯„ä»·è¿‡ç¨‹ï¼Œç”¨äºè®­ç»ƒ Reasoning GRMã€‚
 - HPDv3 Train Subset: æ—  CoT æ ‡æ³¨ï¼Œé‡‡ç”¨ç›´æ¥è¾“å‡ºæ¯”è¾ƒç»“æœçš„å½¢å¼ï¼Œç”¨äºè®­ç»ƒä¸å¸¦ Reasoning è¿‡ç¨‹ çš„ GRMã€‚ï¼ˆè¿™é‡Œçš„ HPDv3 é‡‡ç”¨çš„æ˜¯ä¸ Scalar RM ç›¸åŒçš„å­é›†ï¼‰
- **Batch Size**: å…¨å±€ Batch Size `8`ï¼Œæ¯å¡ Micro Batch Size `4`
- **ç¡¬ä»¶**: åŒå¡ NVIDIA H200 (140GB)
- **ä»»åŠ¡æŒ‡ä»¤**:
    - å¯¹äº ImageGen-CoT-Reward-5Kï¼Œæˆ‘ä»¬ä½¿ç”¨å…¶è‡ªå¸¦çš„ CoT è¯„ä»·æŒ‡ä»¤ï¼›
    - å¯¹äº HPDv3ï¼Œæˆ‘ä»¬ä½¿ç”¨å¦‚ä¸‹ä»»åŠ¡æŒ‡ä»¤ï¼ŒæŒ‡å¯¼æ¨¡å‹ç”Ÿæˆæœ€ç»ˆçš„åå¥½åˆ¤æ–­ï¼š
    ```
    You will act as an expert image evaluator for text-to-image generation.
    Given a text prompt and two generated images, your task is to assess the overall quality of the images and determine which one is better.
    Your evaluation should focus on the following key aspects:
    â€¢ Preference: Which image would a human viewer find more satisfying or visually appealing overall.
    â€¢ Alignment: How well the image content matches the given text prompt in semantics, objects, and attributes.
    Your response must strictly follow the format with no extra text:
    <answer>Image 1 is better</answer>
    or
    <answer>Image 2 is better</answer>
    The task is provided below. Please give your judgment based on the above criteria.
    The prompt used for generation is as follows: {prompt}.
    ```

#### 7.2.2 Text-to-Image (T2I) ä»»åŠ¡æ€§èƒ½

**è®­ç»ƒæ—¥å¿—**:
- *ImageGen-CoT-Reward-5K è®­ç»ƒæŸå¤±æ›²çº¿å›¾*
  ![ImageGen-CoT-Reward-5K è®­ç»ƒæŸå¤±æ›²çº¿å›¾](./images/grm_imagegen_cot_reward_5k_train_loss.png)
- *HPDv3 Train Subset è®­ç»ƒæŸå¤±æ›²çº¿å›¾*
  ![HPDv3 Train Subset è®­ç»ƒæŸå¤±æ›²çº¿å›¾](./images/grm_hpdv3_train_subset_train_loss.png)

**æµ‹è¯•ç»“æœ**:

| æ¨¡å‹å˜ä½“ | è®­ç»ƒæ•°æ® | HPDv3 Test (Acc) | OmniReward-Bench-T2I (Acc) |
| :--- | :--- | :--- | :--- |
| **GRM (w/o reasoning)** (step 8000) | HPDv3 Train Subset (~56K) | **71.88%** | **59.33%** |
| **GRM (w/ reasoning)** (step 3000) | ImageGen-CoT-Reward-5K | 63.02% | 58.35% |

**ç»“è®ºä¸åˆ†æ**:
1.  **æ•°æ®è§„æ¨¡çš„å½±å“**: ä»ç»“æœä¸Šçœ‹ï¼Œ`GRM (w/o reasoning)` åœ¨ä¸¤ä¸ªæµ‹è¯•é›†ä¸Šå‡å–å¾—äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼ˆHPDv3 Test: 71.88% vs 63.02%ï¼‰ã€‚ä½†è¿™å¾ˆå¤§ç¨‹åº¦ä¸Šå½’å› äºè®­ç»ƒæ•°æ®çš„å·®å¼‚ï¼š`GRM (w/o reasoning)` ä½¿ç”¨äº†çº¦ 56K çš„ HPDv3 è®­ç»ƒæ•°æ®ï¼Œè€Œ `GRM (w/ reasoning)` ä»…ä½¿ç”¨äº† 5K çš„ ImageGen-CoT æ•°æ®ã€‚ç‰¹åˆ«æ˜¯åœ¨åŒæºçš„ HPDv3 Test ä¸Šï¼Œæ•°æ®é‡çš„ä¼˜åŠ¿ä½¿å¾— `GRM (w/o reasoning)` è¡¨ç°æ›´ä¸ºçªå‡ºã€‚
2.  **æ¨ç†èƒ½åŠ›çš„æ½œåŠ›**: å°½ç®¡è®­ç»ƒæ•°æ®é‡ä»…ä¸ºå‰è€…çš„ 1/10ï¼Œ`GRM (w/ reasoning)` ä¾ç„¶å±•ç°å‡ºäº†å…·æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ï¼ˆHPDv3 Test 63.02%ï¼‰ã€‚è¿™è¡¨æ˜å¼•å…¥æ€ç»´é“¾ï¼ˆCoTï¼‰æ¨ç†è¿‡ç¨‹èƒ½å¤Ÿå¸®åŠ©æ¨¡å‹åœ¨å°æ ·æœ¬ä¸‹æ›´æœ‰æ•ˆåœ°å­¦ä¹ åå¥½åˆ¤æ–­ã€‚
3.  **æœªæ¥æ”¹è¿›æ–¹å‘**: ç›®å‰ `GRM (w/ reasoning)` çš„æ€§èƒ½å—é™äºé«˜è´¨é‡ CoT æ•°æ®çš„ç¨€ç¼ºã€‚æœªæ¥çš„å·¥ä½œå¯ä»¥é›†ä¸­åœ¨æ„å»ºæ›´å¤§è§„æ¨¡ã€æ›´å¤šæ ·åŒ–çš„ CoT åå¥½æ•°æ®é›†ä¸Šï¼Œæˆ–è€…é€šè¿‡å¼ºåŒ–å¾®è°ƒæ¥è¿›ä¸€æ­¥é‡Šæ”¾æ¨ç†å‹å¥–åŠ±æ¨¡å‹çš„æ½œåŠ›ã€‚
---

## 8. è¿›é˜¶è¯é¢˜

### 8.1 å¤šä»»åŠ¡å­¦ä¹ 

å¯ä»¥åŒæ—¶è®­ç»ƒå¤šä¸ª reward head æ¥æ•è·ä¸åŒç»´åº¦çš„åå¥½ï¼š

```bash
--heads_types preference coherence alignment
```
æˆ‘ä»¬åœ¨ T2V ä»»åŠ¡ä¸­ï¼Œå°è¯•åœ¨ Rapidata-text-2-video-human-preferences-veo3 æ•°æ®é›†ä¸ŠåŒæ—¶è®­ç»ƒ `preference`ï¼Œ `coherence` å’Œ `alignment` ä¸‰ä¸ªç»´åº¦çš„ reward headã€‚åœ¨æ­¤å®éªŒä¸­ï¼Œå¤šä»»åŠ¡å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°äº†å¤šå¤´æŒ¤å å’Œæ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œå¯¼è‡´éƒ¨åˆ†å¤´çš„å¹¶æœªèƒ½æœ‰æ•ˆå­¦ä¹ åˆ°å¯¹åº”çš„åå¥½ä¿¡å·ã€‚

**åŸå› åˆ†æï¼š**
1. **ä»»åŠ¡éš¾åº¦ä¸å¹³è¡¡**ï¼šä¸åŒç»´åº¦çš„åå¥½åˆ¤æ–­éš¾åº¦ä¸åŒï¼ˆä¾‹å¦‚ `alignment` å¯èƒ½æ¯” `coherence` æ›´å®¹æ˜“åˆ¤æ–­ï¼‰ï¼Œå¯¼è‡´ç®€å•ä»»åŠ¡çš„ Loss å¿«é€Ÿä¸‹é™å¹¶ä¸»å¯¼äº†æ¢¯åº¦çš„æ–¹å‘ï¼Œè€Œå›°éš¾ä»»åŠ¡æœªèƒ½å¾—åˆ°å……åˆ†è®­ç»ƒã€‚
2. **æ¢¯åº¦å†²çª (Gradient Conflict)**ï¼šä¸åŒä»»åŠ¡å¯¹å…±äº«å‚æ•°ï¼ˆBackboneï¼‰çš„æ›´æ–°æ–¹å‘å¯èƒ½å­˜åœ¨å†²çªï¼Œå¯¼è‡´æŸäº›ä»»åŠ¡çš„æ¢¯åº¦è¢«æŠµæ¶ˆæˆ–æŠ‘åˆ¶ã€‚
3. **æ ‡ç­¾åˆ†å¸ƒå·®å¼‚**ï¼šä¸åŒç»´åº¦çš„æ ‡ç­¾åˆ†å¸ƒå¯èƒ½å­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´æ¨¡å‹å€¾å‘äºæ‹Ÿåˆå ä¸»å¯¼åœ°ä½çš„æ ‡ç­¾åˆ†å¸ƒã€‚

**æœªæ¥ç ”ç©¶æ–¹å‘ï¼š**
ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹å‘ï¼š
- **åŠ¨æ€æƒé‡è°ƒæ•´**ï¼šå¼•å…¥å¦‚ GradNorm ç­‰ç®—æ³•ï¼Œæ ¹æ®å„ä»»åŠ¡çš„å­¦ä¹ è¿›åº¦åŠ¨æ€è°ƒæ•´ Loss æƒé‡ï¼Œå¹³è¡¡ä»»åŠ¡é—´çš„è®­ç»ƒé€Ÿç‡ã€‚
- **æ¢¯åº¦ä¿®æ­£ç®—æ³•**ï¼šé‡‡ç”¨ PCGrad (Projecting Conflicting Gradients) ç­‰æ–¹æ³•ï¼Œå°†å†²çªçš„æ¢¯åº¦æŠ•å½±åˆ°æ³•çº¿å¹³é¢ï¼Œå‡å°‘ä»»åŠ¡é—´çš„å¹²æ‰°ã€‚

### 8.2 å¥–åŠ±æ¬ºéª— (Reward Hacking)

Reward Hacking çš„å‡ºç°çš„ä¸€ä¸ªå…³é”®åŸå› åœ¨äº **Reward Model ä»…ä»…æ˜¯äººç±»åå¥½çš„æ‹Ÿåˆä¸ä»£ç†ï¼Œè€Œéåå¥½æœ¬èº«**ã€‚ç”±äºè®­ç»ƒæ•°æ®çš„å±€é™æ€§å’Œæ¨¡å‹çš„æ³›åŒ–è¯¯å·®ï¼ŒRM å¾€å¾€å­˜åœ¨åˆ†å¸ƒå¤– (OOD) çš„ç›²åŒºã€‚å½“ Policy Model åœ¨ RL é˜¶æ®µé’ˆå¯¹ RM è¿›è¡Œå¼ºåŠ›ä¼˜åŒ–æ—¶ï¼Œå¾ˆå®¹æ˜“æ¢ç´¢åˆ°è¿™äº›ç›²åŒºï¼Œé€šè¿‡â€œé’»ç©ºå­â€çš„æ–¹å¼æ¥è·å–æé«˜çš„å¥–åŠ±åˆ†ï¼Œè€Œå®é™…ç”Ÿæˆè´¨é‡å´å¤§å¹…ä¸‹é™ã€‚

- **ç¼“è§£ç­–ç•¥**:
    - **ä½¿ç”¨ç”Ÿæˆå¼çš„ RM **: GRM æœ¬èº«å…·æœ‰ä¸€å®šçš„æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡ä½¿ç”¨æ›´å¤§çš„åŸºç¡€æ¨¡å‹å’Œæ›´ä¸°å¯Œçš„è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šæé«˜ RM çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘å¥–åŠ±æ¬ºéª—çš„é£é™©ã€‚
    - **å¤šç»´åº¦çº¦æŸ**: æˆ‘ä»¬å¯ä»¥è®­ç»ƒä¸€ä¸ªèƒ½è¾“å‡ºç»†ç²’åº¦å¤šç»´åº¦è¯„åˆ†çš„ RMï¼ˆå¦‚ Section 9.1 ä¸­ä½¿ç”¨çš„å¤šå¤´ RMï¼‰ï¼Œè¿™æ ·åœ¨ RL é˜¶æ®µå¯ä»¥å¯¹å¤šä¸ªç»´åº¦è¿›è¡Œçº¦æŸï¼Œå‡å°‘å•ä¸€ç»´åº¦è¿‡åº¦ä¼˜åŒ–çš„é£é™©ã€‚
    - **é›†æˆè¯„ä¼° (Ensemble)**: é€šè¿‡é›†æˆå¤šä¸ª RM çš„è¯„åˆ†ï¼Œå¯ä»¥å‡å°‘å•ä¸€æ¨¡å‹çš„ç›²åŒºå½±å“ï¼Œæé«˜æ•´ä½“è¯„ä¼°çš„é²æ£’æ€§ã€‚

### 8.3 å¼ºåŒ–å¾®è°ƒ (Reinforcement Fine-tuning, RFT) å¥–åŠ±æ¨¡å‹

æœ€è¿‘çš„ä¸€äº›å·¥ä½œï¼ˆUnifiedReward-Think, VisualQuality-R1 å’Œ ImageDoctor ç­‰ï¼‰ä¸­é‡‡ç”¨äº†å¼ºåŒ–å¾®è°ƒ (RFT) çš„æ–¹æ³•æ¥æå‡å¥–åŠ±æ¨¡å‹çš„æ€§èƒ½ã€‚é€šè¿‡åœ¨å°‘é‡ CoTï¼ˆChain-of-Thoughtï¼‰æ•°æ®ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼Œèµ‹äºˆå¥–åŠ±æ¨¡å‹ä¸€å®šçš„æ¨ç†æ¨¡å¼ï¼Œç„¶åå†é€šè¿‡æ‹’ç»é‡‡æ ·ï¼ˆRejective Samplingï¼‰å’Œ RL è¿›ä¸€æ­¥æå‡æ¨¡å‹çš„åˆ¤åˆ«èƒ½åŠ›ã€‚

- **ä¼˜åŠ¿**: RFT èƒ½å¤Ÿæå‡å¥–åŠ±æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œæ›´å¥½åœ°æ•æ‰äººç±»åå¥½ã€‚
- **æŒ‘æˆ˜**: RFT è¿‡ç¨‹å¤æ‚ï¼Œ æ¶‰åŠå¤šé˜¶æ®µçš„è®­ç»ƒï¼Œè®­ç»ƒæˆæœ¬è¾ƒé«˜ï¼›éœ€è¦è®¾è®¡åˆé€‚çš„å¥–åŠ±ä¿¡å·å’Œé‡‡æ ·ç­–ç•¥ã€‚

### 8.4 è¿‡ç¨‹å¥–åŠ±æ¨¡å‹

ä¼ ç»Ÿçš„ RM é€šå¸¸åªå¯¹æœ€ç»ˆç»“æœæ‰“åˆ†ã€‚å¯¹äºä¸€äº›å¯¹éœ€è¦å¤šè½®æˆ–é•¿åºåˆ—å†³ç­–çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ Agentic-RLï¼ŒAgent éœ€è¦è¿›è¡Œå¤šæ­¥æ¨ç†å’Œå·¥å…·è°ƒç”¨ï¼Œæ‰èƒ½è¾¾æˆæœ€ç»ˆç›®æ ‡ã€‚è€Œ ORM åªå¯¹æœ€ç»ˆç»“æœåˆ¤æ–­ï¼Œå¥–åŠ±ä¿¡å·ç¨€ç–ã€‚è€Œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ (Process Reward Model, PRM) åˆ™å°è¯•å¯¹æ¯ä¸€æ­¥çš„ä¸­é—´ç»“æœè¿›è¡Œæ‰“åˆ†ï¼Œä»è€Œæä¾›æ›´ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·ã€‚

- **ä¼˜åŠ¿**: æä¾›æ›´å¯†é›†çš„å¥–åŠ±ä¿¡å·ï¼Œè€Œéä»…ä»…ä¾èµ–æœ€ç»ˆç»“æœçš„åé¦ˆã€‚
- **æŒ‘æˆ˜**: æ ‡æ³¨æˆæœ¬é«˜ï¼Œéœ€è¦é’ˆå¯¹æ­¥éª¤çº§åˆ«çš„æ•°æ®ï¼›åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­å®šä¹‰â€œæ­¥éª¤â€å¯èƒ½æ›´ä¸ºå›°éš¾ã€‚
---

## 9. å‚è€ƒèµ„æº

### 9.1 è®ºæ–‡

1. **InstructGPT (2022)**: Training language models to follow instructions with human feedback
2. **DPO (2023)**: Direct Preference Optimization
3. **ImageReward (2023)**: ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation
4. **GenAI-Bench (2024)**: GenAI Arena: An Open Evaluation Platform for Generative Models
5. **HPsv3 (2025)**: HPSv3: Towards Wide-Spectrum Human Preference Score
6. **Omni-Reward (2025)**: Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences
7. **UnifiedReward-Think (2025)**: Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning
8. **VisualQuality-R1 (2025)**: VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank
9. **ImageDoctor (2025)**: ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning

### 9.2 ä»£ç ç¤ºä¾‹

- `examples/demo_srm_training/`: SRM è®­ç»ƒç¤ºä¾‹
- `examples/demo_grm_training/`: GRM è®­ç»ƒç¤ºä¾‹
- `lightrft/models/`: æ¨¡å‹å®ç°
- `lightrft/datasets/`: æ•°æ®é›†å®ç°
- `lightrft/trainer/`: è®­ç»ƒå™¨å®ç°

### 9.3 æ•°æ®é›†

1. **HPDv3**: https://huggingface.co/datasets/MizzenAI/HPDv3
2. **OmniRewardBench**: https://huggingface.co/datasets/HongbangYuan/OmniRewardBench
3. **ImageGen-CoT-Reward-5K**: https://huggingface.co/datasets/CodeGoat24/ImageGen-CoT-Reward-5K
4. **Rapidata**: https://huggingface.co/Rapidata/datasets
5. **ImageRewardDB**: https://huggingface.co/datasets/zai-org/ImageRewardDB

### 9.4 ç¤¾åŒºèµ„æº

- LightRFT GitHub Issues

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1 (with experimental results)
**æœ€åæ›´æ–°**: 2025-12-23
**ç»´æŠ¤è€…**: LightRFT Team
**è”ç³»æ–¹å¼**: opendilab@pjlab.org.cn

---

## è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®æ”¹è¿›å»ºè®®ã€bug æŠ¥å‘Šå’Œæ–°åŠŸèƒ½ï¼

1. Fork æœ¬é¡¹ç›®
2. åˆ›å»º feature åˆ†æ”¯
3. æäº¤ Pull Request
4. æ›´æ–°æ–‡æ¡£

ç¥è®­ç»ƒé¡ºåˆ©ï¼ ğŸš€
