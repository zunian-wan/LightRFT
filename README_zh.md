# LightRFT

<div align="center">

<img src="assets/logo.png" alt="LightRFT Logo" width="600"/>

**è½»é‡åŒ–ã€å…¨æ¨¡æ€å’Œå¥–åŠ±æ¨¡å‹é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¡†æ¶**

[![Version](https://img.shields.io/badge/version-0.1.0-blue.svg)](https://github.com/opendilab/lightrft)
[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-Apache%202.0-green.svg)](LICENSE)

[English](README.md) | ç®€ä½“ä¸­æ–‡

</div>

---

## ğŸ“– ç®€ä»‹

**LightRFT** (Light Reinforcement Fine-Tuning) æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¤šæ¨¡æ€å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ¡†æ¶ï¼Œä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è®¾è®¡ã€‚è¯¥æ¡†æ¶æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•çš„ RLVRï¼ˆReinforcement Learning with Verifiable Rewardsï¼‰ å’Œ RLHFï¼ˆReinforcement Learning from Human Feedbackï¼‰è®­ç»ƒèƒ½åŠ›ï¼Œæ”¯æŒå¤šç§å‰æ²¿ç®—æ³•å’Œåˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½æ¨ç†å¼•æ“**
  - é›†æˆ vLLM å’Œ SGLang ç”¨äºé«˜æ•ˆé‡‡æ ·å’Œæ¨ç†
  - æ”¯æŒ FP8 æ¨ç†ä¼˜åŒ–ï¼Œæ˜¾è‘—é™ä½å»¶è¿Ÿå’Œæ˜¾å­˜å ç”¨
  - çµæ´»çš„å¼•æ“ç¡çœ /å”¤é†’æœºåˆ¶ä¼˜åŒ–èµ„æºåˆ©ç”¨

- ğŸ§  **ä¸°å¯Œçš„ç®—æ³•ç”Ÿæ€**
  - **Policy Optimization**: GRPO, GSPO, GMPO, Dr.GRPO
  - **Advantage Estimation**: REINFORCE++, CPGD
  - **Reward Processing**: Reward Norm/Clip
  - **Sampling Strategy**: FIRE Sampling, Token-Level Policy
  - **Stability Enhancement**: DAPO, select_high_entropy_tokens

- ğŸ”§ **çµæ´»çš„è®­ç»ƒç­–ç•¥**
  - æ”¯æŒ FSDP (Fully Sharded Data Parallel) v2
  - æ”¯æŒ DeepSpeed ZeRO (Stage 1/2/3)
  - æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œæ··åˆç²¾åº¦è®­ç»ƒï¼ˆBF16/FP16ï¼‰
  - Adam Offload å’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯

- ğŸ¯ **åˆ›æ–°çš„èµ„æºååŒæœºåˆ¶**
  - **Colocate Anything**: å¥–åŠ±æ¨¡å‹ä¸è®­ç»ƒæ¨¡å‹ååŒå®šä½ï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡
    - æ”¯æŒå¤šä¸ªå¥–åŠ±æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Šå¹¶è¡Œæ¨ç†
    - åŠ¨æ€æ˜¾å­˜ç®¡ç†ï¼Œè®­ç»ƒ/æ¨ç†é˜¶æ®µè‡ªåŠ¨åˆ‡æ¢
    - å‡å°‘è·¨è®¾å¤‡é€šä¿¡å¼€é”€ï¼Œæå‡ç«¯åˆ°ç«¯è®­ç»ƒæ•ˆç‡
  - **Balance Anything** ğŸš§ (å¼€å‘ä¸­): æ™ºèƒ½è´Ÿè½½å‡è¡¡ç³»ç»Ÿ
    - è‡ªé€‚åº”ä»»åŠ¡è°ƒåº¦å’Œèµ„æºåˆ†é…
    - å¤šèŠ‚ç‚¹è®­ç»ƒè´Ÿè½½è‡ªåŠ¨å‡è¡¡
    - å¼‚æ„ç¡¬ä»¶ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–

- ğŸŒ **å…¨é¢çš„å¤šæ¨¡æ€æ”¯æŒ**
  - **åŸç”Ÿ Vision-Language Model (VLM) è®­ç»ƒ**
    - æ”¯æŒ Qwen-VL ç­‰ä¸»æµè§†è§‰è¯­è¨€æ¨¡å‹
    - å›¾åƒ-æ–‡æœ¬å¤šæ¨¡æ€æ•°æ®å¹¶è¡Œå¤„ç†
    - é«˜æ•ˆçš„å¤šæ¨¡æ€ tokenization å’Œæ‰¹å¤„ç†
  - **å¤šæ¨¡æ€å¥–åŠ±å»ºæ¨¡**
    - æ”¯æŒå¤šä¸ªè§†è§‰å¥–åŠ±æ¨¡å‹ååŒå·¥ä½œ
    - å›¾åƒç†è§£ä¸æ–‡æœ¬ç”Ÿæˆçš„è”åˆä¼˜åŒ–
  - **å®Œæ•´çš„è§†è§‰-è¯­è¨€å¯¹é½è®­ç»ƒæµç¨‹**
    - ä¸“ä¸ºå¤šæ¨¡æ€ RLVR/RLHF ä¼˜åŒ–
    - å†…ç½®è§†è§‰-è¯­è¨€æ¨¡å‹å¾®è°ƒæ”¯æŒ

- ğŸ“Š **å®Œæ•´çš„å®éªŒå·¥å…·é“¾**
  - Weights & Biases (W&B) é›†æˆ
  - æ•°å­¦èƒ½åŠ›åŸºå‡†æµ‹è¯•ï¼ˆGSM8K, Geo3K ç­‰ï¼‰
  - è½¨è¿¹ä¿å­˜å’Œåˆ†æå·¥å…·
  - è‡ªåŠ¨æ£€æŸ¥ç‚¹ç®¡ç†

---

## ğŸ¯ æ”¯æŒçš„ç®—æ³•

è¯¦ç»†ç®—æ³•è¯´æ˜ã€å®ç°ç»†èŠ‚å’Œä½¿ç”¨æŒ‡å—è¯·å‚è€ƒ [ç®—æ³•æ–‡æ¡£](docs/source/quick_start/algorithms_cn.md)ã€‚

| ç®—æ³• | ç±»å‹ | ä¸»è¦æ”¹è¿› | è®ºæ–‡é“¾æ¥ |
|------|------|----------|---------|
| **GRPO** | Policy Optimization | ç»„å½’ä¸€åŒ–ä¼˜åŠ¿ä¼°è®¡ |  [arXiv:2402.03300](https://arxiv.org/pdf/2402.03300)  |
| **GSPO** | Policy Optimization | å¹¿ä¹‰æ›¿ä»£ç›®æ ‡ | [arXiv:2507.18071](https://arxiv.org/abs/2507.18071) |
| **GMPO (WIP)** | Policy Optimization | å¹¿ä¹‰é•œåƒç­–ç•¥ä¼˜åŒ– | [arXiv:2507.20673](https://arxiv.org/abs/2507.20673) |
| **Dr.GRPO** | Policy Optimization | ç¼“è§£é•¿åº¦åå·® | [arXiv:2503.20783](https://arxiv.org/abs/2503.20783) |
| **REINFORCE++** | Advantage Estimation | æ”¹è¿›åŸºçº¿ä¼°è®¡ | [arXiv:2501.03262](https://arxiv.org/abs/2501.03262) |
| **DAPO** | Policy Optimization | è§£è€¦å‰ªè£å’ŒåŠ¨æ€é‡‡æ ·ç­–ç•¥ä¼˜åŒ– | [arXiv:2503.14476](https://arxiv.org/abs/2503.14476) |
| **CPGD** | Advantage Estimation | KLæ¼‚ç§»çº¦æŸ | [arXiv:2505.12504](https://arxiv.org/abs/2505.12504) |
| **FIRE Sampling** | Sampling Strategy | è¿‡æ»¤ä¸æ’åºç­–ç•¥ | [arXiv:2410.21236](https://arxiv.org/abs/2410.21236) |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python >= 3.10
- CUDA >= 12.8
- PyTorch >= 2.5.1

### Docker é•œåƒ

TO BE DONE

### å®‰è£…æ­¥éª¤

å…‹éš†å¹¶å®‰è£… LightRFT:

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/opendilab/LightRFT.git
cd LightRFT

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£… LightRFT
pip install -e .
```


---

## ğŸ“š ä½¿ç”¨æŒ‡å—

### åŸºç¡€ç¤ºä¾‹ï¼šGRPO è®­ç»ƒ

```bash
# å•èŠ‚ç‚¹ 8 GPU è®­ç»ƒç¤ºä¾‹
cd LightRFT

# è¿è¡Œ GRPO è®­ç»ƒ (GSM8K æ•°å­¦æ¨ç†ä»»åŠ¡)
bash examples/gsm8k_geo3k/run_grpo_gsm8k_qwen2.5_0.5b.sh

# æˆ–è€…è¿è¡Œ Geo3K å‡ ä½•é—®é¢˜è®­ç»ƒ (VLM å¤šæ¨¡æ€)
bash examples/gsm8k_geo3k/run_grpo_geo3k_qwen2.5_vl_7b.sh
```

---

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
LightRFT/
â”œâ”€â”€ lightrft/                      # æ ¸å¿ƒåº“
â”‚   â”œâ”€â”€ strategy/                  # è®­ç»ƒ&æ¨ç†ç­–ç•¥
â”‚   â”‚   â”œâ”€â”€ fsdp/                  # FSDP å®ç°
â”‚   â”‚   â”œâ”€â”€ deepspeed/             # DeepSpeed å®ç°
â”‚   â”‚   â”œâ”€â”€ vllm_utils/            # vLLM å·¥å…·
â”‚   â”‚   â””â”€â”€ sglang_utils/          # SGLang å·¥å…·
â”‚   â”œâ”€â”€ models/                    # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ actor_language.py      # è¯­è¨€æ¨¡å‹ Actor
â”‚   â”‚   â”œâ”€â”€ actor_vl.py            # è§†è§‰è¯­è¨€æ¨¡å‹ Actor
â”‚   â”‚   â””â”€â”€ monkey_patch/          # æ¨¡å‹é€‚é…è¡¥ä¸
â”‚   â”œâ”€â”€ trainer/                   # è®­ç»ƒå™¨å®ç°
â”‚   â”‚   â”œâ”€â”€ ppo_trainer.py         # PPO è®­ç»ƒå™¨
â”‚   â”‚   â”œâ”€â”€ ppo_trainer_vl.py      # VLM PPO è®­ç»ƒå™¨
â”‚   â”‚   â”œâ”€â”€ fast_exp_maker.py      # ç»éªŒç”Ÿæˆå™¨
â”‚   â”‚   â”œâ”€â”€ experience_maker.py    # åŸºç¡€ç»éªŒç”Ÿæˆå™¨
â”‚   â”‚   â”œâ”€â”€ experience_maker_vl.py # VLM ç»éªŒç”Ÿæˆå™¨
â”‚   â”‚   â””â”€â”€ spmd_ppo_trainer.py    # SPMD PPO è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ datasets/                  # æ•°æ®é›†å¤„ç†
â”‚   â””â”€â”€ utils/                     # å·¥å…·å‡½æ•°
â”‚       â””â”€â”€ ckpt_scripts/          # æ£€æŸ¥ç‚¹å¤„ç†è„šæœ¬
â”‚
â”œâ”€â”€ examples/                      # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ gsm8k_geo3k/               # GSM8K/Geo3K æ•°å­¦æ¨ç†è®­ç»ƒç¤ºä¾‹
â”‚   â”œâ”€â”€ grm_training/              # ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹è®­ç»ƒç¤ºä¾‹
â”‚   â”œâ”€â”€ srm_training/              # æ ‡é‡å¥–åŠ±æ¨¡å‹è®­ç»ƒç¤ºä¾‹
â”‚   â”œâ”€â”€ chat/                      # æ¨¡å‹å¯¹è¯ç¤ºä¾‹
â”‚
â”œâ”€â”€ docs/                          # ğŸ“š Sphinx æ–‡æ¡£
â”‚   â””â”€â”€ source/
â”‚       â”œâ”€â”€ installation/          # å®‰è£…æŒ‡å—
â”‚       â”œâ”€â”€ quick_start/           # å¿«é€Ÿå¼€å§‹ & ç”¨æˆ·æŒ‡å—
â”‚       â”‚   â”œâ”€â”€ algorithms.md      # ç®—æ³•æ–‡æ¡£ï¼ˆè‹±æ–‡ï¼‰
â”‚       â”‚   â”œâ”€â”€ algorithms_cn.md   # ç®—æ³•æ–‡æ¡£ï¼ˆä¸­æ–‡ï¼‰
â”‚       â”‚   â””â”€â”€ configuration.md   # é…ç½®å‚æ•°å‚è€ƒ
â”‚       â””â”€â”€ best_practice/         # æœ€ä½³å®è·µ & èµ„æº
â”‚           â”œâ”€â”€ strategy_usage.rst   # è®­ç»ƒç­–ç•¥ä½¿ç”¨æŒ‡å—ï¼ˆè‹±æ–‡ï¼‰
â”‚           â”œâ”€â”€ strategy_usage_zh.md # è®­ç»ƒç­–ç•¥ä½¿ç”¨æŒ‡å—ï¼ˆä¸­æ–‡ï¼‰
â”‚           â”œâ”€â”€ faq.md              # å¸¸è§é—®é¢˜
â”‚           â”œâ”€â”€ troubleshooting.md  # é—®é¢˜æ’æŸ¥æŒ‡å—
â”‚           â””â”€â”€ contributing.md     # è´¡çŒ®æŒ‡å—
â”‚
â”œâ”€â”€ assets/                        # èµ„æºæ–‡ä»¶
â”‚   â””â”€â”€ logo.png                   # é¡¹ç›®Logo
â”‚
â”œâ”€â”€ results/                       # è®­ç»ƒç»“æœ
â”œâ”€â”€ rft_logs/                      # è®­ç»ƒæ—¥å¿—
â””â”€â”€ README.md                      # é¡¹ç›®æ–‡æ¡£
```

### ğŸ”‘ å…³é”®ç›®å½•è¯´æ˜

- **`lightrft/`**: LightRFT æ ¸å¿ƒåº“ï¼Œæä¾›è®­ç»ƒç­–ç•¥ã€æ¨¡å‹å®šä¹‰å’Œè®­ç»ƒå™¨å®ç°
- **`examples/`**: å®Œæ•´çš„è®­ç»ƒç¤ºä¾‹å’Œè„šæœ¬
  - `gsm8k_geo3k/`: GSM8Kå’ŒGeo3Kæ•°å­¦æ¨ç†è®­ç»ƒç¤ºä¾‹
  - `grm_training/`: ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹è®­ç»ƒç¤ºä¾‹
  - `srm_training/`: æ ‡é‡å¥–åŠ±æ¨¡å‹è®­ç»ƒç¤ºä¾‹
  - `chat/`: æ¨¡å‹å¯¹è¯ç¤ºä¾‹
- **`docs/`**: Sphinxæ–‡æ¡£ï¼ŒåŒ…å«å®Œæ•´çš„ä½¿ç”¨æŒ‡å—å’ŒAPIæ–‡æ¡£

---

## âš™ï¸ å…³é”®é…ç½®å‚æ•°

### æ‰¹æ¬¡å¤§å°é…ç½®

```bash
TBS=128                           # è®­ç»ƒæ‰¹æ¬¡å¤§å°
RBS=128                           # Rollout æ‰¹æ¬¡å¤§å°
micro_train_batch_size=1          # æ¯å¼ å¡çš„å¾®æ‰¹æ¬¡å¤§å°
micro_rollout_batch_size=2        # Rollout å¾®æ‰¹æ¬¡å¤§å°
```

### ç®—æ³•å‚æ•°

```bash
--advantage_estimator group_norm  # ä¼˜åŠ¿ä¼°è®¡å™¨ï¼šgroup_norm, reinforce, cpgd
--n_samples_per_prompt 8          # æ¯ä¸ªæç¤ºé‡‡æ ·æ•°é‡
--max_epochs 1                    # æ¯ä¸ªepisodeçš„è®­ç»ƒè½®æ•°
--num_episodes 3                  # æ€»è®­ç»ƒè½®æ•°
--kl_estimator k3                 # KL ä¼°è®¡å™¨ç±»å‹
--init_kl_coef 0.001              # KL æƒ©ç½šç³»æ•°
```

### åˆ†å¸ƒå¼è®­ç»ƒ

```bash
--fsdp                            # å¯ç”¨ FSDP
--zero_stage 3                    # DeepSpeed ZeRO Stage
--gradient_checkpointing          # æ¢¯åº¦æ£€æŸ¥ç‚¹
--adam_offload                    # Adam ä¼˜åŒ–å™¨å¸è½½
--bf16                            # BF16 æ··åˆç²¾åº¦
```

### æ¨ç†å¼•æ“

```bash
--rm_use_engine                   # ä½¿ç”¨æ¨ç†å¼•æ“ï¼ˆvLLM/SGLangï¼‰
--engine_mem_util 0.4             # å¼•æ“æ˜¾å­˜åˆ©ç”¨ç‡
--engine_tp_size 1                # å¼•æ“å¼ é‡å¹¶è¡Œåº¦
--enable_engine_sleep             # å¯ç”¨å¼•æ“ç¡çœ æœºåˆ¶
```

---

## ğŸ”§ å¸¸è§é—®é¢˜æ’æŸ¥


è¯¦ç»†è¯´æ˜è§è®­ç»ƒè„šæœ¬ä¸­çš„å‚æ•°éªŒè¯é€»è¾‘ã€‚

### 1. OOM (æ˜¾å­˜ä¸è¶³)

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡å° `micro_train_batch_size` å’Œ `micro_rollout_batch_size`
- å¯ç”¨ `--gradient_checkpointing`
- é™ä½ `--engine_mem_util`
- ä½¿ç”¨ ZeRO Stage 3

### 2. è®­ç»ƒä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å¯ç”¨ Reward Normalization: `--normalize_reward`
- é™ä½å­¦ä¹ ç‡
- ä½¿ç”¨ `--advantage_estimator group_norm`
- å°è¯• DAPO ç®—æ³•


## ğŸ“– æ–‡æ¡£

### ğŸ“š å®Œæ•´æ–‡æ¡£æŒ‡å—

**å¿«é€Ÿå¼€å§‹ï¼š**
- [å®‰è£…æŒ‡å—](docs/source/installation/index_cn.rst) - Docker é•œåƒã€å®‰è£…æ–¹æ³•å’Œé—®é¢˜æ’æŸ¥
- [æ”¯æŒçš„ç®—æ³•](docs/source/quick_start/algorithms_cn.md) - è¯¦ç»†ç®—æ³•æŒ‡å—åŠå®ç°ç»†èŠ‚
- [é…ç½®å‚æ•°å‚è€ƒ](docs/source/quick_start/configuration.md) - å®Œæ•´å‚æ•°æ–‡æ¡£

**æœ€ä½³å®è·µï¼š**
- [è®­ç»ƒç­–ç•¥ä½¿ç”¨](docs/source/best_practice/strategy_usage_zh.md) - FSDPã€DeepSpeed å’Œæ¨ç†å¼•æ“é…ç½®
- [å¸¸è§é—®é¢˜](docs/source/best_practice/faq.md) - å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ
- [é—®é¢˜æ’æŸ¥æŒ‡å—](docs/source/best_practice/troubleshooting.md) - å¸¸è§é—®é¢˜å’Œè°ƒè¯•æ–¹æ³•
- [è´¡çŒ®æŒ‡å—](docs/source/best_practice/contributing.md) - å¦‚ä½•ä¸º LightRFT åšè´¡çŒ®

### æœ¬åœ°æ„å»ºæ–‡æ¡£

å®‰è£…æ–‡æ¡£ä¾èµ–ï¼š
```bash
pip install -r requirements-doc.txt
```

ç”Ÿæˆ HTML æ–‡æ¡£ï¼š
```bash
make docs
# æ‰“å¼€ docs/build/index.html æŸ¥çœ‹æ–‡æ¡£
```

å®æ—¶é¢„è§ˆæ–‡æ¡£ï¼š
```bash
make docs-live
# è®¿é—® http://localhost:8000
```


## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

### ä»£ç è§„èŒƒ

```bash
# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements-dev.txt

# ä»£ç æ ¼å¼åŒ–ï¼ˆYAPFï¼‰
yapf -i -r lightrft/

# ä»£ç æ£€æŸ¥ï¼ˆPylintï¼‰
pylint lightrft/
```

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™ è‡´è°¢

**LightRFT æ˜¯åŸºäº [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) å¼€å‘çš„ã€‚** æˆ‘ä»¬å‘ OpenRLHF å›¢é˜Ÿçš„æ°å‡ºå·¥ä½œè¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ã€‚æœ¬é¡¹ç›®ä¸­çš„éƒ¨åˆ†æ–‡ä»¶å’Œå®ç°æ˜¯ä» OpenRLHF æ”¹ç¼–å’Œå¤ç”¨çš„ã€‚

### åˆä½œå•ä½

æœ¬é¡¹ç›®æ˜¯ä¸**ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤ç³»ç»Ÿå¹³å°ä¸­å¿ƒ**å’Œ**å®‰å…¨å¯ä¿¡AIä¸­å¿ƒ**çš„åŒäº‹åˆä½œå¼€å‘ï¼Œæˆ‘ä»¬å‘å…¶è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ã€‚

### å¼€æºä¾èµ–

æœ¬é¡¹ç›®ä¾æ‰˜äºä»¥ä¸‹ä¼˜ç§€çš„å¼€æºé¡¹ç›®ï¼ˆåŒ…æ‹¬ä½†ä¸é™äºï¼‰:

- **[OpenRLHF](https://github.com/OpenRLHF/OpenRLHF)**ã€**[verl](https://github.com/volcengine/verl)** - æ ¸å¿ƒ RL æ¡†æ¶åŸºç¡€ï¼ˆéƒ¨åˆ†å…³é”®ç»„ä»¶æ”¹é€ å’Œå¤ç”¨ï¼‰
- [vLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½æ¨ç†å¼•æ“
- [SGLang](https://github.com/sgl-project/sglang) - ç»“æ„åŒ–ç”Ÿæˆè¯­è¨€è¿è¡Œæ—¶
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–
- [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html) - å…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ

æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…å’Œæ”¯æŒè€…ï¼

---

## ğŸ—“ï¸ RoadMap

æˆ‘ä»¬æ­£åœ¨è¿›è¡Œä»¥ä¸‹æ”¹è¿›å’ŒåŠŸèƒ½å¼€å‘ï¼š

### æ ¸å¿ƒåŠŸèƒ½å¢å¼º

- [ ] **Trajectory åŠŸèƒ½æ‰©å±•**
  - æ–°å¢æ›´å¤šåˆ†ææŒ‡æ ‡
  - å¢å¼ºè½¨è¿¹ä¿å­˜å’Œåˆ†æèƒ½åŠ›

- [ ] **Reward æœºåˆ¶é‡æ„**
  - é‡æ„ rule-based å’Œ model-based reward è®¡ç®—
  - ä¼˜åŒ– reward dataset å¤„ç†æµç¨‹

### ç®—æ³•ä¼˜åŒ–ä¸é›†æˆ

- [ ] **æ›´å¤šç®—æ³•æ”¯æŒ**
  - Entropy-based token selection
  - GMPO (Generalized Mirror Policy Optimization)
  - GSPO (Generalized Surrogate Policy Optimization)

- [ ] **Advantage è®¡ç®—é‡æ„**
  - ä¼˜åŒ– advantage estimation æ¨¡å—æ¶æ„
  - ç»Ÿä¸€ä¸åŒç®—æ³•çš„ advantage è®¡ç®—æ¥å£

- [ ] **Loss-Filter æœºåˆ¶ä¼˜åŒ–**
  - é‡æ„ loss filtering å®ç°
  - å®Œæˆ GSM8K/Geo3K åŸºå‡†æµ‹è¯•
  - å®éªŒç»“æœè®°å½•å’Œåˆ†æ


æ¬¢è¿ç¤¾åŒºè´¡çŒ®å’Œåé¦ˆï¼

---

## ğŸ“® è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- **Issues**: [GitHub Issues](https://github.com/yourusername/lightrft/issues)
- **é‚®ä»¶**: opendilab@pjlab.org.cn

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼**

Made with â¤ï¸ by LightRFT Team

</div>
