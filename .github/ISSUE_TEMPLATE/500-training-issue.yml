name: ğŸ‹ï¸ Training Issue
description: Report issues specifically related to the training process (OOM, convergence, etc.)
title: "[Training]: "
labels: ["training"]

body:
- type: markdown
  attributes:
    value: >
      #### Before submitting an issue, please make sure the issue hasn't been already addressed by searching through [the existing and past issues](https://github.com/opendilab/LightRFT/issues?q=is%3Aissue+sort%3Acreated-desc+).

- type: markdown
  attributes:
    value: |
      âš ï¸ **SECURITY WARNING:** Please review any text you paste to ensure it does not contain sensitive information such as:
      - API tokens or keys (e.g., Hugging Face tokens, W&B API keys)
      - Passwords or authentication credentials
      - Private URLs or endpoints
      - Personal or confidential data

- type: dropdown
  id: issue-type
  attributes:
    label: Training Issue Type
    description: What type of training issue are you experiencing?
    options:
      - Out of Memory (OOM)
      - Training Instability / NaN Loss
      - Poor Convergence
      - Slow Training Speed
      - Checkpoint Loading/Saving Issue
      - Distributed Training Issue (FSDP/DeepSpeed)
      - Inference Engine Issue (vLLM/SGLang)
      - Reward Model Issue
      - Data Loading Issue
      - Other
  validations:
    required: true

- type: textarea
  attributes:
    label: Environment Information
    description: |
      Please provide detailed environment information:
    value: |
      - LightRFT Version (git commit hash):
      - Python Version:
      - PyTorch Version:
      - CUDA Version:
      - GPU Model and Memory:
      - Number of GPUs:
      - Number of Nodes:
      - Operating System:
      - Distributed Backend (FSDP/DeepSpeed/Both):
      - DeepSpeed ZeRO Stage (if applicable):
      - Inference Engine (vLLM/SGLang/None):
      - Engine Version (if applicable):
  validations:
    required: true

- type: textarea
  attributes:
    label: Training Configuration
    description: |
      Please provide your complete training configuration.

      This is crucial for diagnosing training issues!
    render: bash
    value: |
      # Model Configuration
      MODEL_NAME_OR_PATH=
      REWARD_MODEL_PATH=

      # Algorithm Configuration
      ALGORITHM=  # GRPO, GSPO, etc.
      ADVANTAGE_ESTIMATOR=  # group_norm, reinforce, cpgd
      N_SAMPLES_PER_PROMPT=

      # Batch Size Configuration
      TRAIN_BATCH_SIZE=
      ROLLOUT_BATCH_SIZE=
      MICRO_TRAIN_BATCH_SIZE=
      MICRO_ROLLOUT_BATCH_SIZE=

      # Training Configuration
      MAX_EPOCHS=
      NUM_EPISODES=
      LEARNING_RATE=
      MAX_LEN=
      MAX_NEW_TOKENS=

      # Distributed Training
      FSDP=  # yes/no
      ZERO_STAGE=  # 1/2/3
      GRADIENT_CHECKPOINTING=  # yes/no
      BF16=  # yes/no

      # Inference Engine
      RM_USE_ENGINE=  # yes/no
      ENGINE_MEM_UTIL=
      ENGINE_TP_SIZE=

      # Full training command:
      # (paste your full command here)
  validations:
    required: true

- type: textarea
  attributes:
    label: Issue Description
    description: |
      Please describe the training issue in detail:
      - What happens during training?
      - When does the issue occur (immediate, after N steps/episodes)?
      - What have you tried to fix it?
    placeholder: |
      Training crashes after X steps with OOM error...
      Or: Loss becomes NaN after episode 2...
      Or: Training is very slow, processing only X samples/second...
  validations:
    required: true

- type: textarea
  attributes:
    label: Error Message / Logs
    description: |
      Please provide the full error message and relevant logs.

      For OOM issues: Include memory usage statistics if available
      For NaN losses: Include loss curves or W&B logs
      For slow training: Include throughput statistics
    render: shell
    placeholder: |
      Paste error messages and logs here

- type: textarea
  attributes:
    label: GPU Memory Usage
    description: |
      If relevant (especially for OOM issues), please provide GPU memory statistics:
      - Output of `nvidia-smi` before/during training
      - Peak memory usage per GPU
      - Memory allocation breakdown if available
    render: shell
    placeholder: |
      ```
      nvidia-smi output here
      ```

- type: textarea
  attributes:
    label: Steps Already Tried
    description: |
      What have you already tried to resolve this issue?
    placeholder: |
      - Tried reducing micro_batch_size from X to Y
      - Enabled gradient_checkpointing
      - Tried different zero_stage
      - ...

- type: textarea
  attributes:
    label: Expected Behavior
    description: |
      What did you expect to happen?
    placeholder: |
      Training should complete successfully without OOM...
      Or: Loss should converge smoothly...

- type: textarea
  attributes:
    label: Additional Context
    description: |
      Any other relevant information:
      - W&B run links
      - Training curves/plots
      - Comparison with working configurations
      - Related issues

- type: markdown
  attributes:
    value: >
      ğŸ’¡ **Tip:** Check the [troubleshooting guide](https://github.com/opendilab/LightRFT/tree/main/docs/source/best_practice/troubleshooting.md) for common solutions to training issues!

- type: markdown
  attributes:
    value: >
      Thanks for reporting ğŸ™!

- type: checkboxes
  id: confirmation
  attributes:
    label: Before submitting a new issue...
    options:
      - label: I have checked the [troubleshooting guide](https://github.com/opendilab/LightRFT/tree/main/docs/source/best_practice/troubleshooting.md) and [FAQ](https://github.com/opendilab/LightRFT/tree/main/docs/source/best_practice/faq.md).
        required: true
      - label: I have searched for similar training issues in existing issues.
        required: true
