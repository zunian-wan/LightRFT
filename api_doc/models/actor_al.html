


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lightrft.models.actor_al &mdash; LightRFT v0.1.1 documentation</title>
  

  <link rel="shortcut icon" href="../../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/readthedocs.css" type="text/css" />
  <link rel="index" title="Index" href="../../genindex.html" />
  <link rel="search" title="Search" href="../../search.html" />
  <link rel="next" title="lightrft.models.actor_language" href="actor_language.html" />
  <link rel="prev" title="lightrft.models" href="index.html" />
  <!-- Google Analytics -->
  <script type="text/javascript">
    var collapsedSections = [];
  </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/LightRFT" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/LightZero" target="_blank">
                  <span class="dropdown-title">LightZero </span>
                  <p>OpenDILab Decision Monte Carlo Tree Search Framework</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GenerativeRL" target="_blank">
                  <span class="dropdown-title">GenerativeRL </span>
                  <p>OpenDILab Generative AI Framework</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/index.html">Quick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide &amp; Best Practices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../best_practice/index.html">Best Practices</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../utils/index.html">lightrft.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasets/index.html">lightrft.datasets</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">lightrft.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../strategy/index.html">lightrft.strategy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trainer/index.html">lightrft.trainer</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="index.html">lightrft.models</a> &gt;</li>
        
      <li>lightrft.models.actor_al</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/api_doc/models/actor_al.rst.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="module-lightrft.models.actor_al">
<span id="lightrft-models-actor-al"></span><h1>lightrft.models.actor_al<a class="headerlink" href="#module-lightrft.models.actor_al" title="Permalink to this heading">¶</a></h1>
<p>Audio Language Model Actor Module for Reinforcement Learning.</p>
<p>This module provides the ActorAL class, which implements an actor model specifically designed
for audio-language tasks in reinforcement learning scenarios. The actor is responsible for
generating actions (text sequences) based on audio inputs and textual prompts.</p>
<p>The module supports various optimization techniques including:
- LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning
- Flash Attention 2.0 for improved performance
- DeepSpeed integration for distributed training
- Sample packing for efficient batch processing</p>
<p>Key Features:
- Multi-modal input processing (text + audio)
- Flexible model loading from pretrained checkpoints
- Support for various audio-language model architectures
- Gradient checkpointing for memory optimization
- MoE (Mixture of Experts) model support</p>
<dl class="py class">
<dt class="sig sig-object py" id="lightrft.models.actor_al.ActorAL">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lightrft.models.actor_al.</span></span><span class="sig-name descname"><span class="pre">ActorAL</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/lightrft/models/actor_al.html#ActorAL"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.models.actor_al.ActorAL" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Audio Language Model actor for reinforcement learning applications.</p>
<p>This class serves as a foundation for implementing audio-language actor models in RL,
which are responsible for generating text sequences (actions) based on both audio and
textual inputs. The model supports various optimization techniques including LoRA
adaptation, quantization, and distributed training.</p>
<p>The actor model can be initialized either from a pretrained model path or from an
existing model instance, providing flexibility in model deployment scenarios.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrain_or_model</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>nn.Module</em><em>]</em>) – Either a string path to a pretrained model or a model instance</p></li>
<li><p><strong>use_flash_attention_2</strong> (<em>bool</em>) – Whether to utilize Flash Attention 2.0 for improved performance</p></li>
<li><p><strong>bf16</strong> (<em>bool</em>) – Enable bfloat16 precision for model computations</p></li>
<li><p><strong>lora_rank</strong> (<em>int</em>) – Rank for LoRA adaptation (0 disables LoRA)</p></li>
<li><p><strong>lora_alpha</strong> (<em>int</em>) – Alpha parameter for LoRA scaling</p></li>
<li><p><strong>lora_dropout</strong> (<em>float</em>) – Dropout rate for LoRA layers</p></li>
<li><p><strong>target_modules</strong> (<em>Optional</em><em>[</em><em>list</em><em>]</em>) – List of target modules for applying LoRA (auto-detected if None)</p></li>
<li><p><strong>ds_config</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em>) – Configuration for DeepSpeed distributed training</p></li>
<li><p><strong>device_map</strong> (<em>Optional</em><em>[</em><em>dict</em><em>]</em>) – Device mapping for loading the model onto specific devices</p></li>
<li><p><strong>packing_samples</strong> (<em>bool</em>) – Whether to pack samples during training for efficiency</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize with a pretrained model path</span>
<span class="n">actor</span> <span class="o">=</span> <span class="n">ActorAL</span><span class="p">(</span>
    <span class="n">pretrain_or_model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2-Audio-7B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">use_flash_attention_2</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">lora_rank</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span>
<span class="p">)</span>

<span class="c1"># Generate responses</span>
<span class="n">sequences</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">action_mask</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_tensor</span><span class="p">,</span>
    <span class="n">audio_values</span><span class="o">=</span><span class="n">audio_tensor</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="lightrft.models.actor_al.ActorAL.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.LongTensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_actions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">audio_values</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">packed_seq_lens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../_modules/lightrft/models/actor_al.html#ActorAL.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.models.actor_al.ActorAL.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward pass to compute action log probabilities for reinforcement learning.</p>
<p>This method processes input sequences and audio information to compute log probabilities
of actions (tokens) for RL training. It supports both standard and packed sequence formats
and can return either just the action log probabilities or the full model output.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequences</strong> (<em>torch.LongTensor</em>) – Input token sequences</p></li>
<li><p><strong>num_actions</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>int</em><em>, </em><em>list</em><em>[</em><em>int</em><em>]</em><em>]</em><em>]</em>) – Number of action tokens to extract log probs for</p></li>
<li><p><strong>attention_mask</strong> (<em>Optional</em><em>[</em><em>torch.Tensor</em><em>]</em>) – Attention mask for the sequences</p></li>
<li><p><strong>audio_values</strong> (<em>torch.Tensor</em>) – Preprocessed audio values of input audio</p></li>
<li><p><strong>return_output</strong> (<em>bool</em>) – Whether to return the full model output along with log probs</p></li>
<li><p><strong>packed_seq_lens</strong> (<em>Optional</em><em>[</em><em>list</em><em>[</em><em>int</em><em>]</em><em>]</em>) – Sequence lengths for packed samples</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Action log probabilities or tuple of (action_log_probs, output) if return_output=True</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute action log probabilities for RL training</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">actor</span><span class="p">(</span>
    <span class="n">sequences</span><span class="o">=</span><span class="n">token_sequences</span><span class="p">,</span>
    <span class="n">num_actions</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">audio_values</span><span class="o">=</span><span class="n">audio_tensor</span>
<span class="p">)</span>

<span class="c1"># Get both log probs and full output</span>
<span class="n">log_probs</span><span class="p">,</span> <span class="n">output</span> <span class="o">=</span> <span class="n">actor</span><span class="p">(</span>
    <span class="n">sequences</span><span class="o">=</span><span class="n">token_sequences</span><span class="p">,</span>
    <span class="n">num_actions</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">audio_values</span><span class="o">=</span><span class="n">audio_tensor</span><span class="p">,</span>
    <span class="n">return_output</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.models.actor_al.ActorAL.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">audio_values</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.LongTensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.BoolTensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#lightrft.models.actor_al.ActorAL.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate text sequences based on input text and audio information.</p>
<p>This method performs text generation conditioned on both textual prompts and audio inputs.
It handles the generation process with various sampling strategies and returns the generated
sequences along with attention masks and action masks for RL training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<em>torch.Tensor</em>) – Input token IDs representing the text prompt</p></li>
<li><p><strong>audio_values</strong> (<em>torch.Tensor</em>) – Preprocessed audio values of input audio</p></li>
<li><p><strong>kwargs</strong> (<em>dict</em>) – Additional generation parameters (top_k, top_p, temperature, etc.)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple containing generated sequences, attention mask, and action mask</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Union[Tuple[torch.LongTensor, torch.LongTensor], Tuple[torch.LongTensor, torch.LongTensor, torch.BoolTensor]]  # noqa</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sequences</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">action_mask</span> <span class="o">=</span> <span class="n">actor</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]),</span>
    <span class="n">audio_values</span><span class="o">=</span><span class="n">audio_tensor</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.models.actor_al.ActorAL.gradient_checkpointing_disable">
<span class="sig-name descname"><span class="pre">gradient_checkpointing_disable</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/lightrft/models/actor_al.html#ActorAL.gradient_checkpointing_disable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.models.actor_al.ActorAL.gradient_checkpointing_disable" title="Permalink to this definition">¶</a></dt>
<dd><p>Disable gradient checkpointing to use normal forward/backward computation.</p>
<p>This method restores the default behavior where all intermediate activations
are stored during the forward pass for use in the backward pass. This increases
memory usage but reduces computation time.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Disable gradient checkpointing</span>
<span class="n">actor</span><span class="o">.</span><span class="n">gradient_checkpointing_disable</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.models.actor_al.ActorAL.gradient_checkpointing_enable">
<span class="sig-name descname"><span class="pre">gradient_checkpointing_enable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradient_checkpointing_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{'use_reentrant':</span> <span class="pre">False}</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/lightrft/models/actor_al.html#ActorAL.gradient_checkpointing_enable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.models.actor_al.ActorAL.gradient_checkpointing_enable" title="Permalink to this definition">¶</a></dt>
<dd><p>Enable gradient checkpointing to reduce memory usage during training.</p>
<p>Gradient checkpointing trades compute for memory by recomputing intermediate
activations during the backward pass instead of storing them. This is particularly
useful for training large audio-language models with limited GPU memory.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>gradient_checkpointing_kwargs</strong> (<em>dict</em>) – Additional arguments for gradient checkpointing</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Enable gradient checkpointing with default settings</span>
<span class="n">actor</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

<span class="c1"># Enable with custom settings</span>
<span class="n">actor</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">({</span><span class="s2">&quot;use_reentrant&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="lightrft.models.actor_al.ActorAL.modality">
<span class="sig-name descname"><span class="pre">modality</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'audio'</span></em><a class="headerlink" href="#lightrft.models.actor_al.ActorAL.modality" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.models.actor_al.ActorAL.print_trainable_parameters">
<span class="sig-name descname"><span class="pre">print_trainable_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/lightrft/models/actor_al.html#ActorAL.print_trainable_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.models.actor_al.ActorAL.print_trainable_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Print information about trainable parameters in the model.</p>
<p>This method displays the number and percentage of trainable parameters,
which is particularly useful when using parameter-efficient methods like LoRA.
It helps monitor the efficiency of the fine-tuning approach.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print trainable parameter statistics</span>
<span class="n">actor</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
<span class="c1"># Output: trainable params: 4,194,304 || all params: 7,241,732,096 || trainable%: 0.058</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.models.actor_al.ActorAL.process_sequences">
<span class="sig-name descname"><span class="pre">process_sequences</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequences</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eos_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_token_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../../_modules/lightrft/models/actor_al.html#ActorAL.process_sequences"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.models.actor_al.ActorAL.process_sequences" title="Permalink to this definition">¶</a></dt>
<dd><p>Called by <cite>trainer/fast_exp_maker.py</cite>.</p>
<p>Process generated sequences to create proper attention and action masks.</p>
<p>This method post-processes the generated sequences to ensure proper handling of
end-of-sequence tokens and creates masks needed for reinforcement learning training.
It handles edge cases like multiple EOS tokens and ensures consistent sequence formatting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequences</strong> (<em>torch.Tensor</em>) – Generated token sequences</p></li>
<li><p><strong>input_len</strong> (<em>int</em>) – Length of the input prompt</p></li>
<li><p><strong>eos_token_id</strong> (<em>int</em>) – End-of-sequence token ID</p></li>
<li><p><strong>pad_token_id</strong> (<em>int</em>) – Padding token ID</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple of processed sequences, attention mask, and action mask</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="actor_language.html" class="btn btn-neutral float-right" title="lightrft.models.actor_language" accesskey="n"
      rel="next">Next <img src="../../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="index.html" class="btn btn-neutral" title="lightrft.models" accesskey="p"
      rel="prev"><img src="../../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2025, OpenDILab.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">lightrft.models.actor_al</a><ul>
<li><a class="reference internal" href="#lightrft.models.actor_al.ActorAL"><code class="docutils literal notranslate"><span class="pre">ActorAL</span></code></a><ul>
<li><a class="reference internal" href="#lightrft.models.actor_al.ActorAL.forward"><code class="docutils literal notranslate"><span class="pre">ActorAL.forward()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.models.actor_al.ActorAL.generate"><code class="docutils literal notranslate"><span class="pre">ActorAL.generate()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.models.actor_al.ActorAL.gradient_checkpointing_disable"><code class="docutils literal notranslate"><span class="pre">ActorAL.gradient_checkpointing_disable()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.models.actor_al.ActorAL.gradient_checkpointing_enable"><code class="docutils literal notranslate"><span class="pre">ActorAL.gradient_checkpointing_enable()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.models.actor_al.ActorAL.modality"><code class="docutils literal notranslate"><span class="pre">ActorAL.modality</span></code></a></li>
<li><a class="reference internal" href="#lightrft.models.actor_al.ActorAL.print_trainable_parameters"><code class="docutils literal notranslate"><span class="pre">ActorAL.print_trainable_parameters()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.models.actor_al.ActorAL.process_sequences"><code class="docutils literal notranslate"><span class="pre">ActorAL.process_sequences()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../"
    src="../../_static/documentation_options.js"></script>
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
  <script src="../../_static/doctools.js"></script>
  <script src="../../_static/sphinx_highlight.js"></script>
  <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
  

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/LightRFT" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>