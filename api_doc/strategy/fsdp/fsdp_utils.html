


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lightrft.strategy.fsdp.fsdp_utils &mdash; LightRFT v0.1.1 documentation</title>
  

  <link rel="shortcut icon" href="../../../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/readthedocs.css" type="text/css" />
  <link rel="index" title="Index" href="../../../genindex.html" />
  <link rel="search" title="Search" href="../../../search.html" />
  <link rel="next" title="lightrft.strategy.fsdp.fsdpv2" href="fsdpv2.html" />
  <link rel="prev" title="lightrft.strategy.fsdp.fsdp_optimizer" href="fsdp_optimizer.html" />
  <!-- Google Analytics -->
  <script type="text/javascript">
    var collapsedSections = [];
  </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/LightRFT" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/LightZero" target="_blank">
                  <span class="dropdown-title">LightZero </span>
                  <p>OpenDILab Decision Monte Carlo Tree Search Framework</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GenerativeRL" target="_blank">
                  <span class="dropdown-title">GenerativeRL </span>
                  <p>OpenDILab Generative AI Framework</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start/index.html">Quick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide &amp; Best Practices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../best_practice/index.html">Best Practices</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../utils/index.html">lightrft.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../datasets/index.html">lightrft.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../models/index.html">lightrft.models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">lightrft.strategy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../trainer/index.html">lightrft.trainer</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="../index.html">lightrft.strategy</a> &gt;</li>
        
          <li><a href="index.html">lightrft.strategy.fsdp</a> &gt;</li>
        
      <li>lightrft.strategy.fsdp.fsdp_utils</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../../_sources/api_doc/strategy/fsdp/fsdp_utils.rst.txt" rel="nofollow"><img src="../../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="module-lightrft.strategy.fsdp.fsdp_utils">
<span id="lightrft-strategy-fsdp-fsdp-utils"></span><h1>lightrft.strategy.fsdp.fsdp_utils<a class="headerlink" href="#module-lightrft.strategy.fsdp.fsdp_utils" title="Permalink to this heading">¶</a></h1>
<p>Gradient scaling and optimization utilities for deep learning.</p>
<p>This module provides tools for gradient handling, norm computation, and optimization in PyTorch.
It includes dynamic gradient scaling for mixed precision training, gradient norm computation,
and base classes for optimizers.</p>
<section id="is-meta-initialized">
<h2>is_meta_initialized<a class="headerlink" href="#is-meta-initialized" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.is_meta_initialized">
<span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">is_meta_initialized</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">bool</span></span></span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#is_meta_initialized"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.is_meta_initialized" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if a PyTorch model’s parameters are meta-initialized.</p>
<p>Meta-initialized models contain parameters on a ‘meta’ device, which are placeholders
that don’t allocate actual memory. These are useful for model initialization without
memory overhead, commonly used in model parallelism and large model initialization.</p>
<p>For more information on meta device and meta tensors, see:
<a class="reference external" href="https://docs.pytorch.org/docs/stable/meta.html">https://docs.pytorch.org/docs/stable/meta.html</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>torch.nn.Module</em>) – The PyTorch module to check.</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>TypeError</strong> – if <code class="docutils literal notranslate"><span class="pre">model</span></code> is not an instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>True if any parameter in the model is on a meta device, False otherwise.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>bool</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_meta_initialized</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>  <span class="c1"># False for regular model</span>
<span class="go">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;meta&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">meta_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">is_meta_initialized</span><span class="p">(</span><span class="n">meta_model</span><span class="p">)</span>  <span class="c1"># True for meta model</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="multi-tensor-l2norm-torch">
<h2>multi_tensor_l2norm_torch<a class="headerlink" href="#multi-tensor-l2norm-torch" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.multi_tensor_l2norm_torch">
<span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">multi_tensor_l2norm_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor_list</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_tensor</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#multi_tensor_l2norm_torch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.multi_tensor_l2norm_torch" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute L2 norm of multiple tensors using PyTorch.</p>
<p>This function provides a pure PyTorch implementation for computing L2 norms
when APEX is not available. It converts all tensors to float32 for computation
and returns both overall and per-tensor norms.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor_list</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of tensors to compute norm for</p></li>
<li><p><strong>per_tensor</strong> (<em>bool</em>) – Whether to return per-tensor norms</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Tuple of (overall L2 norm, per-tensor norms)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">overall_norm</span><span class="p">,</span> <span class="n">per_tensor_norms</span> <span class="o">=</span> <span class="n">multi_tensor_l2norm_torch</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">overall_norm</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">per_tensor_norms</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># torch.Size([2])</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="calc-l2-norm">
<h2>calc_l2_norm<a class="headerlink" href="#calc-l2-norm" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.calc_l2_norm">
<span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">calc_l2_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#calc_l2_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.calc_l2_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate L2 norm of gradients using optimized implementation when available.</p>
<p>This function automatically selects the fastest available implementation for
computing L2 norms. It uses APEX’s multi-tensor operations when available
for better performance, otherwise falls back to PyTorch implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>grads</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of gradient tensors</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>L2 norm of gradients</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm</span> <span class="o">=</span> <span class="n">calc_l2_norm</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># scalar value</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="calc-lp">
<h2>calc_lp<a class="headerlink" href="#calc-lp" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.calc_lp">
<span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">calc_lp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#calc_lp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.calc_lp" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate Lp norm of gradients.</p>
<p>Computes the p-norm of a list of gradient tensors, where p is specified
by norm_type. This is useful for gradient clipping and monitoring.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>grads</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of gradient tensors</p></li>
<li><p><strong>norm_type</strong> (<em>float</em>) – The p in Lp norm</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Lp norm of gradients</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l1_norm</span> <span class="o">=</span> <span class="n">calc_lp</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># L1 norm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l2_norm</span> <span class="o">=</span> <span class="n">calc_lp</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>  <span class="c1"># L2 norm</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="get-norm">
<h2>get_norm<a class="headerlink" href="#get-norm" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.get_norm">
<span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">get_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">grads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_cuda_kernels</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#get_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.get_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Get norm of gradients with specified norm type.</p>
<p>This function dispatches to the appropriate norm calculation method based
on the norm type and whether CUDA kernels are available. It handles special
cases like infinity norm and optimized L2 norm computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>grads</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of gradient tensors</p></li>
<li><p><strong>norm_type</strong> (<em>float</em>) – Type of norm to compute (2.0, inf, etc.)</p></li>
<li><p><strong>enable_cuda_kernels</strong> (<em>bool</em>) – Whether to use CUDA optimized kernels</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Norm of gradients</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float or torch.Tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l2_norm</span> <span class="o">=</span> <span class="n">get_norm</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inf_norm</span> <span class="o">=</span> <span class="n">get_norm</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="reduce-grads">
<h2>reduce_grads<a class="headerlink" href="#reduce-grads" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.reduce_grads">
<span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">reduce_grads</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#reduce_grads"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.reduce_grads" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare gradients for norm computation in distributed training.</p>
<p>This function processes gradients to prepare them for norm computation,
particularly in distributed training scenarios. It converts gradients to
float32 for numerical stability during norm calculations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gradients</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of gradient tensors</p></li>
<li><p><strong>parameters</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em>) – List of parameter tensors</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>List of processed gradient tensors</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>list[torch.Tensor]</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">processed_grads</span> <span class="o">=</span> <span class="n">reduce_grads</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="get-tensor-norm">
<h2>get_tensor_norm<a class="headerlink" href="#get-tensor-norm" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.get_tensor_norm">
<span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">get_tensor_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">move_to_cuda</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">torch.Tensor</span></span></span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#get_tensor_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.get_tensor_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert norm to tensor and move to appropriate device.</p>
<p>This utility function ensures that norm values are properly converted to
tensors and placed on the correct device for further computation. It handles
both scalar and tensor inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>norm</strong> (<em>Union</em><em>[</em><em>float</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Norm value as float or tensor</p></li>
<li><p><strong>move_to_cuda</strong> (<em>bool</em>) – Whether to move the tensor to CUDA</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Norm as tensor on appropriate device</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">norm_float</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">norm_tensor</span> <span class="o">=</span> <span class="n">get_tensor_norm</span><span class="p">(</span><span class="n">norm_float</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">norm_tensor</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>  <span class="c1"># cuda:0 (if CUDA available)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="compute-norm">
<h2>compute_norm<a class="headerlink" href="#compute-norm" title="Permalink to this heading">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.compute_norm">
<span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">compute_norm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gradients</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parameters</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#compute_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.compute_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the norm across distributed environment.</p>
<p>This function computes gradient norms in a distributed training setting,
handling device placement and distributed reduction. It’s commonly used
for gradient clipping and monitoring training stability.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>gradients</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em>) – The gradient values</p></li>
<li><p><strong>parameters</strong> (<em>list</em><em>[</em><em>torch.Tensor</em><em>]</em>) – The parameters each gradient corresponds to</p></li>
<li><p><strong>norm_type</strong> (<em>float</em><em> or </em><em>int</em>) – Type of the used p-norm. Can be <code class="docutils literal notranslate"><span class="pre">'inf'</span></code> for infinity norm</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Total norm of the parameters, need total_norm**(1/norm) before using</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">param</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">total_norm</span> <span class="o">=</span> <span class="n">compute_norm</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Gradient norm: </span><span class="si">{</span><span class="n">total_norm</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="basegradscaler">
<h2>BaseGradScaler<a class="headerlink" href="#basegradscaler" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">BaseGradScaler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initial_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseGradScaler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>A base class for the gradient scaler.</p>
<p>This abstract base class defines the interface for gradient scalers used in
mixed precision training. Gradient scalers help prevent gradient underflow
in float16 training by scaling up the loss before backpropagation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>initial_scale</strong> (<em>float</em>) – The initial loss scale</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Subclass implementation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MyGradScaler</span><span class="p">(</span><span class="n">BaseGradScaler</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">overflow</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="c1"># Custom update logic</span>
<span class="gp">... </span>        <span class="k">pass</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.inv_scale">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">inv_scale</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.inv_scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the inverse of the loss scale.</p>
<p>The inverse scale is used to unscale gradients after backpropagation
to restore their original magnitudes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Inverse of current loss scale</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">DynamicGradScaler</span><span class="p">(</span><span class="n">initial_scale</span><span class="o">=</span><span class="mf">1024.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">inv_scale</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># 0.0009765625 (1/1024)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseGradScaler.load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the states of the gradient scaler from a dict object.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<em>Dict</em>) – The states of the gradient scaler</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">DynamicGradScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;scale&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2048.0</span><span class="p">])}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.scale">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">scale</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">torch.Tensor</span></em><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the loss scale.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Current loss scale</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>torch.Tensor</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">DynamicGradScaler</span><span class="p">(</span><span class="n">initial_scale</span><span class="o">=</span><span class="mf">1024.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># 1024.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Dict</span></span></span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseGradScaler.state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the states of the gradient scaler as a dict object.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>State dictionary containing scale</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>Dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">DynamicGradScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="go">dict_keys([&#39;scale&#39;])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.update">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">overflow</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseGradScaler.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the loss scale.</p>
<p>This abstract method must be implemented by subclasses to define
how the loss scale should be updated based on overflow detection.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>overflow</strong> (<em>bool</em>) – Whether overflow occurs</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="dynamicgradscaler">
<h2>DynamicGradScaler<a class="headerlink" href="#dynamicgradscaler" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">DynamicGradScaler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">initial_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">65536</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">growth_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backoff_factor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">growth_interval</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16777216</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hysteresis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">torch.dtype</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">torch.bfloat16</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#DynamicGradScaler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler" title="Permalink to this definition">¶</a></dt>
<dd><p>A gradient scaler which uses dynamic loss scale.</p>
<p>This scaler automatically adjusts the loss scale based on gradient overflow
detection. It increases the scale when training is stable and decreases it
when overflow occurs, providing automatic mixed precision training support.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initial_scale</strong> (<em>float</em>) – The initial loss scale</p></li>
<li><p><strong>growth_factor</strong> (<em>float</em>) – The multiplication factor for increasing loss scale</p></li>
<li><p><strong>backoff_factor</strong> (<em>float</em>) – The multiplication factor for decreasing loss scale</p></li>
<li><p><strong>growth_interval</strong> (<em>int</em>) – The number of steps to increase loss scale when no overflow occurs</p></li>
<li><p><strong>min_scale</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – The minimum loss scale</p></li>
<li><p><strong>max_scale</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – The maximum loss scale</p></li>
<li><p><strong>hysteresis</strong> (<em>int</em>) – The number of overflows before decreasing loss scale</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em>) – The data type used for training</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">DynamicGradScaler</span><span class="p">(</span><span class="n">initial_scale</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">16</span><span class="p">,</span> <span class="n">growth_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># In training loop</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
<span class="gp">... </span>        <span class="c1"># Forward pass with scaled loss</span>
<span class="gp">... </span>        <span class="n">scaled_loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span>
<span class="gp">... </span>        <span class="n">scaled_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># Check for overflow and update scaler</span>
<span class="gp">... </span>        <span class="n">overflow</span> <span class="o">=</span> <span class="n">check_overflow</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="gp">... </span>        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">overflow</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#DynamicGradScaler.load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the states of the gradient scaler from a dict object.</p>
<p>This method restores the scaler state from a previously saved state
dictionary, enabling seamless checkpoint restoration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state_dict</strong> (<em>dict</em>) – The states of the gradient scaler</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">DynamicGradScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">({</span>
<span class="gp">... </span>    <span class="s2">&quot;_scale&quot;</span><span class="p">:</span> <span class="mf">2048.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s2">&quot;_growth_step&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s2">&quot;_hysteresis_step&quot;</span><span class="p">:</span> <span class="mi">0</span>
<span class="gp">... </span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#DynamicGradScaler.state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the states of the gradient scaler as a dict object.</p>
<p>This method provides a complete state dictionary that can be saved
and restored to maintain training consistency across checkpoints.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A dictionary containing the current state of the gradient scaler</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">DynamicGradScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler_state</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">scaler_state</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="go">dict_keys([&#39;_scale&#39;, &#39;_growth_step&#39;, &#39;_hysteresis_step&#39;])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">overflow</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#DynamicGradScaler.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the loss scale based on whether overflow occurred.</p>
<p>This method implements the dynamic scaling algorithm. When overflow occurs,
it increments the hysteresis counter and resets growth progress. When no
overflow occurs for a sufficient period, it increases the scale to maximize
gradient precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>overflow</strong> (<em>bool</em>) – Whether overflow occurs</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span> <span class="o">=</span> <span class="n">DynamicGradScaler</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simulate training steps</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># No overflow, increment growth counter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>   <span class="c1"># Overflow detected, may decrease scale</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="baseoptimizer">
<h2>BaseOptimizer<a class="headerlink" href="#baseoptimizer" title="Permalink to this heading">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lightrft.strategy.fsdp.fsdp_utils.</span></span><span class="sig-name descname"><span class="pre">BaseOptimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Any</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Base Optimizer class that wraps a PyTorch optimizer.</p>
<p>This class provides a wrapper around PyTorch optimizers, exposing the same interface
while allowing for additional functionality like custom backward passes and gradient clipping.
It serves as a foundation for building more sophisticated optimizers with enhanced features
for distributed training, gradient scaling, and custom optimization strategies.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optim</strong> (<em>torch.optim.Optimizer</em>) – The PyTorch optimizer to wrap</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pytorch_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wrapped_optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">pytorch_optimizer</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use wrapped_optimizer like a regular optimizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wrapped_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wrapped_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.add_param_group">
<span class="sig-name descname"><span class="pre">add_param_group</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer.add_param_group"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.add_param_group" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a parameter group to the optimizer.</p>
<p>This method allows adding new parameter groups with potentially different
optimization settings during training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – Positional arguments to pass to the wrapped optimizer</p></li>
<li><p><strong>kwargs</strong> – Keyword arguments to pass to the wrapped optimizer</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Result from the wrapped optimizer’s add_param_group method</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">new_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">add_param_group</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">new_params</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute gradients of the loss.</p>
<p>This method performs backpropagation to compute gradients of the loss
with respect to the model parameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>loss</strong> (<em>torch.Tensor</em>) – The loss tensor to compute gradients for</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.backward_by_grad">
<span class="sig-name descname"><span class="pre">backward_by_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer.backward_by_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.backward_by_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute gradients of the tensor with respect to the provided gradients.</p>
<p>This method allows for custom gradient computation, useful in scenarios
like gradient accumulation or custom loss functions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<em>torch.Tensor</em>) – The tensor to compute gradients for</p></li>
<li><p><strong>grad</strong> (<em>torch.Tensor</em>) – The gradients to backpropagate</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">custom_grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">backward_by_grad</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">custom_grad</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.clip_grad_norm">
<span class="sig-name descname"><span class="pre">clip_grad_norm</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer.clip_grad_norm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.clip_grad_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Clip the gradient norm.</p>
<p>This is a placeholder method that should be implemented by subclasses
to provide gradient clipping functionality. Gradient clipping helps
prevent gradient explosion in deep networks.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">MyOptimizer</span><span class="p">(</span><span class="n">BaseOptimizer</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">clip_grad_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">],</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.defaults">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">defaults</span></span><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.defaults" title="Permalink to this definition">¶</a></dt>
<dd><p>Access to the default parameters of the wrapped optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Default parameters dictionary</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>  <span class="c1"># 0.001</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer.load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the optimizer state.</p>
<p>This method restores the optimizer’s internal state from a state dictionary,
enabling checkpoint restoration and training resumption.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – Positional arguments to pass to the wrapped optimizer</p></li>
<li><p><strong>kwargs</strong> – Keyword arguments to pass to the wrapped optimizer</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;optimizer_checkpoint.pth&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.param_groups">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">param_groups</span></span><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.param_groups" title="Permalink to this definition">¶</a></dt>
<dd><p>Access to the parameter groups of the wrapped optimizer.</p>
<p>Parameter groups allow different sets of parameters to have different
optimization settings like learning rates, weight decay, etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>List of parameter groups</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>list</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))</span>  <span class="c1"># Number of parameter groups</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>  <span class="c1"># Learning rate of first group</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer.state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the state of the optimizer as a dict.</p>
<p>This method provides the optimizer’s complete state for checkpointing,
including parameter states and hyperparameters.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The state of the optimizer</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">state_dict</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">state_dict</span><span class="p">,</span> <span class="s1">&#39;optimizer_checkpoint.pth&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform a single optimization step.</p>
<p>This method executes one optimization step, updating the model parameters
based on their gradients and the optimizer’s algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – Positional arguments to pass to the wrapped optimizer</p></li>
<li><p><strong>kwargs</strong> – Keyword arguments to pass to the wrapped optimizer</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Result from the wrapped optimizer’s step method</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/lightrft/strategy/fsdp/fsdp_utils.html#BaseOptimizer.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the gradients of all optimized tensors.</p>
<p>This method clears the gradients of all parameters, which is typically
done before each backward pass to prevent gradient accumulation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong> – Positional arguments to pass to the wrapped optimizer</p></li>
<li><p><strong>kwargs</strong> – Keyword arguments to pass to the wrapped optimizer</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">BaseOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># Clear gradients</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>        <span class="c1"># Compute new gradients</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>       <span class="c1"># Update parameters</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
</section>


              </article>
              
            </div>
            <footer>
  
  <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
    <a href="fsdpv2.html" class="btn btn-neutral float-right" title="lightrft.strategy.fsdp.fsdpv2" accesskey="n"
      rel="next">Next <img src="../../../_static/images/chevron-right-blue.svg"
        class="next-page"></a>
    
    
    <a href="fsdp_optimizer.html" class="btn btn-neutral" title="lightrft.strategy.fsdp.fsdp_optimizer" accesskey="p"
      rel="prev"><img src="../../../_static/images/chevron-right-blue.svg" class="previous-page"> Previous</a>
    
  </div>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2025, OpenDILab.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">lightrft.strategy.fsdp.fsdp_utils</a><ul>
<li><a class="reference internal" href="#is-meta-initialized">is_meta_initialized</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.is_meta_initialized"><code class="docutils literal notranslate"><span class="pre">is_meta_initialized()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#multi-tensor-l2norm-torch">multi_tensor_l2norm_torch</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.multi_tensor_l2norm_torch"><code class="docutils literal notranslate"><span class="pre">multi_tensor_l2norm_torch()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#calc-l2-norm">calc_l2_norm</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.calc_l2_norm"><code class="docutils literal notranslate"><span class="pre">calc_l2_norm()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#calc-lp">calc_lp</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.calc_lp"><code class="docutils literal notranslate"><span class="pre">calc_lp()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#get-norm">get_norm</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.get_norm"><code class="docutils literal notranslate"><span class="pre">get_norm()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#reduce-grads">reduce_grads</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.reduce_grads"><code class="docutils literal notranslate"><span class="pre">reduce_grads()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#get-tensor-norm">get_tensor_norm</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.get_tensor_norm"><code class="docutils literal notranslate"><span class="pre">get_tensor_norm()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#compute-norm">compute_norm</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.compute_norm"><code class="docutils literal notranslate"><span class="pre">compute_norm()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#basegradscaler">BaseGradScaler</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler"><code class="docutils literal notranslate"><span class="pre">BaseGradScaler</span></code></a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.inv_scale"><code class="docutils literal notranslate"><span class="pre">BaseGradScaler.inv_scale</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.load_state_dict"><code class="docutils literal notranslate"><span class="pre">BaseGradScaler.load_state_dict()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.scale"><code class="docutils literal notranslate"><span class="pre">BaseGradScaler.scale</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.state_dict"><code class="docutils literal notranslate"><span class="pre">BaseGradScaler.state_dict()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseGradScaler.update"><code class="docutils literal notranslate"><span class="pre">BaseGradScaler.update()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#dynamicgradscaler">DynamicGradScaler</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler"><code class="docutils literal notranslate"><span class="pre">DynamicGradScaler</span></code></a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.load_state_dict"><code class="docutils literal notranslate"><span class="pre">DynamicGradScaler.load_state_dict()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.state_dict"><code class="docutils literal notranslate"><span class="pre">DynamicGradScaler.state_dict()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.DynamicGradScaler.update"><code class="docutils literal notranslate"><span class="pre">DynamicGradScaler.update()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#baseoptimizer">BaseOptimizer</a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer</span></code></a><ul>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.add_param_group"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.add_param_group()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.backward"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.backward()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.backward_by_grad"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.backward_by_grad()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.clip_grad_norm"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.clip_grad_norm()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.defaults"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.defaults</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.load_state_dict"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.load_state_dict()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.param_groups"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.param_groups</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.state_dict"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.state_dict()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.step"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.step()</span></code></a></li>
<li><a class="reference internal" href="#lightrft.strategy.fsdp.fsdp_utils.BaseOptimizer.zero_grad"><code class="docutils literal notranslate"><span class="pre">BaseOptimizer.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../../"
    src="../../../_static/documentation_options.js"></script>
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
  <script src="../../../_static/doctools.js"></script>
  <script src="../../../_static/sphinx_highlight.js"></script>
  <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
  <script src="../../../_static/js/language_switch.js"></script>
  

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/LightRFT" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>