.. _installation:

==================================
Installation
==================================

This guide provides instructions for installing and setting up LightRFT, a lightweight and high-performance reinforcement learning fine-tuning framework designed for Large Language Models (LLMs) and Vision-Language Models (VLMs).

Requirements
============

Before installing LightRFT, ensure your environment meets the following requirements:

* Python >= 3.10
* CUDA >= 11.8
* PyTorch >= 2.5.1
* CUDA-compatible GPU(s)

Docker Images
=============

TO BE DONE

Installation
============

Standard Installation
----------------------

Clone and install LightRFT:

.. code-block:: bash

   # Clone the repository
   git clone https://github.com/opendilab/LightRFT.git
   cd LightRFT

   # Install dependencies
   pip install -r requirements.txt

   # Install LightRFT
   pip install -e .


Documentation Generation (Optional)
====================================

To install dependencies for generating documentation:

.. code-block:: bash

   pip install -r requirements-doc.txt

To generate HTML documentation:

.. code-block:: bash

   make docs

The documentation will be generated in the ``docs/build`` directory. Open ``index.html`` to view it.

For live browser documentation with auto-reload:

.. code-block:: bash

   make live

Project Structure
=================

LightRFT is organized into several key modules:

.. code-block:: text

   LightRFT/
   â”œâ”€â”€ lightrft/                      # Core library
   â”‚   â”œâ”€â”€ strategy/                  # Training & inference strategies
   â”‚   â”‚   â”œâ”€â”€ fsdp/                  # FSDP implementation
   â”‚   â”‚   â”œâ”€â”€ deepspeed/             # DeepSpeed implementation
   â”‚   â”‚   â”œâ”€â”€ vllm_utils/            # vLLM utilities
   â”‚   â”‚   â”œâ”€â”€ sglang_utils/          # SGLang utilities
   â”‚   â”‚   â””â”€â”€ utils/                 # Strategy utilities
   â”‚   â”œâ”€â”€ models/                    # Model definitions
   â”‚   â”‚   â”œâ”€â”€ actor_al.py            # Audio-language model actor
   â”‚   â”‚   â”œâ”€â”€ actor_language.py      # Language model actor
   â”‚   â”‚   â”œâ”€â”€ actor_vl.py            # Vision-language model actor
   â”‚   â”‚   â”œâ”€â”€ grm_vl.py              # Generative reward model (Vision-Language)
   â”‚   â”‚   â”œâ”€â”€ srm_al.py              # Scalar reward model (Audio-Language)
   â”‚   â”‚   â”œâ”€â”€ srm_vl.py              # Scalar reward model (Vision-Language)
   â”‚   â”‚   â”œâ”€â”€ loss.py                # Loss functions
   â”‚   â”‚   â”œâ”€â”€ monkey_patch/          # Model adaptation patches for distributed training
   â”‚   â”‚   â”œâ”€â”€ tests/                 # Model tests
   â”‚   â”‚   â””â”€â”€ utils.py               # Model utilities
   â”‚   â”œâ”€â”€ trainer/                   # Trainer implementations
   â”‚   â”‚   â”œâ”€â”€ ppo_trainer.py         # LLM PPO trainer
   â”‚   â”‚   â”œâ”€â”€ ppo_trainer_vl.py      # VLM PPO trainer
   â”‚   â”‚   â”œâ”€â”€ spmd_ppo_trainer.py    # SPMD PPO trainer Extension (**Core**)
   â”‚   â”‚   â”œâ”€â”€ grm_trainer_vl.py      # Generative reward model trainer (Vision-Language)
   â”‚   â”‚   â”œâ”€â”€ srm_trainer_al.py      # Scalar reward model trainer (Audio-Language)
   â”‚   â”‚   â”œâ”€â”€ srm_trainer_vl.py      # Scalar reward model trainer (Vision-Language)
   â”‚   â”‚   â”œâ”€â”€ fast_exp_maker.py      # Fast experience generator (**Core**)
   â”‚   â”‚   â”œâ”€â”€ experience_maker.py    # Base experience generator
   â”‚   â”‚   â”œâ”€â”€ experience_maker_vl.py # Base experience generator for VLM
   â”‚   â”‚   â”œâ”€â”€ replay_buffer.py       # Replay buffer
   â”‚   â”‚   â”œâ”€â”€ replay_buffer_vl.py    # VLM replay buffer
   â”‚   â”‚   â”œâ”€â”€ replay_buffer_utils.py # Replay buffer utilities
   â”‚   â”‚   â”œâ”€â”€ kl_controller.py       # KL divergence controller
   â”‚   â”‚   â””â”€â”€ utils.py               # Trainer utilities
   â”‚   â”œâ”€â”€ datasets/                  # Dataset processing
   â”‚   â”‚   â”œâ”€â”€ audio_alpaca.py        # Audio Alpaca dataset
   â”‚   â”‚   â”œâ”€â”€ grm_dataset.py         # Generative reward model dataset
   â”‚   â”‚   â”œâ”€â”€ hpdv3.py               # HPDv3 reward model dataset
   â”‚   â”‚   â”œâ”€â”€ image_reward_db.py     # Image reward database
   â”‚   â”‚   â”œâ”€â”€ imagegen_cot_reward.py # Image generation CoT generative reward
   â”‚   â”‚   â”œâ”€â”€ omnirewardbench.py     # OmniRewardBench dataset
   â”‚   â”‚   â”œâ”€â”€ process_reward_dataset.py # Reward dataset processing
   â”‚   â”‚   â”œâ”€â”€ prompts_dataset.py     # LLM Prompts dataset
   â”‚   â”‚   â”œâ”€â”€ prompts_dataset_vl.py  # Vision-language prompts dataset
   â”‚   â”‚   â”œâ”€â”€ rapidata.py            # Rapidata reward modeldataset
   â”‚   â”‚   â”œâ”€â”€ sft_dataset.py         # SFT dataset
   â”‚   â”‚   â”œâ”€â”€ sft_dataset_vl.py      # VLM SFT dataset
   â”‚   â”‚   â”œâ”€â”€ srm_dataset.py         # Scalar reward model base dataset
   â”‚   â”‚   â””â”€â”€ utils.py               # Dataset utilities
   â”‚   â””â”€â”€ utils/                     # Utility functions
   â”‚       â”œâ”€â”€ ckpt_scripts/          # Checkpoint processing scripts
   â”‚       â”œâ”€â”€ cli_args.py            # CLI argument parsing
   â”‚       â”œâ”€â”€ distributed_sampler.py # Distributed sampler
   â”‚       â”œâ”€â”€ logging_utils.py       # Logging utilities
   â”‚       â”œâ”€â”€ processor.py           # Data processor for HF model
   â”‚       â”œâ”€â”€ remote_rm_utils.py     # Remote reward model utilities
   â”‚       â”œâ”€â”€ timer.py               # Timer utilities
   â”‚       â”œâ”€â”€ trajectory_saver.py    # Trajectory saver
   â”‚       â””â”€â”€ utils.py               # General utilities
   â”‚
   â”œâ”€â”€ examples/                      # Usage examples
   â”‚   â”œâ”€â”€ gsm8k_geo3k/               # GSM8K/Geo3K math reasoning training examples
   â”‚   â”œâ”€â”€ grm_training/              # Generative reward model training examples
   â”‚   â”œâ”€â”€ srm_training/              # Scalar reward model training examples
   â”‚   â”œâ”€â”€ chat/                      # Model dialogue examples
   â”‚
   â”œâ”€â”€ docs/                          # ðŸ“š Sphinx documentation
   â”‚   â”œâ”€â”€ Makefile                   # Documentation build Makefile
   â”‚   â”œâ”€â”€ make.bat                   # Documentation build batch file
   â”‚   â””â”€â”€ source/                    # Documentation source
   â”‚       â”œâ”€â”€ _static/               # Static files (CSS, etc.)
   â”‚       â”œâ”€â”€ api_doc/               # API documentation
   â”‚       â”œâ”€â”€ best_practice/         # Best practices & resources
   â”‚       â”œâ”€â”€ installation/          # Installation guides
   â”‚       â””â”€â”€ quick_start/           # Quick start & user guides
   â”‚
   â”œâ”€â”€ assets/                        # Assets
   â”‚   â””â”€â”€ logo.png                   # Project logo
   â”‚
   â”œâ”€â”€ CHANGELOG.md                   # Changelog
   â”œâ”€â”€ LICENSE                        # License file
   â”œâ”€â”€ Makefile                       # Project Makefile
   â”œâ”€â”€ README.md                      # Project documentation (English)
   â”œâ”€â”€ README_zh.md                   # Project documentation (Chinese)
   â”œâ”€â”€ requirements.txt               # Python dependencies
   â”œâ”€â”€ requirements-dev.txt           # Development dependencies
   â”œâ”€â”€ requirements-doc.txt          # Documentation dependencies
   â””â”€â”€ setup.py                       # Package setup script

Key Directory Descriptions
--------------------------

* **lightrft/**: LightRFT core library with five main modules:

  * ``datasets/``: Dataset implementations for prompts, SFT, reward modeling (text, vision-language, audio-language)
  * ``models/``: Actor models (language, vision-language, audio-language), reward models, and loss functions
  * ``strategy/``: Training strategies including FSDP, DeepSpeed, vLLM/SGLang integration
  * ``trainer/``: Trainer implementations for PPO, experience generation, and replay buffers
  * ``utils/``: Utility functions for CLI, logging, distributed training, and trajectory saving

* **examples/**: Complete training examples and scripts

  * ``gsm8k_geo3k/``: GSM8K and Geo3K math reasoning training examples
  * ``grm_training/``: Generative reward model training examples
  * ``srm_training/``: Scalar reward model training examples
  * ``chat/``: Model dialogue examples

* **docs/**: Sphinx documentation with complete user guides and API documentation

Verification
============

To verify your installation, run a simple test:

.. code-block:: bash

   python -c "import lightrft; print(lightrft)"

You should see the module path without any import errors.

Quick Start Example
===================

After installation, try a basic GRPO training example:

.. code-block:: bash

   # Single node, 8 GPU training example
   cd /path/to/LightRFT

   # Run GRPO training (GSM8K math reasoning task)
   bash examples/gsm8k_geo3k/run_grpo_gsm8k_qwen2.5_0.5b.sh

   # Or run Geo3K geometry problem training (VLM multimodal)
   bash examples/gsm8k_geo3k/run_grpo_geo3k_qwen2.5_vl_7b.sh

Troubleshooting
===============

Common Issues
-------------

**Issue**: CUDA errors or version mismatch

* **Solution**: Ensure CUDA drivers and toolkit version match your PyTorch installation. Check with ``nvcc --version`` and ``python -c "import torch; print(torch.version.cuda)"``

**Issue**: Out of memory errors during training

* **Solution**:

  * Reduce ``micro_train_batch_size`` or ``micro_rollout_batch_size``
  * Enable gradient checkpointing: ``--gradient_checkpointing``
  * Use FSDP with CPU offload: ``--fsdp --fsdp_cpu_offload``
  * Adjust engine memory utilization: ``--engine_mem_util 0.4``

**Issue**: Slow installation of evaluation dependencies

* **Solution**: Use a mirror or proxy for pip:

  .. code-block:: bash

     pip install -i https://pypi.tuna.tsinghua.edu.cn/simple <package>

For Additional Support
----------------------

If you encounter issues not covered here:

* Check the project's `GitHub Issues <https://github.com/opendilab/LightRFT/issues>`_
* Review the :doc:`../best_practice/strategy_usage` guide for training configuration
* Consult the example scripts in the `examples <https://github.com/opendilab/LightRFT/tree/main/examples>`_ directory

Next Steps
==========

After successful installation:

1. Review the :doc:`../quick_start/index` guide to understand basic usage
2. Explore :doc:`../best_practice/strategy_usage` for distributed training strategies
3. Check out the `examples <https://github.com/opendilab/LightRFT/tree/main/examples>`_ directory for complete training examples
4. Read the algorithm documentation for specific implementation details

Related Documentation
=====================

* :doc:`../quick_start/index` - Quick start guide
* :doc:`../best_practice/index` - Best practices guide