.. _installation:

==================================
Installation Guide
==================================

This guide provides instructions for installing and setting up LightRFT, a lightweight and high-performance reinforcement learning fine-tuning framework designed for Large Language Models (LLMs) and Vision-Language Models (VLMs).

Requirements
============

Before installing LightRFT, ensure your environment meets the following requirements:

* Python >= 3.8
* CUDA >= 11.8
* PyTorch >= 2.5.1
* CUDA-compatible GPU(s)

Docker Images
=============

TO BE DONE

Installation
============

Standard Installation
----------------------

Clone and install LightRFT:

.. code-block:: bash

   # Clone the repository
   git clone https://github.com/opendilab/LightRFT.git
   cd LightRFT

   # Install dependencies
   pip install -r requirements.txt

   # Install LightRFT
   pip install -e .


Documentation Generation (Optional)
====================================

To install dependencies for generating documentation:

.. code-block:: bash

   pip install -r requirements-doc.txt

To generate HTML documentation:

.. code-block:: bash

   make docs

The documentation will be generated in the ``docs/build`` directory. Open ``index.html`` to view it.

For live browser documentation with auto-reload:

.. code-block:: bash

   make docs-live

Project Structure
=================

LightRFT is organized into several key modules:

.. code-block:: text

   LightRFT/
   â”œâ”€â”€ lightrft/                      # Core library
   â”‚   â”œâ”€â”€ datasets/                  # Dataset implementations
   â”‚   â”‚   â”œâ”€â”€ audio_alpaca.py        # Audio dataset
   â”‚   â”‚   â”œâ”€â”€ grm_dataset.py         # General reward model dataset
   â”‚   â”‚   â”œâ”€â”€ prompts_dataset.py     # Prompts dataset
   â”‚   â”‚   â”œâ”€â”€ prompts_dataset_vl.py  # Vision-language prompts dataset
   â”‚   â”‚   â”œâ”€â”€ sft_dataset.py         # SFT dataset
   â”‚   â”‚   â”œâ”€â”€ sft_dataset_vl.py      # Vision-language SFT dataset
   â”‚   â”‚   â”œâ”€â”€ srm_dataset.py         # Safe reward model dataset
   â”‚   â”‚   â””â”€â”€ utils.py               # Dataset utilities
   â”‚   â”œâ”€â”€ models/                    # Model definitions
   â”‚   â”‚   â”œâ”€â”€ actor_al.py            # Audio-language actor model
   â”‚   â”‚   â”œâ”€â”€ actor_language.py      # Language actor model
   â”‚   â”‚   â”œâ”€â”€ actor_vl.py            # Vision-language actor model
   â”‚   â”‚   â”œâ”€â”€ grm_vl.py              # General reward model (VL)
   â”‚   â”‚   â”œâ”€â”€ srm_al.py              # Safe reward model (AL)
   â”‚   â”‚   â”œâ”€â”€ srm_vl.py              # Safe reward model (VL)
   â”‚   â”‚   â”œâ”€â”€ loss.py                # Loss functions
   â”‚   â”‚   â”œâ”€â”€ utils.py               # Model utilities
   â”‚   â”‚   â””â”€â”€ monkey_patch/          # Model adaptation patches
   â”‚   â”‚       â”œâ”€â”€ apply.py           # Patch application
   â”‚   â”‚       â”œâ”€â”€ hf_generate_patch.py  # HuggingFace generate patch
   â”‚   â”‚       â”œâ”€â”€ llama.py           # LLaMA patches
   â”‚   â”‚       â””â”€â”€ qwen.py            # Qwen patches
   â”‚   â”œâ”€â”€ strategy/                  # Training & inference strategies
   â”‚   â”‚   â”œâ”€â”€ config.py              # Strategy configuration
   â”‚   â”‚   â”œâ”€â”€ fake_strategy.py       # Fake strategy for testing
   â”‚   â”‚   â”œâ”€â”€ strategy.py            # Main strategy implementation
   â”‚   â”‚   â”œâ”€â”€ strategy_base.py       # Strategy base class
   â”‚   â”‚   â”œâ”€â”€ deepspeed/             # DeepSpeed implementation
   â”‚   â”‚   â”‚   â”œâ”€â”€ deepspeed.py       # DeepSpeed strategy
   â”‚   â”‚   â”‚   â””â”€â”€ deepspeed_utils.py # DeepSpeed utilities
   â”‚   â”‚   â”œâ”€â”€ fsdp/                  # FSDP implementation
   â”‚   â”‚   â”‚   â”œâ”€â”€ fsdp_optimizer.py  # FSDP optimizer
   â”‚   â”‚   â”‚   â”œâ”€â”€ fsdp_utils.py      # FSDP utilities
   â”‚   â”‚   â”‚   â””â”€â”€ fsdpv2.py          # FSDP v2 implementation
   â”‚   â”‚   â”œâ”€â”€ sglang_utils/          # SGLang utilities
   â”‚   â”‚   â”‚   â”œâ”€â”€ sglang_engine.py   # SGLang engine
   â”‚   â”‚   â”‚   â””â”€â”€ sgl_model_saver.py # SGLang model saver
   â”‚   â”‚   â”œâ”€â”€ vllm_utils/            # vLLM utilities
   â”‚   â”‚   â”‚   â””â”€â”€ vllm_worker_wrap_no_ray.py  # vLLM worker wrapper
   â”‚   â”‚   â””â”€â”€ utils/                 # Strategy utilities
   â”‚   â”‚       â”œâ”€â”€ broadcast_utils.py # Broadcast utilities
   â”‚   â”‚       â”œâ”€â”€ ckpt_utils.py      # Checkpoint utilities
   â”‚   â”‚       â”œâ”€â”€ data_utils.py      # Data utilities
   â”‚   â”‚       â”œâ”€â”€ distributed_util.py  # Distributed utilities
   â”‚   â”‚       â”œâ”€â”€ optimizer_utils.py # Optimizer utilities
   â”‚   â”‚       â”œâ”€â”€ parallel_utils.py  # Parallel utilities
   â”‚   â”‚       â””â”€â”€ statistic.py       # Statistics utilities
   â”‚   â”œâ”€â”€ trainer/                   # Trainer implementations
   â”‚   â”‚   â”œâ”€â”€ experience_maker.py    # Experience generator
   â”‚   â”‚   â”œâ”€â”€ experience_maker_vl.py # VLM experience generator
   â”‚   â”‚   â”œâ”€â”€ fast_exp_maker.py      # Fast experience maker
   â”‚   â”‚   â”œâ”€â”€ grm_trainer_vl.py      # General reward model trainer (VL)
   â”‚   â”‚   â”œâ”€â”€ kl_controller.py       # KL divergence controller
   â”‚   â”‚   â”œâ”€â”€ ppo_trainer.py         # PPO trainer
   â”‚   â”‚   â”œâ”€â”€ ppo_trainer_vl.py      # Vision-language PPO trainer
   â”‚   â”‚   â”œâ”€â”€ replay_buffer.py       # Replay buffer
   â”‚   â”‚   â”œâ”€â”€ replay_buffer_utils.py # Replay buffer utilities
   â”‚   â”‚   â”œâ”€â”€ replay_buffer_vl.py    # Vision-language replay buffer
   â”‚   â”‚   â”œâ”€â”€ spmd_ppo_trainer.py    # SPMD PPO trainer
   â”‚   â”‚   â”œâ”€â”€ srm_trainer_al.py      # Safe reward model trainer (AL)
   â”‚   â”‚   â”œâ”€â”€ srm_trainer_vl.py      # Safe reward model trainer (VL)
   â”‚   â”‚   â””â”€â”€ utils.py               # Trainer utilities
   â”‚   â””â”€â”€ utils/                     # Utility functions
   â”‚       â”œâ”€â”€ cli_args.py            # CLI argument parsing
   â”‚       â”œâ”€â”€ distributed_sampler.py # Distributed sampler
   â”‚       â”œâ”€â”€ logging_utils.py       # Logging utilities
   â”‚       â”œâ”€â”€ processor.py           # Data processors
   â”‚       â”œâ”€â”€ remote_rm_utils.py     # Remote reward model utilities
   â”‚       â”œâ”€â”€ timer.py               # Timer utilities
   â”‚       â”œâ”€â”€ trajectory_saver.py    # Trajectory saving utilities
   â”‚       â””â”€â”€ utils.py               # General utilities
   â”‚
   â”œâ”€â”€ examples/                      # Usage examples
   â”‚   â”œâ”€â”€ chat/                      # Chat model training examples
   â”‚   â”œâ”€â”€ grm_training/              # General reward model training examples
   â”‚   â”œâ”€â”€ gsm8k_geo3k/               # GSM8K/Geo3K math reasoning examples
   â”‚   â”‚   â”œâ”€â”€ data_preprocess/       # Data preprocessing scripts
   â”‚   â”‚   â”œâ”€â”€ train_colocate.py      # Co-located training script
   â”‚   â”‚   â”œâ”€â”€ reward_models_utils.py # Reward model utilities
   â”‚   â”‚   â”œâ”€â”€ run_grpo_gsm8k_qwen2.5_0.5b.sh    # GSM8K training script
   â”‚   â”‚   â””â”€â”€ run_grpo_geo3k_qwen2.5_vl_7b.sh   # Geo3K VLM training script
   â”‚   â”œâ”€â”€ safework_t1/               # Safe and trusted work examples
   â”‚   â””â”€â”€ srm_training/              # Safe reward model training examples
   â”‚
   â”œâ”€â”€ docs/                          # ðŸ“š Sphinx documentation
   â”‚   â””â”€â”€ source/
   â”‚       â”œâ”€â”€ installation/          # Installation guides
   â”‚       â”œâ”€â”€ quick_start/           # Quick start & user guides
   â”‚       â”œâ”€â”€ best_practice/         # Best practices & resources
   â”‚       â””â”€â”€ api_doc/               # API documentation
   â”‚           â”œâ”€â”€ datasets/          # Dataset API
   â”‚           â”œâ”€â”€ models/            # Model API
   â”‚           â”œâ”€â”€ strategy/          # Strategy API
   â”‚           â”œâ”€â”€ trainer/           # Trainer API
   â”‚           â””â”€â”€ utils/             # Utilities API
   â”‚
   â”œâ”€â”€ assets/                        # Assets
   â”‚   â””â”€â”€ logo.png                   # Project logo
   â”‚
   â”œâ”€â”€ results/                       # Training results
   â”œâ”€â”€ rft_logs/                      # Training logs
   â”œâ”€â”€ requirements.txt               # Python dependencies
   â”œâ”€â”€ requirements-dev.txt           # Development dependencies
   â”œâ”€â”€ requirements-doc.txt           # Documentation dependencies
   â”œâ”€â”€ setup.py                       # Package setup
   â””â”€â”€ README.md                      # Project documentation

Key Directory Descriptions
--------------------------

* **lightrft/**: LightRFT core library with five main modules:

  * ``datasets/``: Dataset implementations for prompts, SFT, reward modeling (text, vision-language, audio-language)
  * ``models/``: Actor models (language, vision-language, audio-language), reward models, and loss functions
  * ``strategy/``: Training strategies including FSDP, DeepSpeed, vLLM/SGLang integration
  * ``trainer/``: Trainer implementations for PPO, experience generation, and replay buffers
  * ``utils/``: Utility functions for CLI, logging, distributed training, and trajectory saving

* **examples/**: Complete training examples and scripts

  * ``gsm8k_geo3k/``: GSM8K and Geo3K math reasoning training examples
  * ``grm_training/``: General reward model training examples
  * ``srm_training/``: Safe reward model training examples
  * ``chat/``: Chat model training examples
  * ``safework_t1/``: Safe and trusted work examples

* **docs/**: Sphinx documentation with complete user guides and API documentation

Verification
============

To verify your installation, run a simple test:

.. code-block:: bash

   python -c "import lightrft; print(lightrft)"

You should see the module path without any import errors.

Quick Start Example
===================

After installation, try a basic GRPO training example:

.. code-block:: bash

   # Single node, 8 GPU training example
   cd /path/to/LightRFT

   # Run GRPO training (GSM8K math reasoning task)
   bash examples/gsm8k_geo3k/run_grpo_gsm8k_qwen2.5_0.5b.sh

   # Or run Geo3K geometry problem training (VLM multimodal)
   bash examples/gsm8k_geo3k/run_grpo_geo3k_qwen2.5_vl_7b.sh

Troubleshooting
===============

Common Issues
-------------

**Issue**: CUDA errors or version mismatch

* **Solution**: Ensure CUDA drivers and toolkit version match your PyTorch installation. Check with ``nvcc --version`` and ``python -c "import torch; print(torch.version.cuda)"``

**Issue**: Out of memory errors during training

* **Solution**:

  * Reduce ``micro_train_batch_size`` or ``micro_rollout_batch_size``
  * Enable gradient checkpointing: ``--gradient_checkpointing``
  * Use FSDP with CPU offload: ``--fsdp --fsdp_cpu_offload``
  * Adjust engine memory utilization: ``--engine_mem_util 0.4``

**Issue**: Slow installation of evaluation dependencies

* **Solution**: Use a mirror or proxy for pip:

  .. code-block:: bash

     pip install -i https://pypi.tuna.tsinghua.edu.cn/simple <package>

For Additional Support
----------------------

If you encounter issues not covered here:

* Check the project's `GitHub Issues <https://github.com/opendilab/LightRFT/issues>`_
* Review the :doc:`../best_practice/strategy_usage` guide for training configuration
* Consult the example scripts in the ``examples/`` directory

Next Steps
==========

After successful installation:

1. Review the :doc:`../quick_start` guide to understand basic usage
2. Explore :doc:`../best_practice/strategy_usage` for distributed training strategies
3. Check out the ``examples/`` directory for complete training examples
4. Read the algorithm documentation for specific implementation details

Related Documentation
=====================

* :doc:`../quick_start` - Quick start guide
* :doc:`../best_practice/strategy_usage` - Strategy usage guide
* :doc:`../api/index` - API reference
