


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>lightrft.strategy.fake_strategy &mdash; LightRFT v0.1.1 documentation</title>
  

  <link rel="shortcut icon" href="../../../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/readthedocs.css" type="text/css" />
  <link rel="index" title="Index" href="../../../genindex.html" />
  <link rel="search" title="Search" href="../../../search.html" />
  <!-- Google Analytics -->
  <script type="text/javascript">
    var collapsedSections = [];
  </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../../../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/LightRFT" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/LightZero" target="_blank">
                  <span class="dropdown-title">LightZero </span>
                  <p>OpenDILab Decision Monte Carlo Tree Search Framework</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GenerativeRL" target="_blank">
                  <span class="dropdown-title">GenerativeRL </span>
                  <p>OpenDILab Generative AI Framework</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quick_start/index.html">Quick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide &amp; Best Practices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../best_practice/index.html">Best Practices</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_doc/utils/index.html">lightrft.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_doc/datasets/index.html">lightrft.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_doc/models/index.html">lightrft.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_doc/strategy/index.html">lightrft.strategy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_doc/trainer/index.html">lightrft.trainer</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
            Docs
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>lightrft.strategy.fake_strategy</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <h1>Source code for lightrft.strategy.fake_strategy</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">FakeStrategy for testing LightRFT without distributed environment.</span>

<span class="sd">This module provides a FakeStrategy class that mimics the behavior of real</span>
<span class="sd">distributed training strategies (DeepSpeed, FSDP) but runs in a single process</span>
<span class="sd">without actual distributed communication. This is useful for unit testing and</span>
<span class="sd">development environments where distributed setup is not available.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">contextlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">contextmanager</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">lightrft.strategy</span><span class="w"> </span><span class="kn">import</span> <span class="n">StrategyConfig</span><span class="p">,</span> <span class="n">StrategyBase</span>

<span class="n">ModelOptimPair</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">]</span>
<span class="n">ModelOrModelOptimPair</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ModelOptimPair</span><span class="p">]</span>


<div class="viewcode-block" id="FakeStrategy"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">FakeStrategy</span><span class="p">(</span><span class="n">StrategyBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fake strategy for testing without distributed environment.</span>

<span class="sd">    This strategy provides the same API as real distributed strategies but</span>
<span class="sd">    runs everything in a single process without actual distributed communication.</span>
<span class="sd">    It&#39;s useful for unit testing and development.</span>

<span class="sd">    :param seed: Random seed for reproducibility</span>
<span class="sd">    :type seed: int</span>
<span class="sd">    :param max_norm: Maximum gradient norm for clipping</span>
<span class="sd">    :type max_norm: float</span>
<span class="sd">    :param micro_train_batch_size: Batch size for each training step</span>
<span class="sd">    :type micro_train_batch_size: int</span>
<span class="sd">    :param train_batch_size: Total batch size for training</span>
<span class="sd">    :type train_batch_size: int</span>
<span class="sd">    :param args: Additional configuration arguments</span>
<span class="sd">    :type args: Any</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span>
        <span class="n">max_norm</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">micro_train_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize fake strategy with common parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">micro_train_batch_size</span><span class="p">,</span> <span class="n">train_batch_size</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

        <span class="c1"># Override distributed setup for fake environment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accumulated_gradient</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">micro_train_batch_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy Initialized (single process mode)&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="FakeStrategy.setup_distributed"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.setup_distributed">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">setup_distributed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_gpu_per_node</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake distributed setup - does nothing in single process mode.</span>

<span class="sd">        :param timeout: Maximum time to wait for initialization (ignored)</span>
<span class="sd">        :type timeout: timedelta, optional</span>
<span class="sd">        :param num_gpu_per_node: Number of GPUs per node (ignored)</span>
<span class="sd">        :type num_gpu_per_node: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># Set local rank to 0 for single process</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="s1">&#39;local_rank&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For backward compatibility with args</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;args&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Set device</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy: Running in single process mode&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="FakeStrategy.create_optimizer"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.create_optimizer">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">create_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optimizer</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a standard optimizer for the model.</span>

<span class="sd">        :param model: The model to optimize</span>
<span class="sd">        :type model: nn.Module</span>
<span class="sd">        :param kwargs: Additional optimizer arguments</span>

<span class="sd">        :return: The created optimizer</span>
<span class="sd">        :rtype: Optimizer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;is_actor&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">is_actor</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">model</span>

        <span class="c1"># Use AdamW as default optimizer</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

<div class="viewcode-block" id="FakeStrategy.prepare"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.prepare">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">prepare</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">models_or_model_optim_pairs</span><span class="p">:</span> <span class="n">ModelOrModelOptimPair</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ModelOrModelOptimPair</span><span class="p">],</span> <span class="n">ModelOrModelOptimPair</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare models and optimizers - returns them as-is in fake mode.</span>

<span class="sd">        :param models_or_model_optim_pairs: Models or (model, optimizer) pairs to prepare</span>

<span class="sd">        :return: Prepared models/optimizers (unchanged in fake mode)</span>
<span class="sd">        :rtype: Union[List[ModelOrModelOptimPair], ModelOrModelOptimPair]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">models_or_model_optim_pairs</span><span class="p">:</span>
            <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">ret</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">ret</span></div>

<div class="viewcode-block" id="FakeStrategy.backward"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.backward">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform backward pass using standard PyTorch.</span>

<span class="sd">        :param loss: The loss to backpropagate</span>
<span class="sd">        :type loss: torch.Tensor</span>
<span class="sd">        :param model: The model</span>
<span class="sd">        :type model: nn.Module</span>
<span class="sd">        :param optimizer: The optimizer</span>
<span class="sd">        :type optimizer: Optimizer</span>
<span class="sd">        :param kwargs: Additional arguments</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span></div>

<div class="viewcode-block" id="FakeStrategy.optimizer_step"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.optimizer_step">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Take optimizer step using standard PyTorch.</span>

<span class="sd">        :param optimizer: The optimizer</span>
<span class="sd">        :type optimizer: Optimizer</span>
<span class="sd">        :param model: The model</span>
<span class="sd">        :type model: nn.Module</span>
<span class="sd">        :param scheduler: The learning rate scheduler (optional)</span>
<span class="sd">        :param name: Name for logging purposes</span>
<span class="sd">        :type name: str</span>
<span class="sd">        :param kwargs: Additional arguments</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Apply gradient clipping if max_norm is set</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_norm</span><span class="p">)</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span></div>

<div class="viewcode-block" id="FakeStrategy.save_ckpt"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.save_ckpt">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">save_ckpt</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_num</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_mem</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">client_state</span><span class="o">=</span><span class="p">{},</span> <span class="n">save_latest</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Save checkpoint using standard PyTorch saving.</span>

<span class="sd">        :param model: The model to save</span>
<span class="sd">        :param save_dir: Directory to save the checkpoint</span>
<span class="sd">        :type save_dir: str</span>
<span class="sd">        :param tag: Optional tag for the checkpoint</span>
<span class="sd">        :param max_num: Maximum number of checkpoints to keep</span>
<span class="sd">        :type max_num: int</span>
<span class="sd">        :param max_mem: Maximum memory in MB for checkpoints (ignored)</span>
<span class="sd">        :type max_mem: int</span>
<span class="sd">        :param client_state: Additional state to save</span>
<span class="sd">        :type client_state: dict</span>
<span class="sd">        :param save_latest: Whether to save as latest checkpoint</span>
<span class="sd">        :type save_latest: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">tag</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tag</span> <span class="o">=</span> <span class="s2">&quot;checkpoint&quot;</span>

        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">)</span>

        <span class="c1"># Save model state</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;state_dict&quot;</span><span class="p">):</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model_state_dict&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s1">&#39;client_state&#39;</span><span class="p">:</span> <span class="n">client_state</span><span class="p">}</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;FakeStrategy: Saved checkpoint to </span><span class="si">{</span><span class="n">checkpoint_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="FakeStrategy.load_ckpt"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.load_ckpt">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">load_ckpt</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">load_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">tag</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">load_module_strict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">load_optimizer_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">load_lr_scheduler_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">load_module_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load checkpoint using standard PyTorch loading.</span>

<span class="sd">        :param model: The model to load checkpoint into</span>
<span class="sd">        :param load_dir: Directory containing the checkpoint</span>
<span class="sd">        :type load_dir: str</span>
<span class="sd">        :param tag: Optional specific checkpoint tag to load</span>
<span class="sd">        :param load_module_strict: Whether to use strict loading for module states</span>
<span class="sd">        :type load_module_strict: bool</span>
<span class="sd">        :param load_optimizer_states: Whether to load optimizer states</span>
<span class="sd">        :type load_optimizer_states: bool</span>
<span class="sd">        :param load_lr_scheduler_states: Whether to load learning rate scheduler states</span>
<span class="sd">        :type load_lr_scheduler_states: bool</span>
<span class="sd">        :param load_module_only: Whether to load only the module states</span>
<span class="sd">        :type load_module_only: bool</span>

<span class="sd">        :return: Tuple of (load_path, client_states)</span>
<span class="sd">        :rtype: tuple</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">tag</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tag</span> <span class="o">=</span> <span class="s2">&quot;checkpoint&quot;</span>

        <span class="n">checkpoint_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">load_dir</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">tag</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Checkpoint not found: </span><span class="si">{</span><span class="n">checkpoint_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="s1">&#39;model_state_dict&#39;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;model_state_dict&#39;</span><span class="p">],</span> <span class="n">strict</span><span class="o">=</span><span class="n">load_module_strict</span><span class="p">)</span>

        <span class="n">client_states</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;client_state&#39;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;FakeStrategy: Loaded checkpoint from </span><span class="si">{</span><span class="n">checkpoint_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">checkpoint_path</span><span class="p">,</span> <span class="n">client_states</span></div>

<div class="viewcode-block" id="FakeStrategy.all_reduce"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.all_reduce">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">all_reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake all-reduce operation - returns data unchanged.</span>

<span class="sd">        :param data: Data to be reduced</span>
<span class="sd">        :type data: Union[torch.Tensor, dict]</span>
<span class="sd">        :param op: Reduction operation (ignored in fake mode)</span>
<span class="sd">        :type op: str</span>

<span class="sd">        :return: Data unchanged</span>
<span class="sd">        :rtype: Union[torch.Tensor, dict]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">data</span></div>

<div class="viewcode-block" id="FakeStrategy.all_gather"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.all_gather">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake all-gather operation - returns data wrapped in list.</span>

<span class="sd">        :param data: Data to be gathered</span>
<span class="sd">        :type data: Union[torch.Tensor, dict]</span>

<span class="sd">        :return: Data wrapped to mimic gathered result</span>
<span class="sd">        :rtype: Union[torch.Tensor, dict]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">data</span><span class="p">]</span></div>

<div class="viewcode-block" id="FakeStrategy.is_rank_0"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.is_rank_0">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_rank_0</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Always returns True in fake mode (single process is rank 0).</span>

<span class="sd">        :return: True</span>
<span class="sd">        :rtype: bool</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">True</span></div>

<div class="viewcode-block" id="FakeStrategy.get_rank"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.get_rank">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">get_rank</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Always returns 0 in fake mode (single process).</span>

<span class="sd">        :return: 0</span>
<span class="sd">        :rtype: int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="mi">0</span></div>

<div class="viewcode-block" id="FakeStrategy.setup_inference_engine"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.setup_inference_engine">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">setup_inference_engine</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">engine_type</span><span class="o">=</span><span class="s2">&quot;vllm&quot;</span><span class="p">,</span> <span class="n">actor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake inference engine setup - returns None.</span>

<span class="sd">        :param args: Configuration arguments</span>
<span class="sd">        :type args: argparse.Namespace</span>
<span class="sd">        :param engine_type: Type of inference engine (ignored)</span>
<span class="sd">        :type engine_type: str</span>
<span class="sd">        :param actor: The actor module (ignored)</span>
<span class="sd">        :type actor: torch.nn.Module</span>

<span class="sd">        :return: None</span>
<span class="sd">        :rtype: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy: Inference engine setup skipped&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="FakeStrategy.maybe_sleep_inference_engine"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.maybe_sleep_inference_engine">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">maybe_sleep_inference_engine</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake inference engine sleep - does nothing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy: Inference engine sleep skipped&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="FakeStrategy.wakeup_inference_engine"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.wakeup_inference_engine">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">wakeup_inference_engine</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake inference engine wakeup - does nothing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy: Inference engine wakeup skipped&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="FakeStrategy.engine_generate_local"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.engine_generate_local">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">engine_generate_local</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">multi_modal_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake generation - returns empty results.</span>

<span class="sd">        :param sampling_params: Parameters for generation (ignored)</span>
<span class="sd">        :param prompt_token_ids: Prompt token IDs (ignored)</span>
<span class="sd">        :param multi_modal_inputs: Multimodal inputs (ignored)</span>

<span class="sd">        :return: Empty list</span>
<span class="sd">        :rtype: List</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy: Generation skipped&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[]</span></div>

<div class="viewcode-block" id="FakeStrategy.gather_and_generate"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.gather_and_generate">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">gather_and_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">sampling_params</span><span class="p">,</span>
        <span class="n">all_prompt_token_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">all_prompts</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">all_images</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">sleep_engine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">images_num</span><span class="o">=</span><span class="kc">None</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake gather and generate - returns empty results.</span>

<span class="sd">        :param sampling_params: Parameters for generation (ignored)</span>
<span class="sd">        :param all_prompt_token_ids: All prompt token IDs (ignored)</span>
<span class="sd">        :param all_prompts: All prompts (ignored)</span>
<span class="sd">        :param all_images: All images (ignored)</span>
<span class="sd">        :param sleep_engine: Whether to sleep engine after generation (ignored)</span>
<span class="sd">        :type sleep_engine: bool</span>
<span class="sd">        :param images_num: Number of images (ignored)</span>

<span class="sd">        :return: Empty list</span>
<span class="sd">        :rtype: List</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy: Gather and generate skipped&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[]</span></div>

<div class="viewcode-block" id="FakeStrategy.update_engine_weights"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.update_engine_weights">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">update_engine_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">actor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake engine weight update - does nothing.</span>

<span class="sd">        :param actor: The actor model (ignored)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy: Engine weight update skipped&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="FakeStrategy.init_model_context"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.init_model_context">[docs]</a>    <span class="nd">@contextmanager</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_model_context</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake model initialization context - does nothing.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s2">&quot;FakeStrategy: Model initialization context finished&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="FakeStrategy.maybe_offload_optimizer"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.maybe_offload_optimizer">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">maybe_offload_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake optimizer offloading - returns optimizer unchanged.</span>

<span class="sd">        :param optimizer: The optimizer to potentially offload</span>
<span class="sd">        :type optimizer: torch.optim.Optimizer</span>

<span class="sd">        :return: The original optimizer</span>
<span class="sd">        :rtype: torch.optim.Optimizer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">optimizer</span></div>

<div class="viewcode-block" id="FakeStrategy.maybe_load_optimizer"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.FakeStrategy.maybe_load_optimizer">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">maybe_load_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fake optimizer loading - returns optimizer unchanged.</span>

<span class="sd">        :param optimizer: The optimizer to potentially load</span>
<span class="sd">        :type optimizer: torch.optim.Optimizer</span>
<span class="sd">        :param device: Target device for loading (ignored)</span>
<span class="sd">        :type device: torch.device</span>

<span class="sd">        :return: The original optimizer</span>
<span class="sd">        :rtype: torch.optim.Optimizer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">optimizer</span></div></div>


<div class="viewcode-block" id="get_fake_strategy"><a class="viewcode-back" href="../../../api_doc/strategy/fake_strategy.html#lightrft.strategy.fake_strategy.get_fake_strategy">[docs]</a><span class="k">def</span><span class="w"> </span><span class="nf">get_fake_strategy</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create and return a FakeStrategy instance.</span>

<span class="sd">    This is a convenience function similar to get_strategy() but for fake strategy.</span>

<span class="sd">    :param args: Configuration arguments</span>
<span class="sd">    :type args: object</span>

<span class="sd">    :return: A FakeStrategy instance</span>
<span class="sd">    :rtype: FakeStrategy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">StrategyConfig</span><span class="o">.</span><span class="n">from_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">StrategyConfig</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">FakeStrategy</span><span class="p">(</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">max_norm</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">max_norm</span><span class="p">,</span>
        <span class="n">micro_train_batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">micro_train_batch_size</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">train_batch_size</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="p">)</span></div>
</pre></div>

              </article>
              
            </div>
            <footer>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2025, OpenDILab.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../../../"
    src="../../../_static/documentation_options.js"></script>
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
  <script src="../../../_static/doctools.js"></script>
  <script src="../../../_static/sphinx_highlight.js"></script>
  <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
  <script src="../../../_static/js/language_switch.js"></script>
  

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/LightRFT" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>