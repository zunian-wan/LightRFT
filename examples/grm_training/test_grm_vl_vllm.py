import os
import json
import torch
from tqdm import tqdm
from typing import List, Dict
from loguru import logger
from torch.utils.data import DataLoader
from abc import ABC, abstractmethod

from transformers import AutoProcessor, AutoTokenizer
from vllm import LLM, SamplingParams

from lightrft.datasets import GRMPromptDatasetVL, extract_answer

# Example Task Instruction for GRM Evaluation
TASK_INSTRUCTION_COT_T2I = """Given a caption and two images generated based on this caption, please analyze in detail the two provided images. 
Evaluate them on various dimensions such as semantic consistency (how closely the image content aligns with the caption), 
aesthetics (composition, color usage, artistic expression), authenticity (realism and attention to detail), 
and any other factors you deem relevant. For each evaluation dimension, 
provide a score between 1-10 for both images (e.g., Image 1: 8/10, Image 2: 6/10) and provide a concise rationale for the score. 
Calculate the total score for each image by summing all dimension scores. 
Use a chain-of-thought process to detail your reasoning steps, and enclose all your detailed reasoning within tags. 
Then, in the <answer> tag, output exactly one of the following strings: 'Image 1 is better' or 'Image 2 is better' or 'Both are equal' based on the total scores. 
No additional text is allowed in the <answer> section.
Example output format:
<think>
Semantic consistency: Image 1 (9/10) - ...; Image 2 (7/10) - ...
Aesthetics: Image 2 (8/10) - ...; Image 1 (8/10) - ...
Authenticity: Image 1 (8/10) - ...; Image 2 (5/10) - ...
[Additional dimensions if any]: Image 2 (8/10) - ...; Image 1 (6/10) - ...
Total score:
Image 1: 9+8+8+6=31
Image 2: 7+8+5+8=28
</think>
<answer>Image 1 is better</answer>
Note: In the example above, scores and the final answer are placeholders meant only to demonstrate the format. Your actual evaluation should be based on the quality of two given images.
Your task is provided as follows:
Text Caption: {prompt}
"""

TASK_INSTRUCTION_COT_T2V ="""
Given a caption and two videos generated based on this caption, please analyze in detail the two provided videos. 
Evaluate them on various dimensions such as semantic consistency (how closely the video content aligns with the caption), temporal coherence (smoothness and logical flow of motion across frames), authenticity (realism and attention to detail), and any other factors you deem relevant. 
For each evaluation dimension, provide a score between 1-10 for both videos (e.g., Video 1: 8/10, Video 2: 6/10) and provide a concise rationale for the score. 
Calculate the total score for each video by summing all dimension scores. 
Use a chain-of-thought process to detail your reasoning steps, and enclose all your detailed reasoning within <think> and </think> tags. Then, in the <answer> tag, output exactly one of the following strings:
'Video 1 is better' or 'Video 2 is better' or 'Both are equal' based on the total scores. No additional text is allowed in the <answer> section.
Example output format:
<think>
1. Semantic consistency: Video 1 (9/10) - ...; Video 2 (7/10) - ...
2. Temporal coherence: Video 1 (8/10) - ...; Video 2 (6/10) - ...
3. Authenticity: Video 1 (7/10) - ...; Video 2 (5/10) - ...
...
[Additional dimensions if any]: Video 2 (8/10) - ...; Video 1 (6/10) - ...
Total score:
Video 1: 9+8+7+6=30
Video 2: 7+6+5+8=26
</think>
<answer>Video 1 is better</answer>

Note: In the example above, scores and the final answer are placeholders meant only to demonstrate the format. Your actual evaluation should be based on the quality of two given videos.
Your task is provided as follows:
Text Caption: **{prompt}**
"""

class BaseEvaluator(ABC):
    """Base class: Responsible for parsing generation results and calculating metrics."""
    
    def __init__(self):
        self.correct = 0
        self.total = 0
        self.results = []

    @abstractmethod
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        """
        Process a batch of generation results.
        :param gen_texts: List of texts generated by the model.
        :param extras: Extra information from the dataset (including Ground Truth).
        """
        pass

    def get_accuracy(self) -> float:
        return self.correct / self.total if self.total > 0 else 0.0

    def get_results(self) -> List[Dict]:
        return self.results


class ImageGenCoTEvaluator(BaseEvaluator):
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        for i, (gen_text, other) in enumerate(zip(gen_texts, extras)):
            predicted_answer = extract_answer(gen_text)
            gt_answer = extract_answer(other["response"])   # 'Image1 is better' or 'Image 2 is better'
            
            if gt_answer == predicted_answer:
                self.correct += 1
            elif predicted_answer is None:
                print(f"Could not extract answer from generated text: {gen_text}")
            self.total += 1
                        
            print(f"Batch Sample {i} | Pred: {predicted_answer} | GT: {gt_answer}")

            self.results.append({
                "ground_truth": gt_answer,
                "predicted_answer": predicted_answer,
                "prompt": other.get("system_prompt", ""),
                "generated_text": gen_text,
            })
        

class OmniRewardBenchT2IEvaluator(BaseEvaluator):
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        for i, (gen_text, other) in enumerate(zip(gen_texts, extras)):
            predicted_answer = extract_answer(gen_text)
            gt_preference = other['preference'] # A, B, or C
            
            # Mapping logic: A -> Image 1, B -> Image 2
            if gt_preference == "A" and predicted_answer == "Image 1 is better":
                self.correct += 1
            elif gt_preference == "B" and predicted_answer == "Image 2 is better":
                self.correct += 1
            elif gt_preference == "C" and predicted_answer == "Both are equal":
                self.correct += 1
            elif predicted_answer is None:
                print(f"Could not extract answer from generated text: {gen_text}")
            self.total += 1

            print(f"Sample {i} | Pred: {predicted_answer} | GT: {gt_preference}")

            self.results.append({
                "id": other["id"],
                "prompt": other['prompt'],
                "criteria": other['criteria'],
                "ground_truth": gt_preference,
                "predicted_answer": predicted_answer,
                "generated_text": gen_text,
            })


class OmniRewardBenchT2VEvaluator(BaseEvaluator):
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        for i, (gen_text, other) in enumerate(zip(gen_texts, extras)):
            predicted_answer = extract_answer(gen_text)
            gt_preference = other['preference'] # A, B, or C
            
            # Mapping logic: A -> Image 1, B -> Image 2
            if gt_preference == "A" and predicted_answer == "Video 1 is better":
                self.correct += 1
            elif gt_preference == "B" and predicted_answer == "Video 2 is better":
                self.correct += 1
            elif gt_preference == "C" and predicted_answer == "Both are equal":
                self.correct += 1
            elif predicted_answer is None:
                print(f"Could not extract answer from generated text: {gen_text}")
            self.total += 1

            print(f"Sample {i} | Pred: {predicted_answer} | GT: {gt_preference}")

            self.results.append({
                "id": other["id"],
                "prompt": other['prompt'],
                "criteria": other['criteria'],
                "ground_truth": gt_preference,
                "predicted_answer": predicted_answer,
                "generated_text": gen_text,
            })


class HPDv3GRMEvaluator(BaseEvaluator):
    """Evaluator for HPDv3 Test set."""
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        for i, (gen_text, other) in enumerate(zip(gen_texts, extras)):
            predicted_answer = extract_answer(gen_text)
            gt_preference = other['preference'] # A, B, or C
            
            # Mapping logic: A -> Image 1, B -> Image 2
            if gt_preference == "A" and predicted_answer == "Image 1 is better":
                self.correct += 1
            elif gt_preference == "B" and predicted_answer == "Image 2 is better":
                self.correct += 1
            elif predicted_answer is None:
                print(f"Could not extract answer from generated text: {gen_text}")
            self.total += 1

            print(f"Sample {i} | Pred: {predicted_answer} | GT: {gt_preference}")

            self.results.append({
                "prompt": other['prompt'],
                "ground_truth": gt_preference,
                "predicted_answer": predicted_answer,
                "generated_text": gen_text,
                "model_chosen": other['model_chosen'],
                "model_rejected": other['model_rejected'],
                "preferred_path": other['preferred_path'],
                "rejected_path": other['rejected_path'],
            })


def test_grm_vllm(
    model_path: str,
    data_path: List[str],
    evaluator: BaseEvaluator,
    llm: LLM,
    config: dict = None,
    batch_size: int = 32,
    max_new_tokens: int = 1024,
    save_dir: str = "./test_results",
):
    logger.info(f"Loading model from: {model_path}")
    
    sampling_params = SamplingParams(
        temperature=0.0,  # For deterministic output
        max_tokens=max_new_tokens,
    )

    logger.info(f"Model loaded successfully from {model_path}.")

    # Load Processor and Tokenizer for Dataset
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

    # Load Dataset
    dataset = GRMPromptDatasetVL(
        data_path,
        processor=processor,
        tokenizer=tokenizer,
        strategy=None,
        max_length=8192,
        config=config,
        is_training=False
    )

    data_loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        collate_fn=dataset.collate_fn
    )

    logger.info(f"Starting inference with evaluator: {evaluator.__class__.__name__}")
    
    for batch_idx, batch in enumerate(tqdm(data_loader)):
        input_texts, image_inputs_list, video_inputs_list, extras = batch
        
        inputs = []
        for i in range(len(input_texts)):
            prompt = input_texts[i]
            image_inputs = image_inputs_list[i]
            video_inputs = video_inputs_list[i]
            
            mm_data = {}
            if image_inputs is not None:
                mm_data["image"] = image_inputs
            if video_inputs is not None:
                mm_data["video"] = video_inputs
            
            inputs.append({
                "prompt": prompt,
                "multi_modal_data": mm_data
            })

        # Generate
        outputs = llm.generate(inputs, sampling_params=sampling_params)

        # Decode
        gen_texts = [output.outputs[0].text for output in outputs]

        # Delegate evaluation
        evaluator.evaluate_batch(gen_texts, extras)

    # Summary and Save
    accuracy = evaluator.get_accuracy()
    print(f"Evaluation completed. Accuracy: {accuracy*100:.2f}% ({evaluator.correct}/{evaluator.total})")

    if save_dir:
        new_save_dir = os.path.join(save_dir, os.path.basename(model_path), config["name"])
        os.makedirs(new_save_dir, exist_ok=True)
        
        with open(os.path.join(new_save_dir, "evaluation_info.txt"), "w") as f:
            f.write(f"Dataset paths: {data_path}\n")
            f.write(f"Model path: {model_path}\n")
            f.write(f"Evaluator: {evaluator.__class__.__name__}\n")
            f.write(f"Max new tokens: {max_new_tokens}\n")
            f.write(f"Accuracy: {accuracy*100:.2f}% ({evaluator.correct}/{evaluator.total})\n")

        results = evaluator.get_results()
        with open(os.path.join(new_save_dir, "all_results.jsonl"), "w") as f:
            for item in results:
                f.write(json.dumps(item) + "\n")
        logger.info(f"Results saved to {new_save_dir}")


if __name__ == "__main__":
    model_path = "/path/to/your/grm-vl-model"

    benchmark_configs = [
        {
            "name": "HPDv3-Test",
            "evaluator": HPDv3GRMEvaluator(),
            "data_path": ["hpdv3:/path/to/HPDv3/test.json"],
            "task_instruction": TASK_INSTRUCTION_COT_T2I,
        },
        {
            "name": "OmniRewardBench-T2I", 
            "evaluator": OmniRewardBenchT2IEvaluator(), 
            "data_path": ["omnirewardbench-t2i:/path/to/omnireward-bench/t2i/test.parquet"], 
            "task_instruction": TASK_INSTRUCTION_COT_T2I,
        },
        {
            "name": "OmniRewardBench-T2V", 
            "evaluator": OmniRewardBenchT2VEvaluator(), 
            "data_path": ["omnirewardbench-t2v:/path/to/omnireward-bench/t2v/test.parquet"], 
            "task_instruction": TASK_INSTRUCTION_COT_T2V,
            "video_fps": 2.0
        },
        {
            "name": "ImageGen-CoT-Reward-5K", 
            "evaluator": ImageGenCoTEvaluator(), 
            "data_path": ["imagegen-cot-reward-5k:/path/to/ImageGen-CoT-Reward-5K/train_data.json" ], 
        },
    ]

    # Initialize vLLM
    tensor_parallel_size = 2
    llm = LLM(
        model=model_path,
        tensor_parallel_size=tensor_parallel_size,
        trust_remote_code=True,
        gpu_memory_utilization=0.9,
        limit_mm_per_prompt={
            "image": 2, 
            "video": 2
        },
    )

    for config in benchmark_configs:
        print(f">>> Running {config['name']} Evaluation")
        test_grm_vllm(
            model_path, 
            config["data_path"], 
            evaluator=config["evaluator"],
            llm=llm,
            config=config, 
            batch_size=64,
            max_new_tokens=1024,
        )
