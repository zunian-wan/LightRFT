import os
import json
import torch
from tqdm import tqdm
from typing import List, Dict
from loguru import logger
from torch.utils.data import DataLoader
from abc import ABC, abstractmethod

from lightrft.models import GenerativeRewardModelVL
from transformers import AutoProcessor

from lightrft.datasets import GRMDataset, extract_answer

# Example Task Instruction for GRM Evaluation
TASK_INSTRUCTION = """
You will act as an expert image evaluator for text-to-image generation.
Given a text prompt and two generated images, your task is to assess the overall quality of the images and determine which one is better.
Your evaluation should focus on the following key aspects:
• Preference: Which image would a human viewer find more satisfying or visually appealing overall.
• Alignment: How well the image content matches the given text prompt in semantics, objects, and attributes.

Your response must strictly follow the format with no extra text:
<answer>Image 1 is better</answer>
or
<answer>Image 2 is better</answer>

The task is provided below. Please give your judgment based on the above criteria.
The prompt used for generation is as follows: {prompt}.
"""


class BaseEvaluator(ABC):
    """Base class: Responsible for parsing generation results and calculating metrics."""
    
    def __init__(self):
        self.correct = 0
        self.total = 0
        self.results = []

    @abstractmethod
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        """
        Process a batch of generation results.
        :param gen_texts: List of texts generated by the model.
        :param extras: Extra information from the dataset (including Ground Truth).
        """
        pass

    def get_accuracy(self) -> float:
        return self.correct / self.total if self.total > 0 else 0.0

    def get_results(self) -> List[Dict]:
        return self.results


class ImageGenCoTEvaluator(BaseEvaluator):
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        for i, (gen_text, other) in enumerate(zip(gen_texts, extras)):
            predicted_answer = extract_answer(gen_text)
            gt_answer = extract_answer(other["response"])   # 'Image1 is better' or 'Image 2 is better'
            
            if gt_answer == predicted_answer:
                self.correct += 1
            elif predicted_answer is None:
                print(f"Could not extract answer from generated text: {gen_text}")
            self.total += 1
                        
            print(f"Batch Sample {i} | Pred: {predicted_answer} | GT: {gt_answer}")

            self.results.append({
                "ground_truth": gt_answer,
                "predicted_answer": predicted_answer,
                "prompt": other.get("system_prompt", ""),
                "generated_text": gen_text,
            })
        

class OmniRewardBenchT2IEvaluator(BaseEvaluator):
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        for i, (gen_text, other) in enumerate(zip(gen_texts, extras)):
            predicted_answer = extract_answer(gen_text)
            gt_preference = other['preference'] # A, B, or C
            
            # Mapping logic: A -> Image 1, B -> Image 2
            if gt_preference == "A" and predicted_answer == "Image 1 is better":
                self.correct += 1
            elif gt_preference == "B" and predicted_answer == "Image 2 is better":
                self.correct += 1
            elif gt_preference == "C":
                pass
            elif predicted_answer is None:
                print(f"Could not extract answer from generated text: {gen_text}")
            self.total += 1

            print(f"Sample {i} | Pred: {predicted_answer} | GT: {gt_preference}")

            self.results.append({
                "id": other["id"],
                "prompt": other['prompt'],
                "criteria": other['criteria'],
                "ground_truth": gt_preference,
                "predicted_answer": predicted_answer,
                "generated_text": gen_text,
            })


class HPDv3GRMEvaluator(BaseEvaluator):
    """Evaluator for HPDv3 Test set."""
    def evaluate_batch(self, gen_texts: List[str], extras: List[Dict]) -> None:
        for i, (gen_text, other) in enumerate(zip(gen_texts, extras)):
            predicted_answer = extract_answer(gen_text)
            gt_preference = other['preference'] # A, B, or C
            
            # Mapping logic: A -> Image 1, B -> Image 2
            if gt_preference == "A" and predicted_answer == "Image 1 is better":
                self.correct += 1
            elif gt_preference == "B" and predicted_answer == "Image 2 is better":
                self.correct += 1
            elif predicted_answer is None:
                print(f"Could not extract answer from generated text: {gen_text}")
            self.total += 1

            print(f"Sample {i} | Pred: {predicted_answer} | GT: {gt_preference}")

            self.results.append({
                "prompt": other['prompt'],
                "ground_truth": gt_preference,
                "predicted_answer": predicted_answer,
                "generated_text": gen_text,
                "model_chosen": other['model_chosen'],
                "model_rejected": other['model_rejected'],
                "preferred_path": other['preferred_path'],
                "rejected_path": other['rejected_path'],
            })


@torch.no_grad()
def test_grm(
    model_path: str,
    data_path: List[str],
    evaluator: BaseEvaluator,
    config: dict = None,
    batch_size: int = 32,
    max_new_tokens: int = 1024,
    save_dir: str = "./test_results"
):
    logger.info(f"Loading model from: {model_path}")
    
    # Load Model
    model = GenerativeRewardModelVL(
        model_path,
        bf16=True,
        lora_rank=0,
        lora_alpha=0,
        target_modules=None,
        ds_config=None,
        device_map=None,
    )
    logger.info(f"Model loaded successfully from {model_path}.")
    device = torch.cuda.current_device()
    model.to(device)
    model.eval()

    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True, use_fast=False)

    # Load Dataset
    dataset = GRMDataset(
        data_path,
        tokenizer=processor.tokenizer,
        strategy=None,
        processor=processor,
        max_length=8192,
        config=config,
        is_training=False,
    )

    data_loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        pin_memory=True,
        collate_fn=dataset.collate_fn
    )

    logger.info(f"Starting inference with evaluator: {evaluator.__class__.__name__}")
    
    for batch_idx, batch in enumerate(tqdm(data_loader)):
        ids, mask, pixel_values, image_grid_thws, pixel_values_videos, video_grid_thws, labels, extras = batch
        
        ids = ids.squeeze(1).to(device)
        mask = mask.squeeze(1).to(device)

        if pixel_values is not None:
            pixel_values = pixel_values.to(device)
            image_grid_thws = image_grid_thws.to(device)
        
        if pixel_values_videos is not None:
            pixel_values_videos = pixel_values_videos.to(device)
            video_grid_thws = video_grid_thws.to(device)

        # Generate with unified max_new_tokens
        gen_ids = model.model.generate(
            input_ids=ids,
            attention_mask=mask,
            pixel_values=pixel_values,
            image_grid_thw=image_grid_thws,
            pixel_values_videos=pixel_values_videos,
            video_grid_thw=video_grid_thws,
            max_new_tokens=max_new_tokens, 
            use_cache=True,
        )

        # Decode
        gen_ids_trimmed = [out_ids[len(ids):] for ids, out_ids in zip(ids, gen_ids)]
        gen_text = processor.batch_decode(gen_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)

        # Delegate evaluation
        evaluator.evaluate_batch(gen_text, extras)

    # Summary and Save
    accuracy = evaluator.get_accuracy()
    print(f"Evaluation completed. Accuracy: {accuracy*100:.2f}% ({evaluator.correct}/{evaluator.total})")

    if save_dir:
        new_save_dir = os.path.join(save_dir, os.path.basename(model_path), config["name"])
        os.makedirs(new_save_dir, exist_ok=True)
        
        with open(os.path.join(new_save_dir, "evaluation_info.txt"), "w") as f:
            f.write(f"Dataset paths: {data_path}\n")
            f.write(f"Model path: {model_path}\n")
            f.write(f"Evaluator: {evaluator.__class__.__name__}\n")
            f.write(f"Max new tokens: {max_new_tokens}\n")
            f.write(f"Accuracy: {accuracy*100:.2f}% ({evaluator.correct}/{evaluator.total})\n")

        results = evaluator.get_results()
        with open(os.path.join(new_save_dir, "all_results.jsonl"), "w") as f:
            for item in results:
                f.write(json.dumps(item) + "\n")
        logger.info(f"Results saved to {new_save_dir}")


if __name__ == "__main__":
    model_path = "path/to/your/grm/ckpt"

    benchmark_configs = [
        {
            "name": "HPDv3-Test",
            "evaluator": HPDv3GRMEvaluator(),
            "data_path": ["hpdv3:/path/to/hpdv3/test_data.json"],
            "task_instruction": TASK_INSTRUCTION,
        },
        {
            "name": "OmniRewardBench-T2I", 
            "evaluator": OmniRewardBenchT2IEvaluator(), 
            "data_path": ["omnirewardbench-t2i:/path/to/OmniRewardBench/text_to_image/test.parquet"], 
            "task_instruction": TASK_INSTRUCTION,
        },
        {
            "name": "ImageGen-CoT-Reward-5K", 
            "evaluator": ImageGenCoTEvaluator(), 
            "data_path": ["imagegen-cot-reward-5k:/path/to/ImageGen-CoT-Reward-5K/test.json" ], 
        },
    ]

    for config in benchmark_configs:
        print(f">>> Running {config['name']} Evaluation")
        test_grm(
            model_path, 
            config["data_path"], 
            evaluator=config["evaluator"],
            config=config, 
            batch_size=128,
            max_new_tokens=1024,
        )